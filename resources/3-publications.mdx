---
id: publications
title: Publications
hide_table_of_contents: true
---


import clsx from 'clsx';
import Layout from '@theme/Layout';
import Link from '@docusaurus/Link';
import useDocusaurusContext from '@docusaurus/useDocusaurusContext';
import useBaseUrl from '@docusaurus/useBaseUrl';
import LongCard from "../src/components/LongCard.js";
import LongCardPub from "../src/components/LongCardPub.js";


        
<LongCardPub
  category="Conference Paper"
  date="July 2024"
  title="Evaluating self-attention interpretability through human-grounded experimental protocol"
  description="This paper aims to assess how attention coefficients from Transformer architecture can help in providing interpretability. A new attention-based interpretability method called CLaSsification-Attention (CLS-A) is proposed. A human-grounded experiment is conducted to evaluate and compare CLS-A to other interpretability methods. The experimental protocol relies on the capacity of an interpretability method to provide explanations in line with human reasoning."
  href="https://arxiv.org/abs/1234.56789"
/>

<LongCardPub
  category="Conference Paper"
  date="July 2023"
  title="Evaluating self-attention interpretability through human-grounded experimental protocol"
  description="Incorporating natural language rationales in the prompt and In-Context Learning (ICL) have led to a significant improvement of Large Language Models (LLMs) performance. However, generating high-quality rationales require human-annotation or the use of auxiliary proxy models. In this work, we propose Self-AMPLIFY to automatically generate rationales from post hoc explanation methods applied to Small Language Models (SLMs) to improve their own performance. Self-AMPLIFY is a 3-step method that targets samples, generates rationales and builds a final prompt to leverage ICL. Self-AMPLIFY performance is evaluated on four SLMs and five datasets requiring strong reasoning abilities. Self-AMPLIFY achieves good results against competitors, leading to strong accuracy improvement. Self-AMPLIFY is the first method to apply post hoc explanation methods to autoregressive language models to generate rationales to improve their own performance in a fully automated manner."
  href="https://arxiv.org/abs/1234.56789"
/>

<LongCardPub
  category="Conference Paper"
  date="July 2024"
  title="Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations."
  description="This paper aims to assess how attention coefficients from Transformer architecture can help in providing interpretability. A new attention-based interpretability method called CLaSsification-Attention (CLS-A) is proposed. A human-grounded experiment is conducted to evaluate and compare CLS-A to other interpretability methods. The experimental protocol relies on the capacity of an interpretability method to provide explanations in line with human reasoning."
  href="https://arxiv.org/abs/1234.56789"
/>





