"use strict";(self.webpackChunkeki_lab=self.webpackChunkeki_lab||[]).push([[9343],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>p});var n=a(67294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var c=n.createContext({}),l=function(e){var t=n.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},d=function(e){var t=l(e.components);return n.createElement(c.Provider,{value:t},e.children)},m="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,r=e.originalType,c=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),m=l(a),u=i,p=m["".concat(c,".").concat(u)]||m[u]||h[u]||r;return a?n.createElement(p,o(o({ref:t},d),{},{components:a})):n.createElement(p,o({ref:t},d))}));function p(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=a.length,o=new Array(r);o[0]=u;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[m]="string"==typeof e?e:i,o[1]=s;for(var l=2;l<r;l++)o[l]=a[l];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},30914:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>l});var n=a(87462),i=(a(67294),a(3905));const r={title:"Exploring the links between creative execution and marketing effectiveness - Part II Custom trained Detectron2 for OD",author:"Marina Bermejo Sarmiento, Monica Brondholt Sorensen, Karin Sasaki",author_title:"Data Scientist Consultant",author_url:"mailto:inno@ekimetrics.com",header_image_url:"./img/blog/Eki_Meta_part_II.png",tags:["Object Detection","Optical Character Recognition","Marketing Mix Modelling","Deep Learning","Tesseract"],draft:!1,description:"In this Part II we explore the methodology for training Detectron2 models to detect brand-specific object in creative images.",keywords:["Data Science","EkiLab","Ekimetrics","Eki.Lab","Eki","Machine Learning","Artificial Intelligence","Data Science for business","Operational Research","Optimization","Knapsack problem","Deep Reinforcement Learning"]},o=void 0,s={permalink:"/blog/2022/11/30/creative_execution_and_marketing_effectiveness_part_II",source:"@site/blog/2022-11-30-creative_execution_and_marketing_effectiveness_part_II.md",title:"Exploring the links between creative execution and marketing effectiveness - Part II Custom trained Detectron2 for OD",description:"In this Part II we explore the methodology for training Detectron2 models to detect brand-specific object in creative images.",date:"2022-11-30T00:00:00.000Z",formattedDate:"November 30, 2022",tags:[{label:"Object Detection",permalink:"/blog/tags/object-detection"},{label:"Optical Character Recognition",permalink:"/blog/tags/optical-character-recognition"},{label:"Marketing Mix Modelling",permalink:"/blog/tags/marketing-mix-modelling"},{label:"Deep Learning",permalink:"/blog/tags/deep-learning"},{label:"Tesseract",permalink:"/blog/tags/tesseract"}],readingTime:6.91,hasTruncateMarker:!0,authors:[{name:"Marina Bermejo Sarmiento, Monica Brondholt Sorensen, Karin Sasaki",title:"Data Scientist Consultant",url:"mailto:inno@ekimetrics.com"}],frontMatter:{title:"Exploring the links between creative execution and marketing effectiveness - Part II Custom trained Detectron2 for OD",author:"Marina Bermejo Sarmiento, Monica Brondholt Sorensen, Karin Sasaki",author_title:"Data Scientist Consultant",author_url:"mailto:inno@ekimetrics.com",header_image_url:"./img/blog/Eki_Meta_part_II.png",tags:["Object Detection","Optical Character Recognition","Marketing Mix Modelling","Deep Learning","Tesseract"],draft:!1,description:"In this Part II we explore the methodology for training Detectron2 models to detect brand-specific object in creative images.",keywords:["Data Science","EkiLab","Ekimetrics","Eki.Lab","Eki","Machine Learning","Artificial Intelligence","Data Science for business","Operational Research","Optimization","Knapsack problem","Deep Reinforcement Learning"]},prevItem:{title:"Exploring the links between creative execution and marketing effectiveness - Part III: Tesseract Pre-Trained Optical Character Recognition Models",permalink:"/blog/2022/12/13/creative_execution_and_marketing_effectiveness_part_III"},nextItem:{title:"Exploring the links between creative execution and marketing effectiveness - Part I: Detectron2 Pre-Trained Object Detection Models",permalink:"/blog/2022/11/10/creative_execution_and_marketing_effectiveness_part_I"}},c={authorsImageUrls:[void 0]},l=[{value:"Why you should read this",id:"why-you-should-read-this",level:2},{value:"Dataset",id:"dataset",level:2},{value:"Algorithm",id:"algorithm",level:2},{value:"Hyperparameter Tuning",id:"hyperparameter-tuning",level:2},{value:"Process",id:"process",level:3},{value:"Results",id:"results",level:3},{value:"Useful links",id:"useful-links",level:3},{value:"Other useful code",id:"other-useful-code",level:3},{value:"Imports",id:"imports",level:4},{value:"Next article",id:"next-article",level:2}],d={toc:l},m="wrapper";function h(e){let{components:t,...r}=e;return(0,i.kt)(m,(0,n.Z)({},d,r,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("div",{align:"center"},(0,i.kt)("p",null,"  ",(0,i.kt)("img",{alt:"screenshot-app ",src:a(34076).Z,width:"4928",height:"3264"}))),(0,i.kt)("div",{align:"justify"},(0,i.kt)("p",null,"This article is ",(0,i.kt)("strong",{parentName:"p"},"Part II")," of a set of five technical articles that accompany a ",(0,i.kt)("a",{parentName:"p",href:"https://ekimetrics.com/news-and-events/exploring-the-links-between-creative-execution-and-marketing-effectiveness-exclusivepreview"},"whitepaper")," written in collaboration between Meta and Ekimetrics. Object Detection (OD) and Optical Character Recognition (OCR) were used to detect specific features in creative images, such as faces, smiles, text, brand logos, etc. Then, in combination with impressions data, marketing mix models were used to investigate what objects, or combinations of objects in creative images in marketing campaigns, drive higher ROIs.\nIn this Part II we explore the methodology for training Detectron2 models to detect brand-specific object in creative images.")),(0,i.kt)("h2",{id:"why-you-should-read-this"},"Why you should read this"),(0,i.kt)("div",{align:"justify"},"This article is mostly directed to machine learning practitioners. Here you will find a practical application of object detection algorithms; we present different open-source resources, comparisons and trade-offs in model selection for specific objects, methodology to improve performance for custom datasets, and how the object detection is then used to make inferences on the impact of creatives in marketing strategies."),(0,i.kt)("h2",{id:"dataset"},"Dataset"),(0,i.kt)("div",{align:"justify"},"Our dataset contained >50k image and video creatives across four different brands. The main goal of the custom object detection (OD) was to detect three types of brand-specific objects - logo, product and brand cue - as well as faces and smiles. Initially, faces and smiles were detected by pre-trained algorithms such as Haar cascades and Dlib, but due to the poor performance, it was decided to use a custom algorithm.",(0,i.kt)("p",null,"Before beginning the OD process, videos were converted to images by extracting every tenth frame. Training and Validation sets were then created using the Microsoft Azure Machine Learning Studio labelling tool. The labels were then converted to COCO format, and registered in Detectron as custom COCO libraries. Read more about this in ",(0,i.kt)("a",{parentName:"p",href:"https://ekimetrics.github.io/blog/2022/11/10/creative_execution_and_marketing_effectiveness_part_I"},"Part I"),". "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"## Registering COCO format datasets\nfrom detectron2.data.datasets import register_coco_instances\n\nregister_coco_instances(train, {'thing_classes': train_metadata.thing_classes, 'thing_dataset_id_to_contiguous_id': train_metadata.thing_dataset_id_to_contiguous_id}, train_json_path, TRAINING_IMAGES_PATH)\nregister_coco_instances(valid, {'thing_classes': valid_metadata.thing_classes, 'thing_dataset_id_to_contiguous_id': valid_metadata.thing_dataset_id_to_contiguous_id}, valid_json_path, VALID_IMAGES_PATH)\n"))),(0,i.kt)("h2",{id:"algorithm"},"Algorithm"),(0,i.kt)("div",{align:"justify"},"The Faster Region-based Convolutional Neural Network (R-CNN) model, Faster R-CNN X 101 32x8d FPN 3x, was used for the custom OD model.",(0,i.kt)("p",null,"One model was trained per object per brand using the manually labelled training sets. For detecting faces and smiles, the training set consisted of creatives from all four brands, but each model was developed separately for face and smile per brand. In total there were, thus, 19 custom models. The validation set was used to tune hyperparameters of each model, with accuracy as the main metric. The final models were then used to detect objects in the unlabelled images for all four brands. The training, validation and final detections were all done using a single node GPU (CUDA) on Databricks.")),(0,i.kt)("h2",{id:"hyperparameter-tuning"},"Hyperparameter Tuning"),(0,i.kt)("h3",{id:"process"},"Process"),(0,i.kt)("div",{align:"justify"},"While Detectron2 allows for the customization of many configurations, including learning rate, backbone, image size, and number of images per batch, we limited the scope to just three parameters due to time constraints. These are summarized in Table 1 along with the parameter values tested. The choice of parameters was based on those deemed to be the most influential in performance. In addition to these parameters, the learning rate \u2013 the rate at which the algorithm converges to a solution   \u2013 was customized.  Rather than using the default value of 0.001, the learning rate was determined by the linear learning rate scaling rule. This was done in order to facilitate training on larger batch sizes."),(0,i.kt)("p",null,"\xa0"),(0,i.kt)("div",{align:"center"},(0,i.kt)("p",null," ",(0,i.kt)("img",{alt:"screenshot-app",src:a(94311).Z,width:"1015",height:"268"})),(0,i.kt)("p",null,"Table 1: Parameters Tested in Custom Models")),(0,i.kt)("br",null),(0,i.kt)("p",null,"Example Code"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'## Parameters values to test\nparameters = {\n    \'SOLVER.MAX_ITER\': [300, 500, 1000],\n    \'ROI_HEADS.BATCH_SIZE_PER_IMAGE\': [265, 512, 1024]\n}\n\n## Configuring the algorithmn\ndef config_detectron(train_dataset, max_iter, batchsize):\n    classes = MetadataCatalog.get(train_dataset).thing_classes\n    num_classes = len(classes)\n    print(f"Number of classes in dataset: {num_classes}")\n    cfg = get_cfg()\n    cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"))\n    cfg.DATASETS.TRAIN = (train_dataset, )\n    cfg.DATASETS.TEST = ()\n    cfg.DATALOADER.NUM_WORKERS = 0\n    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml") # Let training initialize from model zoo\n    cfg.SOLVER.IMS_PER_BATCH = 2 ## # How many images per batch? The original models were trained on 8 GPUs with 16 images per batch, since we have 1 GPUs: 16/8 = 2 (we actually have 2 GPUs but we cannot use both as we do not have enough CUDA memory)\n    cfg.SOLVER.BASE_LR = 0.00125 # We do the same calculation with the learning rate as the GPUs, the original model used 0.01, so we\'ll divide by 8: 0.01/8 = 0.00125. \n    cfg.SOLVER.MAX_ITER = max_iter   # How many iterations are we going for? \n    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = batchsize\n    cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes\n    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n    print(dbutils.fs.ls(cfg.OUTPUT_DIR))\n    return cfg\n\n### Training\ndef train_detectron(config):\n    trainer = DefaultTrainer(config)\n    trainer.resume_or_load(resume=False)\n    trainer.train()\n    return trainer\n')),(0,i.kt)("div",{align:"justify"},"Exhaustive Grid Search was used to determine the combination of Max Iterations and Batch Size that yielded the highest accuracy on the validation set for each model. The results were visualized on a heat map, with confidence thresholds of 20%, 40%, 60%, and 80%. An example is shown in Figure 1. In this example, the model with Max Iteration of 1000 and a Batch Size of 265 performed the best, and the results did not improve beyond a confidence threshold of 40%. Confusion Matrices (see example in Figure 2) were also used to gain insight into the False Positive vs. False Negative rate."),(0,i.kt)("p",null,"\xa0"),(0,i.kt)("div",{align:"center"},(0,i.kt)("p",null," ",(0,i.kt)("img",{alt:"screenshot-app",src:a(9365).Z,width:"417",height:"347"})),(0,i.kt)("p",null,"Figure 1 : Example Results of Grid Search")),(0,i.kt)("br",null),(0,i.kt)("div",{align:"center"},(0,i.kt)("p",null," ",(0,i.kt)("img",{alt:"screenshot-app",src:a(37005).Z,width:"1082",height:"373"})),(0,i.kt)("p",null,"Figure 2 : Example Confusion Matrices for Best Performing Model with 80% Confidence Threshold")),(0,i.kt)("br",null),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'## Prediction\ndef prediction(config, test_dataset, date_string, threshold):\n    cfg = config\n    cfg.MODEL.WEIGHTS = f\'/dbfs/mnt/trd/{brand}/objectdetection/custom_output/GPU/{date_string}/model_final.pth\'\n    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = threshold # set the testing threshold for this model\n    cfg.DATASETS.TEST = (test_dataset, )\n    predictor = DefaultPredictor(cfg)\n    return predictor\n\ndef evaluation(config, test_dataset, trainer):\n    # Setup an evaluator, we use COCO because it\'s one of the standards for object detection: https://detectron2.readthedocs.io/modules/evaluation.html#detectron2.evaluation.COCOEvaluator\n    evaluator = COCOEvaluator(dataset_name=test_dataset, \n                              cfg=config, \n                              distributed=False, \n                              output_dir="./output/")\n\n    # Create a dataloader to load in the test data (cmaker-fireplace-valid)\n    val_loader = build_detection_test_loader(config, \n                                             dataset_name=test_dataset)\n\n    # Make inference on the validation dataset: https://detectron2.readthedocs.io/tutorials/evaluation.html\n    inference = inference_on_dataset(model=trainer, # get the model from the trainer\n                         data_loader=val_loader, \n                         evaluator=evaluator)\n    return inference\n\n## Make Predictions\ndef get_predictions(predictor, imagePath):\n        \n    # Get predictions\n    image = cv2.imread(imagePath)\n    predictions = predictor(image)\n\n    instances = predictions["instances"]\n    class_indexes = instances.pred_classes\n    prediction_boxes = instances.pred_boxes\n\n    class_catalog = valid_metadata.thing_classes\n    class_labels = [class_catalog[i] for i in class_indexes] \n\n    class_scores = instances.scores\n\n    return class_indexes, prediction_boxes, class_labels, class_scores\n')),(0,i.kt)("h3",{id:"results"},"Results"),(0,i.kt)("div",{align:"justify"},"The best performing model was then used to determine the optimal threshold for each object type, at 1% intervals ranging from 20% to 99%. The threshold yielding the highest accuracy for each object class per model was then selected. The results are summarized in Table 2. Following these results, the best custom-trained model per object per brand was used to detect object in the unlabelled dataset in batches of 200 creatives. This was done in order to limit the CUDA memory and ensure that work was saved along the way should something happen. The total detection time for each brand ranged between 72-168 hours."),(0,i.kt)("p",null,"\xa0"),(0,i.kt)("div",{align:"center"},(0,i.kt)("p",null,(0,i.kt)("img",{alt:"screenshot-app",src:a(85139).Z,width:"723",height:"460"})),(0,i.kt)("p",null,"Table 2: Final Custom Models")),(0,i.kt)("br",null),(0,i.kt)("h3",{id:"useful-links"},"Useful links"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/facebookresearch/detectron2"},"Detectron2 - Github")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://detectron2.readthedocs.io/en/latest/index.html"},"Detectron2 - Documentation")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md"},"Detectron2 - ModelZoo"))),(0,i.kt)("h3",{id:"other-useful-code"},"Other useful code"),(0,i.kt)("h4",{id:"imports"},"Imports"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"# Install Detectron2 dependencies: https://detectron2.readthedocs.io/tutorials/install.html (use cu100 because colab is on CUDA 10.0)\n!pip install -U torch==1.4+cu100 torchvision==0.5+cu100 -f https://download.pytorch.org/whl/torch_stable.html \n!pip install cython pyyaml==5.1\n!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n\n!pip install awscli # you'll need this if you want to download images from Open Images (we'll see this later)\n\n# Make sure we can import PyTorch (what Detectron2 is built with)\nimport torch, torchvision\ntorch.__version__\n!gcc --version\n!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu100/index.html\n\n# Setup detectron2 logger\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger() # this logs Detectron2 information such as what the model is doing when it's training# import some common detectron2 utilities\n\n# Other detectron2 imports\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import build_detection_test_loader\nfrom detectron2 import model_zoo \nfrom detectron2.engine import DefaultPredictor \nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer \nfrom detectron2.data import MetadataCatalog \n")),(0,i.kt)("h2",{id:"next-article"},"Next article"),(0,i.kt)("p",null,"In the next article, we will showcase Tesseract, a open-source optical character recognition (OCR) Engine, and the image-processing methods we developed to raise the baseline performance of this library, from 68% accuracy, by up to 28 percentage points."))}h.isMDXComponent=!0},34076:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/Eki_Meta_part_II-34e9070bebe22829eed217b330dd3164.png"},94311:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/1-63bf2e1d86f975c672b49017135f6ec8.png"},85139:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/2-2d2d4dac8274023a1627443f50d359e9.png"},9365:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/3-59df6df54647bdb3ecb7a9625ce33b5c.png"},37005:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/4_2-cfad45d3ddbbd7a950805c077bd28cb1.png"}}]);