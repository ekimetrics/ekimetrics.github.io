"use strict";(self.webpackChunkeki_lab=self.webpackChunkeki_lab||[]).push([[7333],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>u});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},d=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},p="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},g=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),p=c(n),g=r,u=p["".concat(l,".").concat(g)]||p[g]||m[g]||i;return n?a.createElement(u,o(o({ref:t},d),{},{components:n})):a.createElement(u,o({ref:t},d))}));function u(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=g;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[p]="string"==typeof e?e:r,o[1]=s;for(var c=2;c<i;c++)o[c]=n[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}g.displayName="MDXCreateElement"},30112:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var a=n(87462),r=(n(67294),n(3905));const i={title:"Exploring the links between creative execution and marketing effectiveness - Part III: Tesseract Pre-Trained Optical Character Recognition Models",author:"Marina Bermejo Sarmiento, Monica Brondholt Sorensen, Karin Sasaki",author_title:"Data Scientist Consultant",author_url:"mailto:inno@ekimetrics.com",header_image_url:"./img/blog/Eki_Meta_part_III.png",tags:["Object Detection","Optical Character Recognition","Marketing Mix Modelling","Deep Learning","Tesseract"],draft:!1,description:"In this Part III we explore the methodology for using Tesseract to detect text in creative images.",keywords:["Data Science","EkiLab","Ekimetrics","Eki.Lab","Eki","Machine Learning","Artificial Intelligence","Data Science for business","Operational Research","Optimization","Knapsack problem","Deep Reinforcement Learning"]},o=void 0,s={permalink:"/blog/2022/12/13/creative_execution_and_marketing_effectiveness_part_III",source:"@site/blog/2022-12-13-creative_execution_and_marketing_effectiveness_part_III.md",title:"Exploring the links between creative execution and marketing effectiveness - Part III: Tesseract Pre-Trained Optical Character Recognition Models",description:"In this Part III we explore the methodology for using Tesseract to detect text in creative images.",date:"2022-12-13T00:00:00.000Z",formattedDate:"December 13, 2022",tags:[{label:"Object Detection",permalink:"/blog/tags/object-detection"},{label:"Optical Character Recognition",permalink:"/blog/tags/optical-character-recognition"},{label:"Marketing Mix Modelling",permalink:"/blog/tags/marketing-mix-modelling"},{label:"Deep Learning",permalink:"/blog/tags/deep-learning"},{label:"Tesseract",permalink:"/blog/tags/tesseract"}],readingTime:7.685,hasTruncateMarker:!0,authors:[{name:"Marina Bermejo Sarmiento, Monica Brondholt Sorensen, Karin Sasaki",title:"Data Scientist Consultant",url:"mailto:inno@ekimetrics.com"}],frontMatter:{title:"Exploring the links between creative execution and marketing effectiveness - Part III: Tesseract Pre-Trained Optical Character Recognition Models",author:"Marina Bermejo Sarmiento, Monica Brondholt Sorensen, Karin Sasaki",author_title:"Data Scientist Consultant",author_url:"mailto:inno@ekimetrics.com",header_image_url:"./img/blog/Eki_Meta_part_III.png",tags:["Object Detection","Optical Character Recognition","Marketing Mix Modelling","Deep Learning","Tesseract"],draft:!1,description:"In this Part III we explore the methodology for using Tesseract to detect text in creative images.",keywords:["Data Science","EkiLab","Ekimetrics","Eki.Lab","Eki","Machine Learning","Artificial Intelligence","Data Science for business","Operational Research","Optimization","Knapsack problem","Deep Reinforcement Learning"]},prevItem:{title:"Exploring the links between creative execution and marketing effectiveness - Part IV: MMM for Creative Marketing Effectiveness",permalink:"/blog/2023/01/15/creative_execution_and_marketing_effectiveness_part_IV"},nextItem:{title:"Exploring the links between creative execution and marketing effectiveness - Part II Custom trained Detectron2 for OD",permalink:"/blog/2022/11/30/creative_execution_and_marketing_effectiveness_part_II"}},l={authorsImageUrls:[void 0]},c=[{value:"Detecting Text with Tesseract",id:"detecting-text-with-tesseract",level:2},{value:"Code snippets",id:"code-snippets",level:2},{value:"Install required libraries",id:"install-required-libraries",level:3},{value:"Import required libraries",id:"import-required-libraries",level:3},{value:"Processing images",id:"processing-images",level:3},{value:"Detect text",id:"detect-text",level:3},{value:"Manipulate bounding boxes",id:"manipulate-bounding-boxes",level:3},{value:"Useful Links",id:"useful-links",level:2},{value:"Next article",id:"next-article",level:2}],d={toc:c},p="wrapper";function m(e){let{components:t,...i}=e;return(0,r.kt)(p,(0,a.Z)({},d,i,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("div",{align:"center"},(0,r.kt)("p",null,"  ",(0,r.kt)("img",{alt:"screenshot-app ",src:n(10795).Z,width:"4095",height:"2726"}))),(0,r.kt)("div",{align:"justify"},(0,r.kt)("p",null,"This article is ",(0,r.kt)("strong",{parentName:"p"},"Part III")," of a set of five technical articles that accompany a ",(0,r.kt)("a",{parentName:"p",href:"https://ekimetrics.com/news-and-events/exploring-the-links-between-creative-execution-and-marketing-effectiveness-exclusivepreview"},"whitepaper")," written in collaboration between Meta and Ekimetrics. Object Detection (OD) and Optical Character Recognition (OCR) were used to detect specific features in creative images, such as faces, smiles, text, brand logos, etc. Then, in combination with impressions data, marketing mix models were used to investigate what objects, or combinations of objects in creative images in marketing campaigns, drive higher ROIs.\nIn this Part III we explore the methodology for using Tesseract to detect text in creative images.")),(0,r.kt)("h2",{id:"detecting-text-with-tesseract"},"Detecting Text with Tesseract"),(0,r.kt)("div",{align:"justify"},"Tesseract is an open-source optical character recognition (OCR) Engine that allows for the recognition of text characters within a digital image. It is an open-source resource, originally developed by Hewlett-Packard, and now managed by Google. This package does not have any parameters to optimise, but, as is exposed here, the basic performance of this resource can be improved with a combination of image processing and detected text correction.",(0,r.kt)("p",null,"In this work, Tesseract was used to detect text in all images, in a process outlined in Figure 1. Performance of this detection tool was done using Confusion Matrices, to gain insight not only into the Accuracy, but also other metrics such as the True Positive and True Negative Rates.  ")),(0,r.kt)("div",{align:"center"},(0,r.kt)("p",null,(0,r.kt)("img",{alt:"screenshot-app",src:n(69702).Z,width:"673",height:"265"})),(0,r.kt)("p",null,"Figure 1 - Three stage process by which the basic performance (accuracy) of Tesseract on the original images was improved by up to 28% points.")),(0,r.kt)("br",null),(0,r.kt)("div",{align:"justify"},"To start with, the functionality was used on the original images to measure a baseline for performance on our sets of images. From this first step, we derived the following learnings:",(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"For general Text (both non-promotional Text and Promotional Text), the Accuracy is 69%, the True Positive Rate is around 65%, the False Positive Rate is about 35%."),(0,r.kt)("li",{parentName:"ul"},"Tesseract does not recognise symbols such as % (which may indicate promotional text) accurately."),(0,r.kt)("li",{parentName:"ul"},"Tesseract does not perform well on images that have a busy background."),(0,r.kt)("li",{parentName:"ul"},"Tesseract does not recognise slanted Text.")),(0,r.kt)("p",null,"Based on the low total volume of Promotional Text in our dataset, as well as the poor performance of Tesseract to detect Promotional Text, the decision was made to remove Promotional Text as one of the desired objects to study."),(0,r.kt)("p",null,"Building on those learnings, we implemented a pipeline, outlined in Figure 2, that would help us improve the performance from baseline, by up to 28 percentage points on Accuracy : ")),(0,r.kt)("div",{align:"center"},(0,r.kt)("p",null,(0,r.kt)("img",{alt:"screenshot-app",src:n(66378).Z,width:"535",height:"308"})),(0,r.kt)("p",null,"Figure 2 - Pipeline for pre-processing images before Optical Character Recognition Models, and correcting detected text.")),(0,r.kt)("br",null),(0,r.kt)("div",{align:"justify"},(0,r.kt)("p",null,"For Step 1, pre-processing methods included the following, as well as combinations of those (illustrated in Figure 3): ",(0,r.kt)("strong",{parentName:"p"},"Binary Threshold, Sharpen, Normalisation, Histogram Equalisation, Histogram Adaptive Equalisation"),". As can be imagined, a processing method that enhances the performance of the detection method for one image, might decrease it for different images. Therefore, to ensure we obtained the highest accuracy for all images, we applied all pre-processing methods to all images, running Tesseract on each modified image, and keeping track of the text detected in each iteration. ")),(0,r.kt)("div",{align:"center"},(0,r.kt)("p",null," ",(0,r.kt)("img",{alt:"screenshot-app",src:n(32491).Z,width:"840",height:"414"})),(0,r.kt)("p",null,"Figure 3 - Effect of various image processing methods on the original. Images for illustrative purposes. Original images sourced from ",(0,r.kt)("a",{href:"https://unsplash.com/s/photos/coca-cola",target:"_top"},"unsplash.com"))),(0,r.kt)("br",null),(0,r.kt)("div",{align:"justify"},"In Step 2 we use Tesseract to detect text in the pre-processed images. Step 3 consisted of removing any \u201cincorrect\u201d text, such as text inside the bounding boxes of Logo or Product objects, if the text was shorter than a threshold length (which depended on the brand), if it consisted of keywords, such as the brand name, or if it had ASCII characters.",(0,r.kt)("p",null,"With the current methodology (figure 2), the accuracy cannot be improved any further by changing the length of text that is considered to be \u201ctrue\u201d Text. Furthermore, there is a trade-off between the True Negative Rate and the True Positive Rate, which helped us choose a threshold for the length of string to accept: Since Accuracy does not improve further after length 4, but this value does optimise the other two metrics, 4 characters long is the threshold chosen. ")),(0,r.kt)("br",null),(0,r.kt)("div",{align:"justify"},"The performance of Tesseract may be improved further by adding another step to the pipeline in figure 2, where the image is rotated several degrees, and after each rotation, text is detected. Due to time constraint issues, and the fact that we had already achieved a significant improvement above baseline and reached an accuracy of high 90s for some of the brands, we did not implement this step."),(0,r.kt)("h2",{id:"code-snippets"},"Code snippets"),(0,r.kt)("h3",{id:"install-required-libraries"},"Install required libraries"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"# To install tesseract\n# !sh apt-get -f -y install tesseract-ocr \n!sudo apt-get install tesseract-ocr -y\n\n# To install pytesseract\n!pip3 install pytesseract\n")),(0,r.kt)("h3",{id:"import-required-libraries"},"Import required libraries"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"import pytesseract\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport cv2\nfrom PIL import Image\n")),(0,r.kt)("h3",{id:"processing-images"},"Processing images"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"def normalise_img(image):\n            \n    # normalise original and detect\n    norm_img = np.zeros((image.shape[0], image.shape[1]))\n    normalised = cv2.normalize(image, norm_img, 0, 255, cv2.NORM_MINMAX)\n    normalised = cv2.threshold(normalised, 100, 255, cv2.THRESH_BINARY)[1]\n    normalised = cv2.GaussianBlur(normalised, (1, 1), 0)\n    \n    return normalised\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"def threshold_image(image, threshold=200):\n    image = cv2.threshold(image, threshold, 255, cv2.THRESH_BINARY)[1]\n    return image\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"def remove_noise_and_smooth(image):\n\n    # convert to grayscale\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # blur\n    blur = cv2.GaussianBlur(gray, (0,0), sigmaX=33, sigmaY=33)\n\n    # divide\n    divide = cv2.divide(gray, blur, scale=255)\n\n    # otsu threshold\n    thresh = cv2.threshold(divide, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n\n    # apply morphology\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n    morph = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n\n    return morph\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"def im_to_gray(image):\n    # convert to grayscale \n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    return gray\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"def create_binary(image):\n    # create binary - converting a colored image (RGB) into a black and white image - Adaptive binarization works based on the features of neighboring pixels (i.e) local window.\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    binary = cv2.threshold(image ,130,255,cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n    return binary\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"def hist_equaliser(image):\n    # Image Contrast and Sharpness: histogram equalization\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    equalised = cv2.equalizeHist(gray)\n    return equalised\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"def adaptive_hist_equaliser(image):\n    # Image Contrast and Sharpness: adaptive histogram equalization\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    equalised = clahe.apply(gray)\n    return equalised\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"def invert_image():\n    # invert the image - Color Inversion when different regions have different Foreground and Background colors\n    inverted = cv2.bitwise_not(image)\n    return inverted\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"def im_blur(image):\n    # Blurring or Smoothing \n    averageBlur = cv2.blur(image, (5, 5))\n    return\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"def im_gaussianBlur(image):\n    # Blurring or Smoothing (Gaussian Blur)\n    gaussian = cv2.GaussianBlur(image, (3, 3), 0)\n    return\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"def im_medianBlur(image):\n    # Blurring or Smoothing (Median Blur (good at removing salt and pepper noises))\n    medianBlur = cv2.medianBlur(image, 9)\n    return\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"def im_bilateralBlur(image):\n    # Blurring or Smoothing (Bilateral Filtering)\n    bilateral = cv2.bilateralFilter(image, 9, 75, 75)\n    return image\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'def unsharp_mask(image, kernel_size=(5, 5), sigma=1.0, amount=1.0, threshold=0):\n    """Return a sharpened version of the image, using an unsharp mask."""\n    blurred = cv2.GaussianBlur(image, kernel_size, sigma)\n    sharpened = float(amount + 1) * image - float(amount) * blurred\n    sharpened = np.maximum(sharpened, np.zeros(sharpened.shape))\n    sharpened = np.minimum(sharpened, 255 * np.ones(sharpened.shape))\n    sharpened = sharpened.round().astype(np.uint8)\n    if threshold > 0:\n        low_contrast_mask = np.absolute(image - blurred) < threshold\n        np.copyto(sharpened, image, where=low_contrast_mask)\n    return sharpened\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"def apply_filter_sharpen(image):\n    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n    image = cv2.filter2D(image, -1, kernel)\n    return image\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"def im_erode_dilate(image):\n    # apply noise reduction techniques like eroding, dilating\n    kernel = np.ones((2,2),np.uint8)\n    image = cv2.erode(image, kernel, iterations = 1)\n    image = cv2.dilate(image, kernel, iterations = 1)\n    return image\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'def process_image(image):\n\n    """\n    Example of a pipeline to pre-process an image.\n    """\n    \n    # convert to grayscale \n    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n     # create binary\n    image = cv2.threshold(image ,130,255,cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n\n    # invert the image\n    image = cv2.bitwise_not(image)\n\n    # apply noise reduction techniques like eroding, dilating\n    kernel = np.ones((2,2),np.uint8)\n    image = cv2.erode(image, kernel, iterations = 1)\n    image = cv2.dilate(image, kernel, iterations = 1)\n    \n    return image\n    \n')),(0,r.kt)("h3",{id:"detect-text"},"Detect text"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"def detect_text(image, brand_name, t, bboxes_file, verbose=True):\n    data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n    bboxes = get_bbox_pts(data)\n    data_out, bboxess, tt = check_text_ok(data, brand_name)\n    \n    return data, bboxes\n")),(0,r.kt)("h3",{id:"manipulate-bounding-boxes"},"Manipulate bounding boxes"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"def get_bbox_pts(data):\n\n    \"\"\"\n    From the output of the text detection model, extract the [x1,x2,y1,y2] points.\n    \"\"\"\n    data['x1'] = []\n    data['x2'] = []\n    data['y1'] = []\n    data['y2'] = []\n    for i, txt in enumerate(data['text']):\n    \n        # extract the width, height, top and left position for that detected word\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n        l = data[\"left\"][i]\n        t = data[\"top\"][i]\n        \n        # define all the surrounding box points\n        x1 = l\n        x2 = l + w\n        y1 = t\n        y2 = t + h\n\n        data['x1'].append(x1)\n        data['x2'].append(x2)\n        data['y1'].append(y1)\n        data['y2'].append(y2)\n        \n    return data\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'def transform_bbox(bbox):\n\n    """\n    Transform Azure labels of valid set from [x,y,w,h] (in absolute numbers, prev. transformed) to [x,y,x1,y1].\n    """\n    x = bbox[0]\n    y = bbox[1]\n    w = bbox[2]\n    h = bbox[3]\n    \n    x1 = x + w\n    y1 = y + h\n\n    return [x,y,x1,y1]\n')),(0,r.kt)("h2",{id:"useful-links"},"Useful Links"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/tesseract-ocr"},"Tesseract - Github")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://tesseract-ocr.github.io/tessdoc/Home.html"},"Tesseract - User Manual")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://pypi.org/project/pytesseract/"},"Pytesseract - Pypi"))),(0,r.kt)("h2",{id:"next-article"},"Next article"),(0,r.kt)("p",null,"In the next article, we outline the MMM process, focusing on how the creative elements are used as variables in a multivariate regression, as well as how the final ROIs were calculated."))}m.isMDXComponent=!0},10795:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/Eki_Meta_part_III-7d188e074342397f2ddb6fb21a645ae7.png"},69702:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/10-dc93d16eaa104bf23fe228afaf7bd6a2.PNG"},66378:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/11-435bc20c4e2284716dcd7b9565a42098.PNG"},32491:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/12-bdf7d0b4655f39b30eaa56e906ef8e6d.PNG"}}]);