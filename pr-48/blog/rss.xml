<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Eki.Lab Blog</title>
        <link>https://ekimetrics.github.io/blog</link>
        <description>Eki.Lab Blog</description>
        <lastBuildDate>Tue, 10 Jun 2025 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Under the Hood: Technical Blueprint of a GenAI-Powered ESG Due Diligence Tool]]></title>
            <link>https://ekimetrics.github.io/blog/Under_The_Hood</link>
            <guid>https://ekimetrics.github.io/blog/Under_The_Hood</guid>
            <pubDate>Tue, 10 Jun 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[This paper introduces the Client ESG Due Diligence Tool, an automated solution that evaluates sustainability risks and opportunities of working with clients, whatever the industry they operate in.]]></description>
            <content:encoded><![CDATA[<div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-technical-implementation">The Technical Implementation<a href="https://ekimetrics.github.io/blog/Under_The_Hood#the-technical-implementation" class="hash-link" aria-label="Direct link to The Technical Implementation" title="Direct link to The Technical Implementation">​</a></h2><p>As mentioned above, the UI tools are:</p><ul>
<li>
<p>The Document Upload Dashboard with the following functionalities:</p>
<ul>
<li>Company file upload</li>
<li>Company score generation</li>
<li>Company report generation</li>
<li>A chatbot that answers questions specific to a company and based only on their documentation</li>
</ul>
</li>
<li>
<p>The Global Assessment Dashboard – which enables management, commercial, and sustainability leads to explore and understand the overall ESG performance of our client-base, at a glance.
In the following pages we explore in detail the tech-stack and technical implementation of: the UI, the backend of the Document Upload Dashboard, and the Global Assessment Dashboard.</p>
</li>
</ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-tech-stack">The Tech-Stack<a href="https://ekimetrics.github.io/blog/Under_The_Hood#the-tech-stack" class="hash-link" aria-label="Direct link to The Tech-Stack" title="Direct link to The Tech-Stack">​</a></h3><p>The Document Upload Dashboard is powered on the backend by the following resources:</p><ul>
<li>Streamlit – For building the front-end UI</li>
<li>Azure App Service – Hosts the webapp</li>
<li>Azure App Service Plan – Defines the computing power and scaling rules for the App Service, ensuring smooth performance.</li>
<li>Azure Storage Account – Where all the uploaded company documents, generated reports, and the due diligence questionnaire are stored.</li>
<li>Eki.Parser – Extracts text from company documents (PDFs, reports, etc.). Library created by Ekimetrics.</li>
<li>Azure Databricks – Hosts the Python functionality that processes the documents, generates vector embeddings (numerical representations of text for better searching), and stores them for efficient retrieval. It also sends requests to GPT-4.o API to answer the pre-defined questions.</li>
<li>Azure Container Registry – Stores Docker images for the front end.</li>
<li>Docker - A containerization platform that packages applications and their dependencies into isolated environments called containers, making it easier to deploy and run applications, as it ensures consistency of dependencies across different environments.</li>
<li>Azure Search Service – Acts as the "brain" for finding relevant documents. When a user asks a question, it looks for the most relevant information in the stored data.</li>
<li>Azure Key Vault – Stores and protects sensitive information such as API keys and passwords.</li>
<li>Azure OpenAI– Service that provides access to an GPT-4.o API -  the LLM that takes the retrieved documents and generates a well-formed response.</li>
<li>Bitbucket – A Git-based platform for source code management and version control. Bitbucket hosts the project repositories, tracks changes, and integrates with the CI/CD pipeline to automatically trigger builds, tests, and deployment processes when updates are made to the dev or main branches. We have two repositories, one for the UI, and the other for the backend of the Document Upload Dashboard.</li>
<li>Poetry – A Python dependency management tool that helps package and manage libraries, ensuring consistent dependencies across different environments.</li>
<li>Python – A high-level programming language widely used for data science, machine learning, web development, and automation, and known for its readability and extensive ecosystem of libraries. This is the only language we use to create the tool.</li>
</ul><p>The Global Assessment Dashboard is powered by:</p><ul>
<li>Storage Account – Where all the generated reports with the company scores are stored.</li>
<li>Power BI – A business analytics tool by Microsoft that enables the creation of interactive visualizations and reports.</li>
</ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-rag-architecture">The RAG architecture<a href="https://ekimetrics.github.io/blog/Under_The_Hood#the-rag-architecture" class="hash-link" aria-label="Direct link to The RAG architecture" title="Direct link to The RAG architecture">​</a></h3><p>RAG stands for Retrieval-Augmented Generation. It is an AI framework that leverages large language models (LLMs) to answer user queries more precisely. It works in three steps:</p><ul>
<li>Retrieval – Searches a knowledge base (e.g., databases, documents, or APIs) for relevant context, based on the user query.</li>
<li>Augmentation – Injects the retrieved data into the prompt to an LLM, to provide additional context.</li>
<li>Generation – Uses an LLM to generate a response, grounding answers in factual data.</li>
</ul><p>As such, RAGs reduce hallucinations by ensuring that responses are based on actual data rather than only the model’s pre-trained knowledge, and enhances explainability by providing references to the text used for generating the response. This architecture also leverages the state of the art LLMs, and avoids having to go through the expensive and complex process of training a specialised LLM.</p><p>Example use cases of RAG implementations include Ekimetrics’ Climate Q&amp;A [^2], and CLAIRE [^3]. Climate Q&amp;A simplifies access to environmental science by answering questions based on extensive scientific reports, including those from the IPCC and IPBES. CLAIRE is a multi-agent generative AI system that helps to democratize topics of Responsible AI. Both are part of the “digital communs” that Ekimetrics provides for free to the community.</p><p>Figure 6 shows the RAG architecture of the ESG Due Diligence tool. The flow begins with a set of predefined due diligence questions, stored as a file in Azure Blob storage, which are also retrievable on the UI to allow users to see the questions when they select the dedicated option. Next, we have users uploading company documents, which are stored in Azure Blob Storage, and are also made retrievable to allow the user to download them on demand (useful, for example, when a user might like to generate an updated company score that uses a recently published financial statement, and they want to see whether that document has already been used). The architecture of data and file storage follows Ekimetrics data engineering standards, with data layers. Next, parsing, chunking, summarising of chunks, embedding and indexing are deployed and raun in Databricks, as a Workflow that calls these functionalities (illustrated in Figure 7). Summaries of chunks are indexed by including meta data such as the company name, document type, document year, page number, etc, ensuring that the retrieval is company-specific, and in the future will facilitate other functionality such as year-specific report generation. With the Indexed Data available, we are ready to do two things:</p><ol>
<li>Generate the report for the company. That is, creating an Excel file with the answers to all the due diligence questions, based on the available company documents. The report also contains all the summarised chunks used to provide answers. This helps the user to check for themselves the references.  This is done as part of the first Databricks Workflow (figure 7).</li>
<li>To answer user queries prompted through the UI chatbot.</li>
</ol><p>The report generated for each company is stored in Azure Blob Storage. The report is then used in three following steps:</p><ol>
<li>It is made available for on-demand retrieval, so users can download the document</li>
<li>It is used for showing the company scores on the UI</li>
<li>It is used for updating the scores on the Global Assessment Dashboard (see dedicated section below)</li>
</ol></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/ESG_Figure_6A-fa624ac7f2a2bceed26e8f0e6efcb3b8.svg" width="602" height="338" class="img_ev3q">
<img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/ESG_Figure_6B-944d74bc29373aef563bfe3fcbe04b1f.svg" width="602" height="338" class="img_ev3q"></p><p><em>Figure 6 - RAG architecture( Data Preparation Workflow and Question Answering Workflow)</em></p><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/ESG_Figure_7-256e22d987ad61a386fadd8fcc38c783.png" width="901" height="274" class="img_ev3q"></p><p><em>Figure 7 - Databricks Workflow</em></p></div>
<div align="justify"><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="genai-and-rag-architecture-optimisations">GenAI and RAG Architecture Optimisations<a href="https://ekimetrics.github.io/blog/Under_The_Hood#genai-and-rag-architecture-optimisations" class="hash-link" aria-label="Direct link to GenAI and RAG Architecture Optimisations" title="Direct link to GenAI and RAG Architecture Optimisations">​</a></h3><p>We have made several optimisations to the baseline Ekimetrics RAG architecture. These are described below.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="reformulation-prompt">Reformulation prompt<a href="https://ekimetrics.github.io/blog/Under_The_Hood#reformulation-prompt" class="hash-link" aria-label="Direct link to Reformulation prompt" title="Direct link to Reformulation prompt">​</a></h4><p>We use a reformulation prompt which converts user input, and historical exchanges within the same session into a well-structured, standalone question. Our reformulation prompt begins by presenting a summary of the previous conversation to provide context. Then, it instructs the model to rewrite the user’s message as a concise, self-contained query in natural language (in this case, English). This structure ensures that the reformulated question is clear and contextually relevant. Reformulation improves document retrieval accuracy. The uplift of this approach still needs to be tested and quantified for our particular use case.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="embedding-of-summary-of-chunks">Embedding of summary of chunks<a href="https://ekimetrics.github.io/blog/Under_The_Hood#embedding-of-summary-of-chunks" class="hash-link" aria-label="Direct link to Embedding of summary of chunks" title="Direct link to Embedding of summary of chunks">​</a></h4><p>We embed summaries of chunks, instead of the raw chunks themselves, as this provides several advantages:</p><ol>
<li>Summaries are shorter, resulting in smaller embeddings, which means that vector searches are faster and storage costs are lower.</li>
<li>Raw text chunks often contain redundant or irrelevant details, but a summary distils the key information and encodes high-level meaning, making embeddings more focused, and leading to better semantic retrieval.</li>
<li>Feeding summaries, rather than raw chunks, to the context window for the LLM can lead to better generation.</li>
<li>It may lead to reduced hallucinations, as LLMs may generate hallucinations when retrieving overly large or unstructured chunks.</li>
</ol><p>Such a strategy is not applicable to a RAG in every context. For example, when using a RAG to cite legal text, it would be necessary to have specific details; summarisation would not be appropriate.</p><p>The uplift of this approach still needs to be tested and quantified for our particular use case.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="choice-of-model">Choice of model<a href="https://ekimetrics.github.io/blog/Under_The_Hood#choice-of-model" class="hash-link" aria-label="Direct link to Choice of model" title="Direct link to Choice of model">​</a></h4><p>We are using LLMs at four stages of the RAG process:</p><ol>
<li>Reformulation</li>
<li>Chunk summarisation</li>
<li>Generation</li>
<li>Testing framework</li>
</ol><p>We have used ChatGPT 4.o for all the stages due to lower costs and performance..  We plan on testing different models for different purposes, including using a superior model in a testing framework to judge the performance of ChatGPT 4.o in answering the questions.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="chatbot-prompts">Chatbot prompts<a href="https://ekimetrics.github.io/blog/Under_The_Hood#chatbot-prompts" class="hash-link" aria-label="Direct link to Chatbot prompts" title="Direct link to Chatbot prompts">​</a></h4><p>We have a structured prompting strategy for the LLM that answers, both, the due diligence questions and the chatbot user queries, that provides several advantages for the RAG. We force fact-based responses that are grounded on the documents provided. This ensures that responses are more accurate, and hallucinations are reduced. We provide the tool clear guidelines on how to handle ambiguity, for example, if a company does not meet a criterion or if it does not find relevant information, the model will state "No" as an answer to the question,  preventing the model from making up answers. We force the tool to provide structured answers, for example, clear ‘yes’/ ‘no’ responses, which, in turn, help the later stages in the pipeline to automate the scores calculations.</p><p>We instruct the tool to provide references, to ensure traceability and verifiability of facts. This is important, not only to provide confidence to the users to trust the output, but also, in fields such as finance, ESG reporting, or compliance, clear referencing and fact-based responses help meet regulatory requirements.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="parameter-value">Parameter value<a href="https://ekimetrics.github.io/blog/Under_The_Hood#parameter-value" class="hash-link" aria-label="Direct link to Parameter value" title="Direct link to Parameter value">​</a></h4><p>To improve the quality of the answers generated by the tool, several key parameters were optimized based on Faithfulness, Contextual Relevancy, and Answer Relevancy performance scores. These metrics are explained in detail in [GenAI Testing Framework section] of the article. We focus on specific parameters that impact how the system processes, retrieves, and generates responses.</p><ul>
<li>Optimising <strong>Chunking parameters</strong>: Chunking directly influences text segmentation and  encoding; it determines how the input text is divided into segments, affecting how much context is retained per segment, before each chunk is summarised, indexed and stored. The specific metrics optimised were the maximum chunk size (max_tokens), and the chunk_overlap, which is a parameter that ensures continuity between chunks, so that details are not lost.</li>
<li>Optimising <strong>Retrieval parameters</strong>: Retrieval parameters control how the tool selects relevant (summarised) chunks of text from the (indexed) database before generating answers. The parameter optimised is the number of relevant chunks retrieved for answering a question (top_k).</li>
<li>Optimising <strong>Generation parameters</strong>: The Generation Parameters controls the LLM output tuning, that is, how the model formulates responses, balancing creativity and precision. The parameters that have been tuned are the temperature, which makes the responses more deterministic and grounded on the resources available at lower values (close to 0), or more creative, at values closer to 1. The top_p parameter controls response diversity by selecting the most probable rather than random variations or words.</li>
</ul><p>When tuning the parameters for chunking, retrieval, and generation, different trade-offs emerge. The challenge is to find a balance that maximizes faithfulness, contextual relevancy, and answer relevancy without compromising efficiency or cost. For example, optimising chunk size is a trade-off between completeness vs. overload, where larger chunks (i.e. higher values for max_token and chunk_overlap) would improve between faithfulness (because we would have more context within a single pass) and improved contextual relevancy  (because it would reduce fragmentation), but it would incur higher computation costs, and have a risk of losing critical context and introducing irrelevant information as the chunks may contain unrelated data. Similarly, optimising retrieval depth is an exercise in striking the right balance between relevance and noise; retrieving more chunks (higher top_k), would provide more information and increase the chances of retrieving the right context, therefore improving answer relevancy; however, this would come at a cost of introducing contextual irrelevancy and slower performance due to having to process more data. Generation parameters define the balance between precision and diversity; lower temperature would ground answers on retrieved information, but would make the answers too rigid. Lower top_p would prioritise high-confidence responses, but may ignore less common but relevant insights.</p><p>Table 3A shows the optimised parameter values, and Tables 3B and 3C show performance metrics based on these parameter values. Noting that the scores on table 3B are the arithmetic averages of the scores across all the general (36) and industry specific (5) questions for one company. The total_score on table 3C is the average mean between the faithfulness_score and the answer_relevancy score. This global metric was chosen as it is a simple and intuitive way to summarize the performance of the whole tool and  provides a balanced assessment of both accuracy and direct usefulness.</p><p>When evaluating retrieval performance, in our case, it's important not to overemphasize <strong>contextual relevancy</strong>, as this metric assesses the proportion of retrieved sources that are pertinent to the query. If only some references are relevant, the score may appear low, even when the relevant information has been correctly extracted. In our case, we aim to determine whether all pertinent information has been extracted, even if, for example, only one out of ten documents is relevant. Ideally, if the language model utilizes that single relevant document appropriately, the contextual relevancy should be 100%. However, DeepEval's contextual relevancy metric evaluates the overall relevance of all extracted documents to the question, as an example, resulting in a score of 10% when only one out of ten documents is relevant. Creating a custom metric within DeepEval would better align with our evaluation objectives, and we plan to look into this in the future.</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="data:image/png;base64,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" width="336" height="51" class="img_ev3q"></p><p><em>Table 3A – Optimised parameters values</em></p><p><img decoding="async" loading="lazy" alt="screenshot-app " src="data:image/png;base64,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" width="635" height="53" class="img_ev3q"></p><p><em>Table 3B – Optimised performance metrics</em></p><p><img decoding="async" loading="lazy" alt="screenshot-app " src="data:image/png;base64,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" width="188" height="53" class="img_ev3q"></p><p><em>Table 3C – Total performance score of the tool</em></p></div>
<div align="justify"><p>As mentioned above, the optimisation of the metrics has so far been implemented based on one company, and using a grid search of a specific set of values for each parameter value, recommended by members of Ekimetrics’ GenAI Taskforce. More work will need to be done to optimise other parameter values, such as the temperature, as well as to take into account the answers across all industry sectors, and to evaluate the performance based on other considerations, such as number of documents available for example.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="genai-testing-framework">GenAI Testing framework<a href="https://ekimetrics.github.io/blog/Under_The_Hood#genai-testing-framework" class="hash-link" aria-label="Direct link to GenAI Testing framework" title="Direct link to GenAI Testing framework">​</a></h3><p>We have established an evaluation pipeline for assessing responses generated by the tool, using the DeepEval framework [^4]. We focus on three evaluation metrics: Faithfulness, Answer Relevancy and Context Relevancy. These three metrics have been prioritised for practical purposes, as they provide a comprehensive testing of the performance of both the retriever and the generator (Table 4), and, essentially, together they provide a measure of “reliability” of the answer.</p><p>Contextual Relevancy tests the relevance of the retrieved text to the user input. Mathematically it is calculated as shown below, hence taking values between 0 and 1. Higher values indicate that the retriever selects chunks that are relevant to the user query (and previous conversations due to the reformulation prompt – see the relevant section under “GenAI and RAG Architecture Optimisations”).</p><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/ESG_Formula_1-864d00445990c36ec8880fb404fc740e.png" width="751" height="91" class="img_ev3q"></p><p>Faithfulness assesses whether the response accurately reflects the provided source material. Mathematically it is calculated as shown below, therefore ranging between 0 and 1. Higher values indicate more factual responses.</p><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/ESG_Formula_2-e20cdb3cd25a3d4a744030e492dd5bb9.png" width="526" height="100" class="img_ev3q"></p><p>Answer Relevancy checks if the response appropriately answers the question.  Mathematically it is calculated as shown below, therefore ranging between 0 and 1. Higher values indicate more on point responses.</p><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/ESG_Formula_3-46d78406f41f13f506da661c9067b75a.png" width="709" height="94" class="img_ev3q"></p><p>Other metrics we considered implementing were Hallucination, Summarisation, and Faithfulness against the testing dataset (see table 3).</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="data:image/png;base64,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" width="903" height="154" class="img_ev3q"></p><p><em>Table 4 - RAG performance metrics; in blue are those implemented in our solution.</em></p></div>
<div align="justify"><p>The testing framework essentially consists of a function that iterates over the Pandas DataFrame containing the questions, answers and references that will be used to generate the company report; it scores the responses with the performance metrics, storing the result for each question-answer pair. Then, the arithmetic averages of each metric across all answers, as well as the arithmetic mean of the Faithfulness and Answer Relevancy metrics is also calculated – named the total score. Tables 3B and 3C show the values of these global metrics with the optimised parameter values in table 3A. Section GenAI and RAG Architecture Optimisations explains how the parameters were optimised.</p><p>The testing framework is integrated into the report-generating Databricks workflow (Figure 7). To evaluate parameters, we set a chosen parameter value and run the workflow for a company that has already been parsed and embedded. The output is an Excel file containing answers to each question, their respective scores on the three selected metrics, and a summary tab with overall scores and the parameter values used in that test run. This structure allows for easy comparison of parameters, scores, and responses across different test runs.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="packaged-solution-with-cicd">Packaged Solution with CI/CD<a href="https://ekimetrics.github.io/blog/Under_The_Hood#packaged-solution-with-cicd" class="hash-link" aria-label="Direct link to Packaged Solution with CI/CD" title="Direct link to Packaged Solution with CI/CD">​</a></h3><p>To enhance the scalability of our solution, we have transitioned from a manual deployment process to an automated, package-based approach. This shift improves maintainability, reduces human error, and streamlines the integration of new features, thus ensuring the stability and longevity of the tool.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="poc-version-of-the-tool">POC version of the tool<a href="https://ekimetrics.github.io/blog/Under_The_Hood#poc-version-of-the-tool" class="hash-link" aria-label="Direct link to POC version of the tool" title="Direct link to POC version of the tool">​</a></h4><p>The POC production pipeline ran directly on Databricks. This setup presented several risks: dependencies were partially listed in workflow tasks and partially installed dynamically during execution, so the process required manual management of dependencies. The POC also had environment file vulnerabilities as .env files were not versioned, making scripts sensitive to unintended modifications. There was no automated testing - updates would require manual testing. Version control was also not well established, creating versioning risks with later teams of developers.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-packaged-solution-with-cicd">The Packaged Solution with CI/CD<a href="https://ekimetrics.github.io/blog/Under_The_Hood#the-packaged-solution-with-cicd" class="hash-link" aria-label="Direct link to The Packaged Solution with CI/CD" title="Direct link to The Packaged Solution with CI/CD">​</a></h4><p>To address these challenges, we implemented a CI/CD pipeline in Bitbucket, which automates packaging and deployment to Databricks. The full pipeline is illustrated in Figure 8. The key improvements include a package-based deployment to Databricks, automated dependency management using Poetry and automatically made available to job clusters, bundled in the CI/CD pipeline, .env files were phased out in favour of environment variables managed in Bitbucket and Azure Keyvault (for secrets), and automated testing with a dedicated test stage in the Bitbucket pipeline which runs unit tests (and in the future integration tests) before allowing merges into dev or main (i.e. if tests fail, the merge is blocked).</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/ESG_Figure_8-a705cf47ebdf4796e2146f200d99cc36.svg" width="573" height="322" class="img_ev3q"></p><p><em>Figure 8 – Product packaging and CI/CD pipeline</em></p></div>
<div align="justify"><p>Now, updates to the code base will work as follows:</p><ol>
<li>Developers create a feature branch in Bitbucket’s dev workspace and start an interactive cluster where all dependencies are pre-installed.</li>
<li>Code changes are implemented, and if new dependencies are required, they are added to the Poetry configuration.</li>
<li>A merge request is submitted to the dev branch, triggering the CI/CD pipeline.</li>
<li>The CI/CD pipeline automatically:<!-- -->
<ul>
<li>Runs tests to validate the changes.</li>
<li>Packages the project and dependencies.
o	Updates the workflow tasks on Databricks dev.</li>
</ul>
</li>
<li>Testing is streamlined: once the pipeline finishes, the new version is ready to test.</li>
<li>Deployment to production: when merging dev into main, the same CI/CD pipeline executes, ensuring a smooth, error-free rollout.</li>
</ol><p>The uplift in efficiency of the tool translates to accelerated feature development and release cycles, as well as freeing up valuable development time to focus on innovation rather than maintenance.</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/ESG_Table_9-17d6671afe9b35ac5f85da6bfe3f2ac3.svg" width="602" height="338" class="img_ev3q"></p><p><em>Table 9 – Development workflow in packaged version of the solution.</em></p></div>
<div align="justify"><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="responsible-ai">Responsible AI<a href="https://ekimetrics.github.io/blog/Under_The_Hood#responsible-ai" class="hash-link" aria-label="Direct link to Responsible AI" title="Direct link to Responsible AI">​</a></h3><p>Ekimetrics’ Responsible AI Framework [5] ensures that AI models are developed and deployed with comprehensive awareness of their impact. The framework consists of seven key pillars: transparency, explicability, interpretability, security, robustness, vigilance, and sustainability. By applying these principles, Ekimetrics ensures we implement solutions that use AI responsibly, preventing misuse and ensuring they are ethical, reliable, and sustainable.</p><p>Our tool abides by these pillars in the following ways:</p><p>Transparency: This pilar refers to adopting ways to clearly communicate the actual way the tools is implemented, how the data is collected, and how it is envisioned to be used. Concretely, Ekimetrics suggests several approaches, including creating a model card  and releasing open source code and scientific papers. We start our journey with this pillar through the publication of this paper, with work on a model card under way.</p><p>Explicability: To adopt the explicability pillar we provide users with all the references, or retrieved data, that was used to generate each answer, both for the report and the chatbot functionality. Future implementation will include user feedback features.</p><p>Robustness: Robustness is about measuring and ensuring the performance of the system and its stability all along its life cycle, together with monitoring and retraining when necessary, as well as the ability to manage adversarial attacks. We have implemented a robust testing framework with through-out relevant performance metrics. The tool is designed to keep track of the performance of reports generated year-on-year.</p><p>Sustainability: is concerned with the environmental costs of running the solution. We have used Ekimetrics’ existing tool, the Carbon Tracking Dashboard, which helps us monitor the carbon footprint and costs of IT solutions ran on Azure resources. This tool enables optimised use of Databricks clusters, both for cost and carbon footprint management.</p><p>Security: The core concept of this pillar is to ensure the protection of personal and sensitive data. As no personal data is used in the tool, this is not a concern.</p><p>Vigilance: The purpose of this pilar is to avoid any possible harm to the users’ interests, especially to exclude discriminatory biases. One immediately applicable concern could be that the tool could provide more favourable scores to companies that have more, rather than less documentation provided to the tool. It would be important to also ensure that we study the correlation of the score against other company criteria, such as gender and ethnicity distribution of the company’s leadership, geography, company size, etc, to ensure there is no inherent bias. As of yet, we have not observed such bias, but a detailed study is outstanding.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="backend-of-the-global-assessment-tool">Backend of the Global Assessment Tool<a href="https://ekimetrics.github.io/blog/Under_The_Hood#backend-of-the-global-assessment-tool" class="hash-link" aria-label="Direct link to Backend of the Global Assessment Tool" title="Direct link to Backend of the Global Assessment Tool">​</a></h3><p>This is a sub-flow from that shown in Figure 6. Given all the company reports, the Data bricks Workflow in figure 7 finishes the last step and stores the report in Azure blog storage, which then feeds the Global Assessment Dashboard. The data is refreshed automatically on the web app.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="implementation-timeline-and-team-expertise">Implementation timeline and team expertise<a href="https://ekimetrics.github.io/blog/Under_The_Hood#implementation-timeline-and-team-expertise" class="hash-link" aria-label="Direct link to Implementation timeline and team expertise" title="Direct link to Implementation timeline and team expertise">​</a></h2><p>It took 80 FTE days to build this solution to the standard described above. This time represents a significant internal investment, mainly from Ekimetrics UK and Ekimetrics France, contributing to the global sustainability effort. The tool was built on existing Ekimetrics resources, such as the Eki-parser and a proprietary templated RAG architecture, which was optimised by the developer team for this particular use case. The team consisted of specialists in sustainability, infrastructure as code, data engineering and GenAI methodologies, as well as general data science consultants and developers.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="looking-ahead">Looking Ahead<a href="https://ekimetrics.github.io/blog/Under_The_Hood#looking-ahead" class="hash-link" aria-label="Direct link to Looking Ahead" title="Direct link to Looking Ahead">​</a></h2><p>We are committed to continuously improving the tool to enhance its usability, accuracy, and impact (table 5).</p><p>On the user adoption front, we will track user uptake metrics and integrate with our CRM to ensure the tool remains aligned with our customer base. Additionally, we plan to develop an automated gap analysis and recommendation engine to guide our teams, alongside year-on-year sustainability maturity assessments and benchmarking capabilities.</p><p>To refine model performance, we will assess the use of a reranker step, enhance the GenAI testing framework with automated monitoring, and improve prompting to ensure better alignment with sustainability topics. For tool performance, we will implement logging and monitoring, add background information on CSR regulations, and integrate insights on Ekimetrics’ offerings. From a data perspective, we will establish a structured database to support analysis and reporting, ensuring transparency and accountability through model cards. We will assess the GenAI performance metrics based on industry sectors, company size, geography, number of documents and other factors, to ensure we assess any bias.</p><p>We are also committed to making our Generative AI usage more sustainable by optimizing energy consumption and reducing unnecessary model usage. To achieve this, we will track environmental performance through the Green IT Dashboard, monitoring carbon footprint and energy consumption at each stage of the RAG process. Specifically, we will evaluate trade-offs between model size, accuracy, and sustainability, experiment with smaller models where possible.</p><p>Finally, as we expand go-to-market features, we will incorporate additional sustainability frameworks to enhance the tool’s versatility and relevance.</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/ESG_Table_5-0608727d7b7015b93a1f3dc1b99e7384.png" width="780" height="342" class="img_ev3q"></p><p><em>Table 5. Planned improvements for the Client ESG Due Diligence Tool</em></p></div>
<div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="acknowledgements">Acknowledgements<a href="https://ekimetrics.github.io/blog/Under_The_Hood#acknowledgements" class="hash-link" aria-label="Direct link to Acknowledgements" title="Direct link to Acknowledgements">​</a></h2><p>This work would not have been possible without the support of Caroline Milliotte, Ekimetrics' Global Director of Sustainability, whose leadership and vision as an "Agent of Change" have been instrumental in aligning sustainability with business strategy. We extend our gratitude to our project lead, Karin Sasaki who steered the team and the many contributors with great consistency and clarity throughout the project, and to our lead developer, Ruslana Tymchyk and Tiago Fleur, as well as to Daniel Martin-Williams, Ricardo Nasr, and Farah Amorri for their contributions to building and refining the ESG Due Diligence Tool. Tsiresy Rasoamanarivo carried out in- depth user testing, helping us share the development of the tool. Vinod played a key role in establishing the UK’s GenAI taskforce, while Jean-Baptiste Gette, Basile El Azhari, Anand Krishnakumar, Julien Le Cerf, Alexis Cunin, Jean Le Long, and Hasnaa TAHRI provided invaluable expertise on GenAI, Data Engineering, GreenIT, and Cloud Infrastructure. Finally, we thank William Nait Mazi and Claire Saignol for their insights on sustainability topics, which have helped shape this initiative.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="references">References<a href="https://ekimetrics.github.io/blog/Under_The_Hood#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">​</a></h2><p>[^2] – Climate Q&amp;A <a href="https://www.climateqa.com/" target="_blank" rel="noopener noreferrer">Chat | ClimateQ&amp;A</a><br>
<!-- -->[^3] – CLAIRE <a href="https://clair.bot/" target="_blank" rel="noopener noreferrer">clair.bot</a><br>
<!-- -->[^4] – DeepEval - [DeepEval - The Open-Source LLM Evaluation Framework] (<a href="https://deepeval.com/" target="_blank" rel="noopener noreferrer">https://deepeval.com/</a>)<br>
<!-- -->[^5] –Ekimetrics Responsible AI Pillars <a href="https://www.ekimetrics.com/articles/why-responsible-ai-security-is-key-to-protecting-data-and-gaining-a-competitive-edge" target="_blank" rel="noopener noreferrer">Why Responsible AI security is key to protecting data and gaining a competitive edge</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="appendix--model-card">Appendix – Model Card<a href="https://ekimetrics.github.io/blog/Under_The_Hood#appendix--model-card" class="hash-link" aria-label="Direct link to Appendix – Model Card" title="Direct link to Appendix – Model Card">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="model-overview">Model Overview<a href="https://ekimetrics.github.io/blog/Under_The_Hood#model-overview" class="hash-link" aria-label="Direct link to Model Overview" title="Direct link to Model Overview">​</a></h3><p>The ESG Due Diligence Tool leverages a Retrieval-Augmented Generation (RAG) architecture built on top of OpenAI’s GPT-4o to automate the evaluation of ESG risks and opportunities from company documentation. The tool integrates advanced NLP techniques with responsible AI principles to provide accurate, traceable, and standardised assessments aligned with Ekimetrics’ sustainability mission.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="intended-use">Intended Use<a href="https://ekimetrics.github.io/blog/Under_The_Hood#intended-use" class="hash-link" aria-label="Direct link to Intended Use" title="Direct link to Intended Use">​</a></h3><p><strong>Primary Use Case:</strong>
Automated ESG due diligence on prospective and current clients by:</p><ul>
<li>Generating structured responses to a set of predefined ESG questions.</li>
<li>Generating a company risk score.</li>
<li>Producing downloadable company reports.</li>
<li>Powering a chatbot to answer user queries based only on company-provided documents.</li>
<li>Feeding ESG scoring dashboards to guide decision-making.</li>
</ul><p><strong>Intended Users</strong>:</p><ul>
<li>Commercial teams</li>
<li>Sustainability leads</li>
<li>Risk and compliance officers</li>
</ul><p><strong>Operational Context:</strong></p><ul>
<li>B2B consulting</li>
<li>Internal client onboarding and review processes</li>
<li>Non-public or sensitive documentation processing (uploaded PDFs, ESG reports, etc.)</li>
</ul></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="model-details">Model Details<a href="https://ekimetrics.github.io/blog/Under_The_Hood#model-details" class="hash-link" aria-label="Direct link to Model Details" title="Direct link to Model Details">​</a></h3>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/ESG_Model_Details-5eb09504c9a2e9677dd5f7f0aee8e7b3.png" width="828" height="231" class="img_ev3q"></p></div>
<div align="justify"><ul>
<li>Vector Embeddings: Generated using OpenAI embedding model via Databricks.</li>
<li>Chunking Strategy: Configurable chunk sizes with summarisation before embedding for efficient storage and retrieval.</li>
</ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="training-data">Training Data<a href="https://ekimetrics.github.io/blog/Under_The_Hood#training-data" class="hash-link" aria-label="Direct link to Training Data" title="Direct link to Training Data">​</a></h3><p><strong>LLM Pretraining</strong>:
The base model (GPT-4o) is trained by OpenAI on publicly available and licensed data. No fine-tuning has been applied by Ekimetrics.
<strong>Custom Knowledge Base:</strong>
All responses are grounded in uploaded company documentation. Summaries of these documents (rather than raw text) are used for embedding and retrieval.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="rag-system-architecture">RAG System Architecture<a href="https://ekimetrics.github.io/blog/Under_The_Hood#rag-system-architecture" class="hash-link" aria-label="Direct link to RAG System Architecture" title="Direct link to RAG System Architecture">​</a></h3><p><strong>Retrieval-Augmented Generation Flow:</strong></p><ol>
<li>User uploads documents.</li>
<li>Eki.Parser extracts raw text.</li>
<li>Databricks processes: parsing, chunking, summarising, embedding.</li>
<li>Azure Cognitive Search retrieves top-k relevant summaries.</li>
<li>GPT-4o generates structured answers based on retrieved summaries.</li>
<li>Tool calculates company risk score based on generated answers.</li>
<li>Reports and chatbot outputs are served via Streamlit UI and dashboards.</li>
</ol><p><strong>Optimisations:</strong></p><ul>
<li><strong>Summarised chunk embedding</strong>: Improves semantic relevance and reduces hallucination risk.</li>
<li><strong>Prompt engineering</strong>: Structured, fact-grounded responses with referencing.</li>
<li><strong>Parameter tuning</strong>:<!-- -->
<ul>
<li>Chunking: max_tokens, chunk_overlap</li>
<li>Retrieval: top_k</li>
<li>Generation: temperature, top_p</li>
</ul>
</li>
</ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="evaluation-metrics">Evaluation Metrics<a href="https://ekimetrics.github.io/blog/Under_The_Hood#evaluation-metrics" class="hash-link" aria-label="Direct link to Evaluation Metrics" title="Direct link to Evaluation Metrics">​</a></h3><p>Evaluation is done manually and with automated scripts using:</p><ul>
<li><strong>Faithfulness</strong>: Are answers supported by documents?</li>
<li><strong>Contextual Relevancy</strong>: Do retrieved chunks relate to the user’s query?</li>
<li><strong>Answer Relevancy</strong>: Does the final answer actually address the question?</li>
</ul><p>Future iterations will use an <strong>LLM-powered test framework</strong> for automatic scoring of accuracy and hallucination rates.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="limitations">Limitations<a href="https://ekimetrics.github.io/blog/Under_The_Hood#limitations" class="hash-link" aria-label="Direct link to Limitations" title="Direct link to Limitations">​</a></h3><ul>
<li><strong>Domain coverage</strong>: Tool is only as accurate as the documentation uploaded.</li>
<li><strong>LLM hallucination</strong>: Mitigated through strict prompting and RAG, but not eliminated.</li>
<li><strong>Summarisation trade-offs</strong>: May omit nuance required in certain regulatory or legal contexts.</li>
<li><strong>Static ESG questionnaire</strong>: Currently based on a fixed set of questions – not dynamically adaptive to sector-specific ESG frameworks.</li>
</ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="ethical-considerations">Ethical Considerations<a href="https://ekimetrics.github.io/blog/Under_The_Hood#ethical-considerations" class="hash-link" aria-label="Direct link to Ethical Considerations" title="Direct link to Ethical Considerations">​</a></h3><ul>
<li><strong>Bias</strong>: Responses rely on uploaded data, which may reflect biased reporting or gaps in ESG disclosure.</li>
<li><strong>Transparency</strong>: Generated reports include document references to ensure traceability.</li>
<li><strong>Security</strong>: Azure Key Vault and storage policies protect sensitive data.</li>
<li><strong>Explainability</strong>: Summary-backed answers help users trace the origin of conclusions.</li>
</ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="responsible-ai-practices">Responsible AI Practices<a href="https://ekimetrics.github.io/blog/Under_The_Hood#responsible-ai-practices" class="hash-link" aria-label="Direct link to Responsible AI Practices" title="Direct link to Responsible AI Practices">​</a></h3><ul>
<li>Adheres to Ekimetrics' Responsible AI framework.</li>
<li>Answers are factual, grounded in evidence, and structured.</li>
<li>Ambiguity is managed with fallback logic (e.g., “No data found” or “No” where appropriate).</li>
<li>Future versions will include <strong>version control</strong> of reports and <strong>audit trails.</strong></li>
</ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="version--deployment">Version &amp; Deployment<a href="https://ekimetrics.github.io/blog/Under_The_Hood#version--deployment" class="hash-link" aria-label="Direct link to Version &amp; Deployment" title="Direct link to Version &amp; Deployment">​</a></h3><ul>
<li><strong>Model Version</strong>: GPT-4o (OpenAI, via Azure OpenAI Service)</li>
<li><strong>Tool Version</strong>: v1.0 (internal release)</li>
<li><strong>Deployment</strong>: Hosted on Azure App Service using Docker containers. Codebase managed on Bitbucket with CI/CD.</li>
</ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="contact--ownership">Contact &amp; Ownership<a href="https://ekimetrics.github.io/blog/Under_The_Hood#contact--ownership" class="hash-link" aria-label="Direct link to Contact &amp; Ownership" title="Direct link to Contact &amp; Ownership">​</a></h3><ul>
<li><strong>Owner</strong>: Ekimetrics Client ESG Due Diligence Team</li>
<li><strong>Contact</strong>: <a href="mailto:Caroline.Milliotte@Ekimetrics.com" target="_blank" rel="noopener noreferrer">Caroline.Milliotte@Ekimetrics.com</a></li>
</ul><p>For feedback or error reporting, please reach out to the product team directly.</p></div>]]></content:encoded>
            <category>ESG</category>
            <category>Due diligence</category>
            <category>GenAI</category>
            <category>RAG</category>
            <category>industrialised software</category>
            <category>operational excellence</category>
            <category>automation</category>
            <category>process standardisation</category>
        </item>
        <item>
            <title><![CDATA[Revolutionizing ESG Due Diligence: A User-Centric Overview of a GenAI-Powered Sustainability Assessment Tool]]></title>
            <link>https://ekimetrics.github.io/blog/ESG_Due_Diligence</link>
            <guid>https://ekimetrics.github.io/blog/ESG_Due_Diligence</guid>
            <pubDate>Mon, 09 Jun 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[This paper introduces the Client ESG Due Diligence Tool, an automated solution that evaluates sustainability risks and opportunities of working with clients, whatever the industry they operate in.]]></description>
            <content:encoded><![CDATA[<div align="justify"><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="summary">Summary<a href="https://ekimetrics.github.io/blog/ESG_Due_Diligence#summary" class="hash-link" aria-label="Direct link to Summary" title="Direct link to Summary">​</a></h3><p>As a mission-driven company, Ekimetrics is legally committed to integrating sustainability throughout the organization, operational processes, and business services we offer. This paper introduces the Client ESG Due Diligence Tool, an automated solution that evaluates sustainability risks and opportunities of working with clients, whatever the industry they operate in. Questioning how we do our business is an essential part of our "Mission". Built on our rigorous GenAI, Data Engineering, Responsible AI and MLOps standards, it is a packaged product that showcases our expertise in creating and implementing GenAI solutions for businesses, which enhance efficiency by streamlining workflows and standardizing processes. We share our approach, tech stack, and implementation insights, showcasing how this kind of automation can drive impactful ESG business decisions.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="introduction">Introduction<a href="https://ekimetrics.github.io/blog/ESG_Due_Diligence#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h2><p>Ekimetrics' Sustainability Report[^1] showcases our company's commitment to embedding sustainability into its operations, processes, and services. Assessing the sustainability practices of clients and prospects enables us to ensure that our services contribute to positive impact, rather than enabling unsustainable business models. The tool described in this article, the Client ESG Due Diligence Tool, plays a crucial role in this endeavour. The tool automates client company evaluation, providing structured insights that help identify risks, as well as areas where Ekimetrics can support clients in improving their own sustainability efforts. It is a similar concept to the classic "supplier due diligence" ESG-check, except that here, we question our downward value chain for its sustainability alignment, rather than the upward value chain.</p><p>The tool is, in essence, a RAG (Retrieval-Augmented Generation), an AI framework that leverages large language models (LLMs) to answer user queries more precisely, and in specialised domains. This kind of architecture ensures that responses are based on factual data, rather than only the model's pre-trained knowledge, and enhances explainability by providing references to the text used for generating responses. Such optimisation avoids having to go through the expensive and complex process of training a dedicated LLMs for a specific topic.</p><p>The tool is built upon Ekimetrics' rigorous standards in GenAI, Data Engineering, MLOps and Responsible AI, ensuring reliability, scalability and trustworthiness. This work is reflective of our expertise in developing robust GenAI software solutions that drive business operational efficiency, by streamlining workflows, freeing up valuable time from maintenance, for innovation, as well as providing standardisation to processes. We believe this specific solution – an automated due diligence based on documentation – is, indeed, applicable, and valuable, to other business settings, so in this paper we share, in detail, our approach, showcasing the tech stack and technical implementation of the solution.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-the-tool-is-used">How the Tool is Used<a href="https://ekimetrics.github.io/blog/ESG_Due_Diligence#how-the-tool-is-used" class="hash-link" aria-label="Direct link to How the Tool is Used" title="Direct link to How the Tool is Used">​</a></h2><p>ESG due diligence allows partners, managers, and commercial teams at Ekimetrics to determine how well a client company aligns with our own sustainability goals.
Ekimetrics' ESG due diligence process involves analysing company reports that are, publicly available online, such as financial reports, sustainability reports, supplier code of conduct, charter and certifications, and other company documentation, and extracting relevant information in order to answer predefined ESG-specific questions. If done manually, this process can be time-consuming and resource-intensive, risking inconsistent assessments and delayed decision-making.</p><p>The Client ESG Due Diligence tool is designed to address these pain points, in a way that is intuitive and user friendly. It consists of a dashboard – the Document Upload Dashboard – where users can upload company documents, which are then used to automatically carry out the due diligence and to calculate company risk scores (Figure 1). The dashboard also facilitates the search for scores of existing and already-screened companies and the download of the full company diligence report (Figure 2). Finally, a specialised chatbot, available directly on the same dashboard, assists users in developing a deeper understanding of the screened companies, and helps them identify new opportunities where Ekimetrics could support clients on their own sustainability practices (Figure 3). The tool also allows the user to check existing company documents in the database, as well as past due diligence reports, which helps them assess the date and reliability of the scores. The more documents provided for a client company, the more accurate their ESG risk score.</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/ESG_Figure_1-2e2794e8e6f725bd8f38c320d151eae0.png" width="903" height="421" class="img_ev3q"></p><p><em>Figure 1 – Users can upload company documents on the Document Upload Dashboard</em></p></div>
<div align="justify"><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-due-diligence-questionnaire">The Due Diligence Questionnaire<a href="https://ekimetrics.github.io/blog/ESG_Due_Diligence#the-due-diligence-questionnaire" class="hash-link" aria-label="Direct link to The Due Diligence Questionnaire" title="Direct link to The Due Diligence Questionnaire">​</a></h3><p>The due diligence questionnaire consists of a mixture of questions that, either apply to all companies, or that are industry specific. The questions are labelled by the categories shown in Table 1, and include questions such as "Does {the company} have a sustainability policy to promote diversity and inclusion?" (as an example of the general question) or "Does {company} have a strategy with clear targets and timelines to switch to renewable energy sources?" (as an example of an industry-specific question for the Transportation/Mobility sector). Each question is answered by the tool automatically, basing the response only on the company documents provided by the user.</p><p>The questions have been developed by Ekimetrics' ESG team, with the support of an external consultant to ensure the relevance and alignment on international frameworks. The tool allows for the questions to evolve over time.</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/ESG_Table_1-85802fd7bb78335f3a0fcc7a9e18ac8d.png" width="837" height="197" class="img_ev3q"></p><p><em>Table 1 – Topics and categories of Ekimetrics' client ESG due diligence</em></p></div>
<div align="justify"><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-company-report">The Company Report<a href="https://ekimetrics.github.io/blog/ESG_Due_Diligence#the-company-report" class="hash-link" aria-label="Direct link to The Company Report" title="Direct link to The Company Report">​</a></h3><p>Each company's report is generated as an Excel file containing automatically produced responses to both general and industry-specific questions. For each answer, the report includes the supporting references, detailing the extracted text along with the document name and page number. This structure allows users to delve into the specifics and assess the reliability of each response as needed. Scores are then calculated based on the number of affirmative answers, as outlined in the following section.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-scoring-system">The Scoring System<a href="https://ekimetrics.github.io/blog/ESG_Due_Diligence#the-scoring-system" class="hash-link" aria-label="Direct link to The Scoring System" title="Direct link to The Scoring System">​</a></h3><p>Figure 2 shows that for each client there are multiple scores. In fact, we derive thirteen elementary scores:</p><ul>
<li>one for each of the categories listed in Table 1</li>
<li>one for the general questions</li>
<li>one for the industry questions</li>
<li>one total score – i.e. for all the questions</li>
</ul><p>and one composite score, made up of the policy, reporting and certifications scores. The elementary scores are calculated as the proportion of questions that have "yes" as an answer, out of all the questions. The composite score is the sum of the policy, reporting and or certification scores, depending on the industry sector (see Table 2).</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/ESG_Figure_2-03907f50b0d62af8172bd27da4d86686.png" width="903" height="181" class="img_ev3q"></p><p><em>Figure 2 – Check the scores for a company</em></p></div>
<div align="justify"><p>Once a score is calculated, deciding how to engage with the client is an automated process, based on a decision tree (Figure 3). The decision tree is not only dependent on the score, but also the industry sector. First, industry sectors are categorised into four groups of risk: critical risk, high risk, medium risk, and no risk. By default, we willingly engage in business with companies that fall in the no risk industry sectors category, which include green technologies and renewable energy. As a matter of policy, Ekimetrics does not engage in work with companies in critical risk industry sectors. For the high and medium risk categories, Ekimetrics' engagement is determined by their commercial and sustainability goals, strategies, and commitments.</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/ESG_Table_2-3e894067e3c3de948d3772cb503f41ad.png" width="623" height="669" class="img_ev3q"></p><p><em>Table 2 – Procedure for calculating the composite risk score depending on the industry sector.</em></p><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/ESG_Figure_3-e88362e4a6938cc9edbb2fb070261e12.svg" width="602" height="338" class="img_ev3q"></p><p><em>Figure 3 – Decision tree on how to engage with clients – the actual threshold of acceptance for each industry sectors within a particular risk category also has a range and depends on the specific sector.</em></p></div>
<div align="justify"><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-chatbot">The Chatbot<a href="https://ekimetrics.github.io/blog/ESG_Due_Diligence#the-chatbot" class="hash-link" aria-label="Direct link to The Chatbot" title="Direct link to The Chatbot">​</a></h3><p>Figure 4 shows the chatbot tab on the User Interface (UI). The user selects the company they want to explore, and asks a question on the dedicated window. The tool answers all questions based only on the available company documents as well as the history of their conversation. Further, following our company's standards for Responsible AI [5], specifically the Explainability pillar, all parts of the answer are referenced to specific text in the resources section of the page.</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/ESG_Figure_4-e8cf1c53e243d80dcd96724150e586b4.png" width="903" height="408" class="img_ev3q"></p><p><em>Figure 4 – Specialised chatbot that helps the user to understand the company more deeply.</em></p></div>
<div align="justify"><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-global-assessment-dashboard">The Global Assessment Dashboard<a href="https://ekimetrics.github.io/blog/ESG_Due_Diligence#the-global-assessment-dashboard" class="hash-link" aria-label="Direct link to The Global Assessment Dashboard" title="Direct link to The Global Assessment Dashboard">​</a></h3><p>This resource displays summarised visualisations of all the scores for all the client companies in the database (Figure 5), allowing top management, commercial and sustainability leads to understand the overall ESG performance of our client-base at a glance. This can help them identify high-risk companies, and assist them to prioritize industry sectors for sustainability improvements and strategic engagement. As the tool is built on PowerBI, it enables easy manipulation and filtering of the data, allowing the user to focus on specific sectors, companies, or groups of companies that fall within a specific range of a score, or are a particular risk category.</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/ESG_Figure_5-5810bbb5b7e3a3c6c9641ee5e62f5979.png" width="903" height="501" class="img_ev3q"></p><p><em>Figure 5 - The Global Assessment Dashboard</em></p></div>
<div align="justify"><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="user-feedback">User feedback<a href="https://ekimetrics.github.io/blog/ESG_Due_Diligence#user-feedback" class="hash-link" aria-label="Direct link to User feedback" title="Direct link to User feedback">​</a></h3><p>From Renaud Pirel (Partner at Ekimetrics):</p><ul>
<li>We all carry unconscious biases when assessing the "ESG compliance" of a particular industry or business. Leveraging a data-driven tool helps us make more objective decisions and stay aligned with our mission at Ekimetrics.</li>
</ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="technical-implementation">Technical Implementation<a href="https://ekimetrics.github.io/blog/ESG_Due_Diligence#technical-implementation" class="hash-link" aria-label="Direct link to Technical Implementation" title="Direct link to Technical Implementation">​</a></h3><p>In our second article we describe in detail <a href="https://ekimetrics.github.io/blog/Under_The_Hood" target="_blank" rel="noopener noreferrer">the implementation and techstack of this tool</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="references">References<a href="https://ekimetrics.github.io/blog/ESG_Due_Diligence#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">​</a></h2><p>[^1] Ekimetrics' 2023 ESG report <a href="https://www.ekimetrics.com/en-gb/articles/ekimetrics-2023-esg-report" target="_blank" rel="noopener noreferrer">ekimetrics-2023-esg-report</a></p></div>]]></content:encoded>
            <category>ESG</category>
            <category>Due diligence</category>
            <category>GenAI</category>
            <category>RAG</category>
            <category>industrialised software</category>
            <category>operational excellence</category>
            <category>automation</category>
            <category>process standardisation</category>
        </item>
        <item>
            <title><![CDATA[Speed vs. Control vs. Flexibility: A Technical Framework for AI Architecture Selection]]></title>
            <link>https://ekimetrics.github.io/blog/AI_Architecture_Selection</link>
            <guid>https://ekimetrics.github.io/blog/AI_Architecture_Selection</guid>
            <pubDate>Fri, 06 Jun 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[This article introduces a structured framework for evaluating AI Models, AI Workflows, and AI Agents, emphasizing empirical selection principles based on operational constraints and industry-specific requirements rather than technological trends.]]></description>
            <content:encoded><![CDATA[<div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="abstract">Abstract<a href="https://ekimetrics.github.io/blog/AI_Architecture_Selection#abstract" class="hash-link" aria-label="Direct link to Abstract" title="Direct link to Abstract">​</a></h2><p>The proliferation of generative AI has introduced multiple architectural paradigms beyond standalone large language models. This paper presents a systematic framework for selecting between AI Models, AI Workflows, and AI Agents based on empirical evidence from production implementations across multiple industries. Our analysis demonstrates that architectural decisions should be driven by operational constraints and business requirements rather than technological novelty. We provide quantitative comparisons and decision criteria derived from real-world deployments in FinTech, Gaming, and Non-profit sectors.</p><hr><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="architectural-taxonomy">Architectural Taxonomy<a href="https://ekimetrics.github.io/blog/AI_Architecture_Selection#architectural-taxonomy" class="hash-link" aria-label="Direct link to Architectural Taxonomy" title="Direct link to Architectural Taxonomy">​</a></h2><p>Following the widespread adoption of generative AI, two primary design patterns have emerged beyond direct model integration: AI Workflows, which employ orchestrated, deterministic processes, and AI Agents, which utilize autonomous, LLM-driven decision-making. These architectures represent distinct approaches to balancing development velocity, operational control, and system adaptability.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="architectural-definitions">Architectural Definitions<a href="https://ekimetrics.github.io/blog/AI_Architecture_Selection#architectural-definitions" class="hash-link" aria-label="Direct link to Architectural Definitions" title="Direct link to Architectural Definitions">​</a></h3><p><strong>AI Models</strong>: Direct integration of large language models without additional orchestration layers. Implementation involves prompt engineering and API integration for language generation tasks.</p><ul>
<li>Advantages: Minimal development overhead, cost-effective deployment</li>
<li>Limitations: Constrained transparency, absence of external system integration</li>
</ul><p><strong>AI Workflows</strong>: Structured, deterministic sequences that orchestrate LLMs with complementary tools, APIs, and data sources through predefined logic.</p><ul>
<li>Advantages: High operational control, comprehensive auditability, regulatory compliance</li>
<li>Limitations: Architectural rigidity, substantial upfront design requirements</li>
</ul><p><strong>AI Agents</strong>: Systems employing LLM-driven reasoning for autonomous decision-making, dynamic tool selection, and adaptive behavior based on contextual analysis.</p><ul>
<li>Advantages: Flexible adaptation, complex task handling, conversational interaction patterns</li>
<li>Limitations: Development complexity, elevated operational costs, reduced predictability</li>
</ul><hr><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="empirical-case-studies">Empirical Case Studies<a href="https://ekimetrics.github.io/blog/AI_Architecture_Selection#empirical-case-studies" class="hash-link" aria-label="Direct link to Empirical Case Studies" title="Direct link to Empirical Case Studies">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="implementation-1-automated-climate-risk-assessment-fintech">Implementation 1: Automated Climate Risk Assessment (FinTech)<a href="https://ekimetrics.github.io/blog/AI_Architecture_Selection#implementation-1-automated-climate-risk-assessment-fintech" class="hash-link" aria-label="Direct link to Implementation 1: Automated Climate Risk Assessment (FinTech)" title="Direct link to Implementation 1: Automated Climate Risk Assessment (FinTech)">​</a></h3><p><strong>Technical Challenge</strong>: Systematic automation of climate risk assessments incorporating 200+ data points while maintaining regulatory compliance and audit requirements.</p><p><strong>Architectural Evaluation</strong>:</p><ul>
<li>AI Models: Rejected due to output inconsistency across execution cycles, incompatible with financial risk assessment standards</li>
<li>AI Agents: Excluded based on development timeline constraints (3-month delivery) and regulatory stability requirements</li>
</ul><p><strong>Selected Architecture</strong>: AI Workflow
<strong>Implementation Details</strong>: Deterministic processing pipeline with embedded LLM components for structured reasoning and analysis.</p><p><strong>Quantitative Outcomes</strong>:</p><ul>
<li>Data integration: 200+ risk indicators per assessment</li>
<li>Industry coverage: 1,023 ISIC-classified industries</li>
<li>Automation rate: 100% (zero manual intervention required)</li>
<li>Compliance: Full audit trail and expert validation integration</li>
</ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="implementation-2-natural-language-data-interface-non-profit">Implementation 2: Natural Language Data Interface (Non-profit)<a href="https://ekimetrics.github.io/blog/AI_Architecture_Selection#implementation-2-natural-language-data-interface-non-profit" class="hash-link" aria-label="Direct link to Implementation 2: Natural Language Data Interface (Non-profit)" title="Direct link to Implementation 2: Natural Language Data Interface (Non-profit)">​</a></h3><p><strong>Technical Challenge</strong>: Democratization of complex sustainability datasets for non-technical users through conversational interaction paradigms.</p><p><strong>Architectural Evaluation</strong>:</p><ul>
<li>AI Models: Insufficient for contextual memory and iterative query refinement</li>
<li>AI Workflows: Inadequate for open-ended exploratory data analysis patterns</li>
</ul><p><strong>Selected Architecture</strong>: AI Agent
<strong>Implementation Details</strong>: Dynamic query generation with real-time visualization and iterative analytical refinement capabilities.</p><p><strong>Quantitative Outcomes</strong>:</p><ul>
<li>Query accuracy improvement: 40% over traditional SQL approaches</li>
<li>Execution time reduction: 70% (average response time ~5 seconds)</li>
<li>User accessibility: Enabled independent data exploration for non-technical stakeholders</li>
<li>Scalability: Adaptive system growth alongside organizational data expansion</li>
</ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="implementation-3-text-to-sql-interface-gaming">Implementation 3: Text-to-SQL Interface (Gaming)<a href="https://ekimetrics.github.io/blog/AI_Architecture_Selection#implementation-3-text-to-sql-interface-gaming" class="hash-link" aria-label="Direct link to Implementation 3: Text-to-SQL Interface (Gaming)" title="Direct link to Implementation 3: Text-to-SQL Interface (Gaming)">​</a></h3><p><strong>Technical Challenge</strong>: Real-time data access for business users without SQL proficiency, integrated with existing communication infrastructure.</p><p><strong>Architectural Rationale</strong>: Predictable query patterns and accelerated delivery timeline favored architectural stability over adaptive capabilities.</p><p><strong>Selected Architecture</strong>: AI Workflow
<strong>Implementation Details</strong>: Structured text-to-SQL conversion with deterministic validation and comprehensive error handling mechanisms.</p><p><strong>Quantitative Outcomes</strong>:</p><ul>
<li>Efficiency improvement: 98% reduction in query resolution time (30 minutes to 30 seconds)</li>
<li>SQL generation accuracy: 90% success rate</li>
<li>System integration: Seamless messenger application compatibility</li>
<li>Operational impact: Elimination of IT team dependency for routine data requests</li>
</ul><hr><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="comparative-technical-analysis">Comparative Technical Analysis<a href="https://ekimetrics.github.io/blog/AI_Architecture_Selection#comparative-technical-analysis" class="hash-link" aria-label="Direct link to Comparative Technical Analysis" title="Direct link to Comparative Technical Analysis">​</a></h2><table><thead><tr><th>Evaluation Criteria</th><th>AI Models</th><th>AI Workflows</th><th>AI Agents</th></tr></thead><tbody><tr><td><strong>Development Timeline</strong></td><td>Days to Weeks</td><td>Weeks to Months</td><td>Months</td></tr><tr><td><strong>Technical Complexity</strong></td><td>Low</td><td>Moderate</td><td>High</td></tr><tr><td><strong>Operational Costs</strong></td><td>Low</td><td>Medium</td><td>High</td></tr><tr><td><strong>Output Determinism</strong></td><td>Limited</td><td>High</td><td>Moderate</td></tr><tr><td><strong>System Explainability</strong></td><td>Variable</td><td>High</td><td>Medium</td></tr><tr><td><strong>Scalability Pattern</strong></td><td>Vertical</td><td>Manual Extension</td><td>Adaptive Evolution</td></tr><tr><td><strong>Integration Complexity</strong></td><td>Minimal</td><td>Moderate</td><td>Substantial</td></tr><tr><td><strong>Maintenance Overhead</strong></td><td>Low</td><td>Medium</td><td>High</td></tr></tbody></table><hr><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="implementation-guidelines">Implementation Guidelines<a href="https://ekimetrics.github.io/blog/AI_Architecture_Selection#implementation-guidelines" class="hash-link" aria-label="Direct link to Implementation Guidelines" title="Direct link to Implementation Guidelines">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="architectural-selection-principles">Architectural Selection Principles<a href="https://ekimetrics.github.io/blog/AI_Architecture_Selection#architectural-selection-principles" class="hash-link" aria-label="Direct link to Architectural Selection Principles" title="Direct link to Architectural Selection Principles">​</a></h3><p><strong>Evidence-Based Decision Making</strong>: Architectural choices should be driven by empirical requirements analysis rather than technological trends or novelty considerations.</p><p><strong>Constraint-Driven Design</strong>: System architecture must align with operational constraints including timeline, budget, regulatory requirements, and organizational technical capabilities.</p><p><strong>Incremental Evolution Strategy</strong>: Successful implementations often follow a progression from simple model integration through structured workflows to adaptive agent systems based on validated user requirements.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="common-implementation-patterns">Common Implementation Patterns<a href="https://ekimetrics.github.io/blog/AI_Architecture_Selection#common-implementation-patterns" class="hash-link" aria-label="Direct link to Common Implementation Patterns" title="Direct link to Common Implementation Patterns">​</a></h3><p><strong>Hybrid Architecture Approaches</strong>: Production systems frequently combine elements from multiple architectural paradigms to optimize for specific functional requirements while maintaining overall system coherence.</p><p><strong>Progressive Enhancement</strong>: Organizations often begin with AI Model implementations for initial validation, evolve to AI Workflows for production stability, then selectively introduce AI Agent capabilities for complex interaction requirements.</p><p><strong>Risk-Adjusted Implementation</strong>: High-stakes applications typically favor deterministic workflow approaches, while exploratory or user-facing applications benefit from agent-based adaptive capabilities.</p><hr><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="architectural-evolution-trajectory">Architectural Evolution Trajectory<a href="https://ekimetrics.github.io/blog/AI_Architecture_Selection#architectural-evolution-trajectory" class="hash-link" aria-label="Direct link to Architectural Evolution Trajectory" title="Direct link to Architectural Evolution Trajectory">​</a></h2><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">AI Models → AI Workflows → AI Agents</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ↑           ↑            ↑</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Validation   Production    Adaptive</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Phase        Stability     Evolution</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Empirical evidence suggests that successful AI implementations often follow a maturation path from direct model integration through structured workflow orchestration to autonomous agent capabilities, with each phase addressing different organizational maturity and requirement complexity levels.</p><hr><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="technical-conclusions">Technical Conclusions<a href="https://ekimetrics.github.io/blog/AI_Architecture_Selection#technical-conclusions" class="hash-link" aria-label="Direct link to Technical Conclusions" title="Direct link to Technical Conclusions">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="key-findings">Key Findings<a href="https://ekimetrics.github.io/blog/AI_Architecture_Selection#key-findings" class="hash-link" aria-label="Direct link to Key Findings" title="Direct link to Key Findings">​</a></h3><ol>
<li><strong>Constraint-driven architecture selection</strong> consistently outperforms technology-driven approaches in production environments</li>
<li><strong>Hybrid architectural strategies</strong> frequently deliver superior outcomes compared to single-paradigm implementations</li>
<li><strong>Operational control versus system adaptability</strong> represents the fundamental architectural trade-off requiring explicit design consideration</li>
<li><strong>User interaction patterns</strong> should drive architectural decisions rather than underlying technical capabilities</li>
<li><strong>Agent complexity overhead</strong> requires substantial organizational investment in development, testing, and operational processes</li>
</ol><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="future-considerations">Future Considerations<a href="https://ekimetrics.github.io/blog/AI_Architecture_Selection#future-considerations" class="hash-link" aria-label="Direct link to Future Considerations" title="Direct link to Future Considerations">​</a></h3><p>As large language model capabilities advance and agentic frameworks mature, we anticipate architectural convergence with workflows gaining dynamic capabilities and agents achieving greater controllability. However, the fundamental design principles of constraint-driven selection, empirical validation, and incremental evolution remain constant.</p><p>Organizations should focus on systematic requirement analysis, quantitative outcome measurement, and iterative architectural refinement rather than pursuing technological novelty for its own sake.</p><hr><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="references-and-further-reading">References and Further Reading<a href="https://ekimetrics.github.io/blog/AI_Architecture_Selection#references-and-further-reading" class="hash-link" aria-label="Direct link to References and Further Reading" title="Direct link to References and Further Reading">​</a></h2><p>For detailed implementation case studies and quantitative analysis methodologies, refer to our comprehensive research documentation. Technical discussions regarding specific use cases and architectural patterns are available through our engineering team consultation process.</p><p>To gain a comprehensive understanding of the trade-offs among 3 AI architectures and visualize your decision-making with a strategic AI deployment map, download <a href="https://www.ekimetrics.com/en-apac/articles/speed-vs-control-vs-flexibility-what-your-ai-architecture-says-about-your-business" target="_blank" rel="noopener noreferrer">the guide</a> now.</p><p><strong>Research Areas</strong>: #AI-Architecture #LLM-Integration #Production-AI #System-Design #Technical-Decision-Frameworks
If you are interested, please <a href="mailto:inno@ekimetrics.com" target="_blank" rel="noopener noreferrer">connect with us</a></p></div>]]></content:encoded>
            <category>Agentic AI</category>
            <category>AI Architecture</category>
            <category>AI Workflow</category>
            <category>AI Agent</category>
            <category>AI Models</category>
            <category>Generative AI</category>
            <category>Data Science</category>
            <category>LLM-Integration</category>
            <category>Production-AI</category>
            <category>System-Design</category>
            <category>Technical-Decision-Frameworks</category>
        </item>
        <item>
            <title><![CDATA[Part 2: From Theory to Measurement - Quantifying the Impact of Communication on Our Carbon Footprint]]></title>
            <link>https://ekimetrics.github.io/blog/Quantifying_Impact_Communication</link>
            <guid>https://ekimetrics.github.io/blog/Quantifying_Impact_Communication</guid>
            <pubDate>Mon, 26 May 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[In a context where companies are called upon to reduce their greenhouse gas emissions by 5% per year, the communication industry faces a fundamental question: how to evaluate and optimize its real contribution to the ecological transition?]]></description>
            <content:encoded><![CDATA[<div align="justify"><p>"Numbers tell stories that words cannot express." This maxim takes on its full meaning when addressing the environmental impact of communication. In our previous article, we established a conceptual framework distinguishing between the direct and indirect impact of communication, revealing the complexity of the mechanisms through which this sector influences our collective carbon footprint. Today, we transform this theoretical framework into a concrete measurement tool.<br>
<!-- -->In a context where companies are called upon to reduce their greenhouse gas emissions by 5% per year, the communication industry faces a fundamental question: how to evaluate and optimize its real contribution to the ecological transition?</p><p>This question is all the more crucial as the recent <a href="https://www.linforme.com/energie/article/le-rapport-choc-qui-veut-rendre-la-publicite-plus-durable_2686.html" target="_blank" rel="noopener noreferrer">French ministerial report</a> proposes a transformation of climate contracts and increased regulation of commercial communications. To respond in a quantified manner, we have conducted an in-depth analysis distinguishing between the direct footprint (linked to agency and media activities) and the indirect footprint (resulting from generated consumption behaviors).
Our study reveals an undeniable finding: while the direct footprint of the sector represents approximately 1.02 MtCO2e (or 0.27% of French national emissions), its indirect footprint reaches 16.55 MtCO2e, equivalent to 4.44% of the national carbon footprint. This ratio of 1 to 16 between direct and indirect impact persists even in our most conservative sensitivity analyses, where it varies from 4 to 80 depending on the assumptions. This disproportion upends the traditional perspective and demonstrates that beyond eco-designing campaigns, the sector's main lever for action lies in the strategic orientation of marketing investments.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="direct-effect-quantifying-the-operational-footprint-of-communication">Direct Effect: Quantifying the Operational Footprint of Communication<a href="https://ekimetrics.github.io/blog/Quantifying_Impact_Communication#direct-effect-quantifying-the-operational-footprint-of-communication" class="hash-link" aria-label="Direct link to Direct Effect: Quantifying the Operational Footprint of Communication" title="Direct link to Direct Effect: Quantifying the Operational Footprint of Communication">​</a></h2><p>To evaluate the direct carbon footprint of the sector, two methodological approaches are available:</p><ul>
<li>The <strong>top-down approach</strong> relies on carbon assessments published by major players in the sector to estimate the overall impact of the industry. This method offers a macroscopic vision and allows capturing the diversity of communication professions.</li>
<li>The <strong>bottom-up approach</strong> uses specific emission factors by type of activity to build an estimate from elementary processes. It allows a more granular analysis of emission sources.</li>
</ul><p>The table below synthesizes the strengths and limitations of each method:</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Comparative_Table_1-e0e51d77f703b0e7d0d241af6150f542.png" width="790" height="676" class="img_ev3q"></p><p><em>Table 1: Comparative study of methods for estimating the direct impact of communication.</em></p></div>
<div align="justify"><p>For this first global assessment, we favored the top-down approach, while recognizing that it may lead to a high estimate of the sector's footprint.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="data-from-major-communication-groups">Data from Major Communication Groups<a href="https://ekimetrics.github.io/blog/Quantifying_Impact_Communication#data-from-major-communication-groups" class="hash-link" aria-label="Direct link to Data from Major Communication Groups" title="Direct link to Data from Major Communication Groups">​</a></h3><p>Our analysis is based on the CSR reports of the six largest global communication groups, whose carbon intensities (kgCO₂e/€) were calculated by relating their total emissions (scopes 1, 2, and 3) to their turnover:</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Comparative_Table_2-a4130ab59d3643d8c9b244863b7eed65.png" width="670" height="381" class="img_ev3q"></p><p><em>Table 2: Comparison of emissions from communications companies according to their level of activity</em></p></div>
<div align="justify"><p>* No clear location-based data were available for scope 2, therefore market-based emissions might have been used.<br>
<!-- -->** Average conversion factor was used for the year to turn the data in euro<br>
<!-- -->*** Scope 3 covers only aviation, so the data were ignored</p><p>This analysis reveals an interesting trend: French groups (Publicis, Havas) present a significantly lower carbon intensity than their international counterparts. This difference could be explained by earlier sensitivity to environmental issues in France or by specificities in the structure of activities. However, the variation remains contained within a factor of 1 to 4, suggesting a certain sectoral consistency.</p><p>For our national estimate, we retained the average value of <strong>0.031 kgCO₂e/€</strong>, which represents a prudent and robust approximation of the sector's carbon intensity.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="carbon-footprint-of-the-sector-in-france">Carbon Footprint of the Sector in France<a href="https://ekimetrics.github.io/blog/Quantifying_Impact_Communication#carbon-footprint-of-the-sector-in-france" class="hash-link" aria-label="Direct link to Carbon Footprint of the Sector in France" title="Direct link to Carbon Footprint of the Sector in France">​</a></h3><p>According to data from Arcom (Arcom, 2024), the business volume of communication in France amounts to 33 billion euros, divided between media (52%) and non-media (48%). By applying our average carbon intensity factor of 0.031 kgCO₂e/€ to this activity volume, we obtain a direct footprint <strong>of 1.02 MtCO₂e for the entire sector, or about 0.27% of French national emissions</strong> (373 MtCO₂e in 2023 according to CITEPA, 2024).</p><p>This estimate, although significant, places the communication sector among moderate contributors to direct national emissions. However, as we will see in the next section, this vision captures only a fraction of the sector's real environmental impact.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="indirect-effect-measuring-the-impact-of-communication-on-consumption">Indirect Effect: Measuring the Impact of Communication on Consumption<a href="https://ekimetrics.github.io/blog/Quantifying_Impact_Communication#indirect-effect-measuring-the-impact-of-communication-on-consumption" class="hash-link" aria-label="Direct link to Indirect Effect: Measuring the Impact of Communication on Consumption" title="Direct link to Indirect Effect: Measuring the Impact of Communication on Consumption">​</a></h2><p>After quantifying the direct operational footprint of communication, we tackle a more complex challenge: evaluating its impact on consumption behaviors and, consequently, on the associated greenhouse gas emissions.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="review-of-scientific-literature">Review of Scientific Literature<a href="https://ekimetrics.github.io/blog/Quantifying_Impact_Communication#review-of-scientific-literature" class="hash-link" aria-label="Direct link to Review of Scientific Literature" title="Direct link to Review of Scientific Literature">​</a></h3><p>Our approach is based on an in-depth analysis of academic studies and gray literature. We identified two main categories of relevant publications:</p><ul>
<li>ROI aggregation studies that estimate the economic effectiveness of communication at the advertiser level.</li>
<li>Macroeconomic analyses that measure the effect of advertising on consumption volume and GDP growth.</li>
</ul></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Decomposition-f53e79899f87bbe0fabefd76bd930bfc.png" width="529" height="412" class="img_ev3q"></p><p><em>Figure 1: Decomposition of measured effects as a function of the observed variable.</em></p></div>
<div align="justify"><p>The table below summarizes the main international studies on the subject:</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Table_analysis-8e50995c33b0c39a776eab4a1feeb444.png" width="756" height="730" class="img_ev3q"></p><p><em>Table 3: Summary of analyses on the impact of communication</em></p></div>
<div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="methodology-translating-economic-impact-into-carbon-footprint">Methodology: Translating Economic Impact into Carbon Footprint<a href="https://ekimetrics.github.io/blog/Quantifying_Impact_Communication#methodology-translating-economic-impact-into-carbon-footprint" class="hash-link" aria-label="Direct link to Methodology: Translating Economic Impact into Carbon Footprint" title="Direct link to Methodology: Translating Economic Impact into Carbon Footprint">​</a></h2><p>To establish a robust method, we distinguished three different approaches in these studies:</p><ol>
<li><strong>ROI aggregation</strong> without accounting for the adverse effect between competitors</li>
<li><strong>Estimation of overall impact</strong> on consumption (useful for modeling contribution to growth)</li>
<li><strong>Calculation of marginal impact</strong> allowing modeling of the effect of variations in investments</li>
</ol><p>We favored the results of the "Value of Advertising" study (Deloitte, 2017) as a starting point, while adjusting them by a corrective factor integrating the more moderate results of other publications.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="structuring-hypotheses">Structuring Hypotheses<a href="https://ekimetrics.github.io/blog/Quantifying_Impact_Communication#structuring-hypotheses" class="hash-link" aria-label="Direct link to Structuring Hypotheses" title="Direct link to Structuring Hypotheses">​</a></h3><p>Our model is based on three fundamental hypotheses:</p><ol>
<li><strong>Hypothesis of communication market efficiency</strong>: We consider that media and non-media, each representing about 50% of investments, generate an equivalent median ROI.</li>
<li><strong>Hypothesis of marginality of the price effect</strong>: The price effect is considered secondary compared to the market growth effect. The work of Dubois, Griffith, and O'Connell (Dubois, 2018, p. 421) on the British chip market shows that a total stop of advertising would generate only an increase in price sensitivity of less than 3%.</li>
<li><strong>Hypothesis of GDP-Energy-Carbon correlation</strong>: We rely on the work of Giraud (Giraud, 2014) demonstrating the strong historical correlation between GDP and energy consumption, allowing us to establish a carbon intensity factor per euro of GDP.</li>
</ol><p>For methodological prudence, we did not integrate the adverse effect (redistribution between competitors) in our model, which leads to a minimal estimate of the potential impact of communication.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="calculation-of-the-indirect-footprint">Calculation of the Indirect Footprint<a href="https://ekimetrics.github.io/blog/Quantifying_Impact_Communication#calculation-of-the-indirect-footprint" class="hash-link" aria-label="Direct link to Calculation of the Indirect Footprint" title="Direct link to Calculation of the Indirect Footprint">​</a></h3><p>Our approach breaks down into three steps:</p><p><strong>Step 1: Estimation of generated consumption</strong></p><p>We start with the GDP generation factor of €7.85 per euro invested (Deloitte, 2017), which we adjust by a corrective factor of 0.48 to reflect the average of other studies (ratio 0.43/0.9 between the average impact and that of the reference study).
Calculation: €33 billion (communication investment) × 7.85 (GDP factor) × 0.48 (corrective factor) = <strong>€125.2 billion</strong> of additional consumption generated by communication in France.</p><p><strong>Step 2: Conversion to carbon intensity</strong></p><p>We calculate the average carbon intensity of French GDP by dividing national emissions by GDP:</p><ul>
<li>National emissions 2023: 373 MtCO₂e (CITEPA, 2024)</li>
<li>French GDP 2023: €2,822.5 billion (INSEE, 2024)</li>
</ul><p>Yielding an intensity of <strong>0.13 kgCO₂e per euro of GDP.</strong></p><p><strong>Step 3: Calculation of the indirect footprint</strong></p><p>By applying this carbon intensity to the generated consumption: €125.2 billion × 0.13 kgCO₂e/€ = <strong>16.55 MtCO₂e</strong> This indirect footprint represents approximately <strong>4.44% of the French national carbon footprint</strong>, or more than 16 times the direct impact of the sector.<br>
<!-- -->This result underlines the critical importance of considering the influence of communication on consumption behaviors when evaluating its overall environmental impact.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion-transforming-communication-into-a-lever-for-ecological-transition">Conclusion: Transforming Communication into a Lever for Ecological Transition<a href="https://ekimetrics.github.io/blog/Quantifying_Impact_Communication#conclusion-transforming-communication-into-a-lever-for-ecological-transition" class="hash-link" aria-label="Direct link to Conclusion: Transforming Communication into a Lever for Ecological Transition" title="Direct link to Conclusion: Transforming Communication into a Lever for Ecological Transition">​</a></h2><p>Our study reveals a fundamental truth for the communication industry: with 94% of its carbon impact coming from indirect effects versus only 6% from direct impact, the sector must profoundly rethink its contribution to the ecological transition. This disproportion, robust even in our most conservative sensitivity analyses, demonstrates that eco-designing campaigns, while necessary, cannot suffice given the scale of the challenge.
The real issue lies in the strategic orientation of the €125.2 billion of additional consumption generated each year by the sector in France. How can this considerable influence be transformed into a positive force for the planet?</p><p>At Ekimetrics, we have developed the MM4S (Marketing Mix for Sustainability) methodology to precisely answer this question. Our approach quantifies and optimizes the dual dimension of marketing:</p><ul>
<li>Its traditional commercial performance</li>
<li>Its overall environmental impact</li>
</ul><p>Our analyses demonstrate that a strategic reallocation of marketing budgets can simultaneously improve sales and reduce the carbon footprint. In practice, this means:</p><ul>
<li>Reducing advertising pressure on high-impact products</li>
<li>Accelerating the visibility of sustainable offerings</li>
<li>Objectively quantifying marketing's contribution to the company's environmental objectives</li>
</ul><p>At a time when regulations are rapidly evolving, this data-driven approach allows companies to anticipate future constraints while transforming their communication into a real lever for ecological transition. Because the message is clear: communication can no longer be content with greening its image – it must become a measurable catalyst for the transformation of our consumption patterns.</p><p>Faced with this observation, professionals in the sector now have the opportunity to reinvent their profession, combining marketing expertise and environmental responsibility. It is this new paradigm that Ekimetrics accompanies, with concrete solutions to measure, pilot, and optimize the global impact of your marketing investments.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="cited-works">Cited Works<a href="https://ekimetrics.github.io/blog/Quantifying_Impact_Communication#cited-works" class="hash-link" aria-label="Direct link to Cited Works" title="Direct link to Cited Works">​</a></h2><p>Arcom. (2024). <em>Perspectives d’évolution du marché</em>.<br>
<!-- -->CITEPA. (2024). É<em>missions de gaz à effet de serre 1990-2023</em>.<br>
<!-- -->Delloite. (2017). <em>The economic contribution of advertising in Europe. A report for the World Federation of Advertisers.</em><br>
<!-- -->Dubois, P. G. (2018). The effects of banning advertising in junk food markets. <em>The Review of Economic Studies</em>, 85(1), 396-436.<br>
<!-- -->Ekimetrics. (2023). <em>How MMM can become a powerfull tool for sustainble business performance.</em><br>
<!-- -->Erdem, T. K. (2008). The impact of advertising on consumer price sensitivity in experience goods markets. <em>Quantitative Marketing and Economics</em>, 6, 139-176.<br>
<!-- -->Giraud, G. (2014). <em>How Dependent is Growth from Primary Energy ?</em><br>
<!-- -->Greenpeace France, l. R. (2020). <em>PUBLICITE <!-- -->:POUR<!-- --> UNE LOI EVIN CLIMAT.</em><br>
<!-- -->INSEE. (2024). <em>LES COMPTES DE LA NATION EN 2023.</em><br>
<!-- -->Molinari, B. &amp;. (2017). Advertising and Aggregate Consumption: A Bayesian DSGE Assessment. <em>The Economic Journal</em>, 128. 10.1111/ecoj.12514.</p></div>]]></content:encoded>
            <category>Communication Industry</category>
            <category>Direct Impact</category>
            <category>Indirect Impact</category>
            <category>Corporate Responsibility</category>
            <category>Advertising Effect</category>
            <category>Marketing Impact</category>
            <category>Economic science</category>
        </item>
        <item>
            <title><![CDATA[Part 1: The Dual Impact of Communication: A Conceptual Framework for Understanding its Environmental Impact]]></title>
            <link>https://ekimetrics.github.io/blog/Dual_Impact_Communication</link>
            <guid>https://ekimetrics.github.io/blog/Dual_Impact_Communication</guid>
            <pubDate>Thu, 22 May 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[This analysis examines how the communication sector contributes to 5% of French carbon emissions, distinguishing between direct impacts (production and distribution) and the more significant indirect effects on consumer behavior, offering a fresh perspective on communication's role in ecological transition.]]></description>
            <content:encoded><![CDATA[<div align="justify"><p>Have you ever considered the ecological footprint left by an advertising banner or a magazine filled with advertisements? In a world where ecological transition has become imperative, no sector can escape scrutiny of its environmental impact - and communication is no exception, being directly and indirectly responsible for 5% of French emissions.</p><p>Our research, of which we present the first part here, aims to break down and analyze the carbon footprint of the communication industry. This analysis is structured around two complementary publications:</p><p>In this first article, we establish the conceptual foundations necessary to understand the complexity of the sector's environmental impact. We explore the crucial distinction between direct and indirect effects, before delving into the various mechanisms through which communication influences our consumption behaviors. This theoretical framework is essential for grasping the true environmental challenges facing the sector.</p><p>The second part will focus on precisely quantifying these impacts through a rigorous methodology. We will reveal data on the carbon impact of communication in France, showing orders of magnitude that could transform our perception of this sector's role in the climate crisis.</p><p>Together, these two articles offer a holistic understanding of the challenges and opportunities facing the communication industry considering the climate emergency. Because while understanding is the first step, precise measurement is the essential prerequisite for any effective action.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="a-dual-impact">A Dual Impact<a href="https://ekimetrics.github.io/blog/Dual_Impact_Communication#a-dual-impact" class="hash-link" aria-label="Direct link to A Dual Impact" title="Direct link to A Dual Impact">​</a></h2><p>The environmental impact of communication breaks down into two main effects, fundamentally different in their nature and scale:</p><ul>
<li><strong>Direct emissions</strong>: linked to the consumption and purchases necessary for the creation and distribution of communications. These correspond, for example, to the pollution generated by powering advertising servers and printing posters.</li>
<li><strong>Indirect emissions</strong>: resulting from changes in consumption and behaviors caused by these communications. Advertising contributes to expanding market sizes, and consequently the production of polluting goods.</li>
</ul></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Breakdown_marketing_impact_table-47ff4fed7169b32eea22ce686d2a4e85.png" width="1042" height="461" class="img_ev3q">
<em>Figure 1 : Breakdown of marketing impact (Ekimetrics, 2023)</em></p></div>
<div align="justify"><p>This distinction is key to properly understanding the environmental responsibility of the sector. Let's examine each of these dimensions in more detail.</p><p>However, be careful, the notion of scope 4 is often used to refer to these indirect emissions. This doesn't make sense because indirect emissions are related to emissions that the company records in its scopes 1, 2, and 3 to produce the goods and services sold thanks to marketing. However, these emissions could have been avoided if marketing had promoted other less emissive products or not promoted products at all.</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Contribution_graph-9802722dd054b26031df3a17e992485a.png" width="940" height="282" class="img_ev3q">
<em>Figure 2 : Contribution of direct and indirect impact to a company's carbon footprint</em></p></div>
<div align="justify"><p>Let's look at each of these elements in more detail:</p><br><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="direct-impact-a-highly-structured-area-in-france-with-ongoing-internationalization">Direct Impact: A Highly Structured Area in France with Ongoing Internationalization<a href="https://ekimetrics.github.io/blog/Dual_Impact_Communication#direct-impact-a-highly-structured-area-in-france-with-ongoing-internationalization" class="hash-link" aria-label="Direct link to Direct Impact: A Highly Structured Area in France with Ongoing Internationalization" title="Direct link to Direct Impact: A Highly Structured Area in France with Ongoing Internationalization">​</a></h2><p>Unlike the indirect impact, still largely unexplored, the direct impact of communication benefits from growing attention and significant standardization efforts in methodology to quantify environmental impacts. These efforts focus on establishing consistent measurement frameworks and assessment protocols across the industry.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="french-professionalization-of-measurement">French Professionalization of Measurement<a href="https://ekimetrics.github.io/blog/Dual_Impact_Communication#french-professionalization-of-measurement" class="hash-link" aria-label="Direct link to French Professionalization of Measurement" title="Direct link to French Professionalization of Measurement">​</a></h3><p>France is at the forefront of this structuring, with an ecosystem of specialized tools according to communication sectors. It's interesting to distinguish between work done to evaluate emissions due to production and those related to message distribution:</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="production-france-conditions-its-aid-on-environmental-assessment">Production: France Conditions its Aid on Environmental Assessment<a href="https://ekimetrics.github.io/blog/Dual_Impact_Communication#production-france-conditions-its-aid-on-environmental-assessment" class="hash-link" aria-label="Direct link to Production: France Conditions its Aid on Environmental Assessment" title="Direct link to Production: France Conditions its Aid on Environmental Assessment">​</a></h3><p>In the field of audiovisual production, several calculators have been approved by the National Center for Cinema (CNC):</p><ul>
<li><a href="https://www.flyingsecoya.com/" target="_blank" rel="noopener noreferrer">SeCO2</a> developed by Secoya Eco-tournage, allowing measurement of the footprint of live-action productions.</li>
<li><a href="https://ecoprod.com/" target="_blank" rel="noopener noreferrer">Carbon' Clap</a> created by the Ecoprod association, a pioneer in this field.</li>
<li><a href="https://greenly.earth/" target="_blank" rel="noopener noreferrer">Carbon Stage</a> designed by the company Greenly.</li>
</ul><p>The animation sector is also well represented with <a href="https://www.animfrance.fr/" target="_blank" rel="noopener noreferrer">Carbulator</a>, developed by Anim France, while the video game industry has  <a href="https://www.gameonly.org/" target="_blank" rel="noopener noreferrer">Jyros</a>, created by Game Only. This structuring is accompanied by progressive institutionalization: the CNC now conditions certain subsidies on conducting environmental impact analyses, strongly encouraging the adoption of these practices.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="distribution-towards-standardization-of-methods">Distribution: Towards Standardization of Methods<a href="https://ekimetrics.github.io/blog/Dual_Impact_Communication#distribution-towards-standardization-of-methods" class="hash-link" aria-label="Direct link to Distribution: Towards Standardization of Methods" title="Direct link to Distribution: Towards Standardization of Methods">​</a></h3><p>On the communication distribution side, professional unions and audience measurers such as <a href="https://www.sri.fr/sri/fr/" target="_blank" rel="noopener noreferrer">SRI</a> or <a href="https://www.admtv.org/communiques/le-snptv-publie-son-referentiel-methodologique-pour-la-mesure-de-lempreinte-carbone-de-la-diffusion-des-campagnes-publicitaires-en-tv-lineaire/" target="_blank" rel="noopener noreferrer">SNPTV</a> are developing calculators to homogenize data collection and impact analysis. Among advertisers, the Union Des Marques (UDM) is taking a step back and proposing to aggregate these initiatives in a <a href="https://uniondesmarques.fr/nos-services/actualit%C3%A9s/article/2023/06/29/LUnion-des-marques-lance-son-m%C3%A9ta-r%C3%A9f%C3%A9rentiel-sur-la-mesure-de-limpact-carbone-des-campagnes-de-communication" target="_blank" rel="noopener noreferrer">meta-framework</a> intended to harmonize and make visible all these tools (Press: <a href="https://www.lapressemagazine.fr/actualite/presse-magazine-sengage-le-climat-le-sepm-lance-eco-impact-sepm-calculateur-dempreinte" target="_blank" rel="noopener noreferrer">SEPM</a>, Digital: <a href="https://www.sri.fr/sri/fr/" target="_blank" rel="noopener noreferrer">SRI</a>, Radio: <a href="https://d-k.io/media/pages/ressources/referentiel-bureau-de-la-radio/3155003852-1690875731/radio-et-audio-referentiel-calcul-empreinte-carbone-diffusion-campagnes-publicitaires-radio-audio-vdef.pdf" target="_blank" rel="noopener noreferrer">Bureau de la Radio</a>, TV: <a href="https://www.admtv.org/communiques/le-snptv-publie-son-referentiel-methodologique-pour-la-mesure-de-lempreinte-carbone-de-la-diffusion-des-campagnes-publicitaires-en-tv-lineaire/" target="_blank" rel="noopener noreferrer">SNPTV</a>, outdoor: <a href="https://carbone-calculateur-adoohcc.upe.fr/%E2%80%A6" target="_blank" rel="noopener noreferrer">UPE</a>)</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="an-internationalizing-dynamic">An Internationalizing Dynamic<a href="https://ekimetrics.github.io/blog/Dual_Impact_Communication#an-internationalizing-dynamic" class="hash-link" aria-label="Direct link to An Internationalizing Dynamic" title="Direct link to An Internationalizing Dynamic">​</a></h3><p>This French movement is rapidly extending internationally. The multiplication of specialized third parties measuring environmental footprint (<a href="https://scope3.com/" target="_blank" rel="noopener noreferrer">Scope 3</a>, <a href="https://icare.com/" target="_blank" rel="noopener noreferrer">I-Care</a>, <a href="https://d-k.io/en" target="_blank" rel="noopener noreferrer">DK</a>, <a href="https://alice.publicisgroupe.com/fr/" target="_blank" rel="noopener noreferrer">ALICE developed by Publicis</a>, <a href="https://www.ekimetrics.com/fr" target="_blank" rel="noopener noreferrer">Ekimetrics</a>) has led to the birth of a global initiative: <a href="https://adnetzero.com/" target="_blank" rel="noopener noreferrer">Add Net Zero</a>. This coalition aims to harmonize methodologies and create a global standard for the communication industry. Digital giants are also beginning to integrate these concerns. For example, Google now publishes information on the carbon footprint of its advertising campaigns.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="a-persistent-blind-spot">A Persistent Blind Spot<a href="https://ekimetrics.github.io/blog/Dual_Impact_Communication#a-persistent-blind-spot" class="hash-link" aria-label="Direct link to A Persistent Blind Spot" title="Direct link to A Persistent Blind Spot">​</a></h3><p>Despite these significant advances, a fundamental dimension remains absent from current frameworks: the impact of the message conveyed. Existing tools focus exclusively on production and distribution processes, neglecting the effect of content on consumption behaviors.<br>
<!-- -->This gap is problematic because our conceptual analysis suggests that this indirect impact could far exceed that of direct operations. Precisely measuring the carbon footprint of a filming or the energy consumption of a digital campaign is necessary but insufficient to understand the overall environmental responsibility of the sector. That's why we want to address it as an equally important subject to indirect impact.</p><br><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="indirect-effect-the-hidden-power-of-communication-on-our-consumption-choices">Indirect Effect: The Hidden Power of Communication on Our Consumption Choices<a href="https://ekimetrics.github.io/blog/Dual_Impact_Communication#indirect-effect-the-hidden-power-of-communication-on-our-consumption-choices" class="hash-link" aria-label="Direct link to Indirect Effect: The Hidden Power of Communication on Our Consumption Choices" title="Direct link to Indirect Effect: The Hidden Power of Communication on Our Consumption Choices">​</a></h2><p>The indirect impact of communication represents a complex ecosystem of intertwined influences on our consumption behaviors. Far from being monolithic, this impact unfolds through several distinct mechanisms, forming a web of influences with varied environmental consequences.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-triptych-of-advertising-impact">The Triptych of Advertising Impact<a href="https://ekimetrics.github.io/blog/Dual_Impact_Communication#the-triptych-of-advertising-impact" class="hash-link" aria-label="Direct link to The Triptych of Advertising Impact" title="Direct link to The Triptych of Advertising Impact">​</a></h3><p>Our analysis of scientific literature has allowed us to identify three fundamental mechanisms through which communication modifies consumption behaviors:</p><p><strong>The (adverse) redistribution effect</strong> postulates that the market is a zero-sum game and that advertising changes the distribution of demand among brands. When a car brand communicates effectively, it can attract 100 customers who would otherwise have bought from a competitor, without increasing the total number of vehicles sold (Dubois et al., 2018). It's a transfer of market share without modification of the overall volume of sales.</p><p><strong>The growth effect</strong> represents the ability of communication to extend the boundaries of the market itself. A campaign for smartphones can convince consumers who had not planned to buy a new device this year to take the plunge, thus creating additional demand that would not have existed without this communication.</p><p><strong>The price effect</strong>, particularly subtle, leads advertising to transform the perception of value and price sensitivity (Erdem et al., 2008). Premium communication for organic coffee can simultaneously justify a higher price for certain consumers (potentially decreasing the volume sold) while drawing attention to the entire category, stimulating price sensitivity and sometimes pushing competitors' prices downward (thus increasing the overall volume).</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="radically-different-environmental-consequences">Radically Different Environmental Consequences<a href="https://ekimetrics.github.io/blog/Dual_Impact_Communication#radically-different-environmental-consequences" class="hash-link" aria-label="Direct link to Radically Different Environmental Consequences" title="Direct link to Radically Different Environmental Consequences">​</a></h3><p>These three effects are compatible with brands' objective of maximizing profitability. However, they fundamentally diverge in their environmental implications:</p><p><strong>The redistribution effect</strong>, although seemingly neutral since it does not modify the overall volume of consumption, can nevertheless lead to significant indirect consequences. If an SUV manufacturer captures market share at the expense of a manufacturer of low-consumption city cars, the overall carbon impact increases without any additional cars being sold. This effect reveals the ability of communication to redirect consumption flows between products with different carbon footprints.</p><p><strong>The growth effect</strong> constitutes the most directly problematic mechanism from an environmental perspective. Each additional act of consumption represents a new drain on planetary resources and new greenhouse gas emissions. The effectiveness of communication in generating additional demand then becomes a direct amplifier of environmental pressure.</p><p><strong>The price effect</strong> presents a fundamental ambivalence. On one hand, communication can help valorize sustainable products, justifying a price premium that finances ecological innovation. On the other hand, by making consumers less sensitive to price, it can democratize access to certain high-impact product categories, increasing their diffusion. The directionality of this effect fundamentally depends on the nature of the products promoted.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="towards-an-operational-analytical-framework">Towards an Operational Analytical Framework<a href="https://ekimetrics.github.io/blog/Dual_Impact_Communication#towards-an-operational-analytical-framework" class="hash-link" aria-label="Direct link to Towards an Operational Analytical Framework" title="Direct link to Towards an Operational Analytical Framework">​</a></h3><p>To establish a rigorous conceptual framework for the indirect impact of communication, we must recognize these nuances while keeping in mind the objective of a practical assessment. The redistribution effect, although important for understanding market dynamics, presents an apparent neutrality that can be misleading when applied to products with different environmental footprints. The price effect, meanwhile, would require detailed econometric data to be properly evaluated.</p><p>This complexity reminds us that any measure of the environmental impact of communication must integrate not only the volume of consumption generated, but also the nature of the products and services promoted, as well as their alternatives in the market.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="https://ekimetrics.github.io/blog/Dual_Impact_Communication#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><p>At the end of this analysis, one observation stands out: the iceberg of the environmental impact of communication shows only its tip. With only 6% direct impact against 94% indirect impact, we must fundamentally rethink our approach to the sector.</p><p>In our next article, we will dive into the precise figures and methodologies that allow us to quantify this impact, revealing how communication could become a major lever for ecological transition rather than an accelerator of unsustainable consumption.</p><p>The question that awaits us is crucial: how to transform a sector whose historical purpose has been to stimulate consumption into a catalyst for sobriety?</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="works-cited">Works Cited<a href="https://ekimetrics.github.io/blog/Dual_Impact_Communication#works-cited" class="hash-link" aria-label="Direct link to Works Cited" title="Direct link to Works Cited">​</a></h2><p>Arcom. (2024). <em>Perspectives d’évolution du marché.</em><br>
<!-- -->CITEPA. (2024). <em>Émissions de gaz à effet de serre 1990-2023.</em><br>
<!-- -->Delloite. (2017). <em>The economic contribution of advertising in Europe. A report for the World Federation of Advertisers.</em><br>
<!-- -->Dubois, P. G. (2018). The effects of banning advertising in junk food markets. <em>The Review of Economic Studies</em>, 85(1), 396-436.<br>
<!-- -->Ekimetrics. (2023). *How MMM can become a powerfull tool for sustainble business performance. *<br>
<!-- -->Erdem, T. K. (2008). The impact of advertising on consumer price sensitivity in experience goods markets. <em>Quantitative Marketing and Economics</em>, 6, 139-176.<br>
<!-- -->Giraud, G. (2014). *How Dependent is Growth from Primary Energy ? *<br>
<!-- -->Greenpeace France, l. R. (2020). <em>PUBLICITE <!-- -->:POUR<!-- --> UNE LOI EVIN CLIMAT.</em><br>
<!-- -->INSEE. (2024). *LES COMPTES DE LA NATION EN 2023. *<br>
<!-- -->Molinari, B. &amp;. (2017). Advertising and Aggregate Consumption: A Bayesian DSGE Assessment. <em>The Economic Journal</em>, 128. 10.1111/ecoj.12514.</p></div>]]></content:encoded>
            <category>Communication Industry</category>
            <category>Direct Impact</category>
            <category>Indirect Impact</category>
            <category>Corporate Responsibility</category>
            <category>Advertising Effect</category>
            <category>Marketing Impact</category>
            <category>Economic science</category>
        </item>
        <item>
            <title><![CDATA[Counterfactual Explanations: Enhancing Machine Learning Transparency and Delivering Actionable Insights]]></title>
            <link>https://ekimetrics.github.io/blog/Counterfactual_Explanations</link>
            <guid>https://ekimetrics.github.io/blog/Counterfactual_Explanations</guid>
            <pubDate>Wed, 16 Apr 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Discover what counterfactual explanations are and how illustrating changes that lead to different model outcomes enhances transparency, fairness, and actionable decision-making in machine learning.]]></description>
            <content:encoded><![CDATA[<div align="justify"><p>Counterfactual explanations are transforming how we interpret machine learning models by answering critical 'what-if' questions. Instead of simply revealing why a model made a particular decision, counterfactuals show how to change the outcome, bridging the gap between algorithmic decisions and human understanding.<br>
<!-- -->In this article, we explore what counterfactuals explanations are, why they matter, how they can be generated, and their future role in explainable AI (XAI).</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="i-what-is-a-counterfactual-explanation-in-machine-learning">I. What is a Counterfactual Explanation in Machine Learning?<a href="https://ekimetrics.github.io/blog/Counterfactual_Explanations#i-what-is-a-counterfactual-explanation-in-machine-learning" class="hash-link" aria-label="Direct link to I. What is a Counterfactual Explanation in Machine Learning?" title="Direct link to I. What is a Counterfactual Explanation in Machine Learning?">​</a></h2><p>Counterfactual explanations (or simply counterfactuals) describe how modifying specific input features could lead to a different model outcome. They provide minimal, actionable changes that would flip a prediction. They can be generated for both classification and regression problems. In the literature, the majority of studies have focused on classification problems, where counterfactuals suggest feature modifications to change a categorical outcome. Regression-based counterfactuals are now gaining more attention while facing specific issues related to continuous variable outcomes.</p><p>Examples of Counterfactuals:</p><ul>
<li>
<p><strong>Classification Example (Loan Approval)</strong>: If a model predicts that a loan application is denied, a counterfactual might suggest that increasing the applicant’s income by $10,000 or reducing their existing debt by $5,000 could change the outcome to approved.</p>
</li>
<li>
<p><strong>Regression Example (House Price Prediction)</strong>: If a model estimates a house's value at $300,000, a counterfactual might suggest that increasing the lot size by 500 square feet or adding an extra bedroom could raise the predicted price to $350,000.</p>
</li>
</ul><p>Unlike traditional interpretability techniques which highlight which features most influence a prediction, counterfactuals focus on how to change the outcome. Feature importance methods can be categorized into global and local approaches: global feature importance (e.g., Gini index) assesses the overall impact of each feature on the model across all predictions, while local feature importance (e.g., SHAP) explains the contribution of features to a specific instance’s prediction. Counterfactuals, in contrast, provide instance-specific insights by suggesting changes needed to obtain a different outcome.</p><p>For example:</p><ul>
<li>Global feature importance tells us that income and credit score are the most important factors in a loan decision in general, not for a specific instance.</li>
<li>SHAP quantifies how much each feature contributed to the final prediction.</li>
<li>Counterfactuals go a step further by suggesting precise changes, such as increasing income by $10,000 to get the loan approved.</li>
</ul><p>This focus on actionability makes counterfactual explanations particularly valuable when users need clear next steps, rather than just a breakdown of feature contributions.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="ii-why-counterfactuals-matter-in-machine-learning">II. Why Counterfactuals Matter in Machine Learning<a href="https://ekimetrics.github.io/blog/Counterfactual_Explanations#ii-why-counterfactuals-matter-in-machine-learning" class="hash-link" aria-label="Direct link to II. Why Counterfactuals Matter in Machine Learning" title="Direct link to II. Why Counterfactuals Matter in Machine Learning">​</a></h2><p>Counterfactual explanations can enhance AI transparency and fairness by addressing several key challenges:</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="actionable-insights">Actionable Insights<a href="https://ekimetrics.github.io/blog/Counterfactual_Explanations#actionable-insights" class="hash-link" aria-label="Direct link to Actionable Insights" title="Direct link to Actionable Insights">​</a></h4><p>As mentioned, unlike traditional explainability methods that only highlight influential features, counterfactuals provide clear, practical steps to achieve a desired outcome. Instead of just showing that income is important for loan approval, counterfactuals tell users exactly how much income needs to increase for approval.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="trust-and-compliance">Trust and Compliance<a href="https://ekimetrics.github.io/blog/Counterfactual_Explanations#trust-and-compliance" class="hash-link" aria-label="Direct link to Trust and Compliance" title="Direct link to Trust and Compliance">​</a></h4><p>As AI systems play a growing role in decision-making, <a href="https://dl.acm.org/doi/10.1145/3593013.3594069" target="_blank" rel="noopener noreferrer">regulations require that automated decisions be explainable and fair</a>. Counterfactuals help organizations justify decisions by demonstrating the exact changes that would have led to a different outcome.</p><p>This improves:</p><ul>
<li><strong>Transparency</strong> – Users understand why they received a particular decision.</li>
<li><strong>Fairness</strong> – Decision-making processes become more accountable.</li>
<li><strong>Trust</strong> – People are more likely to accept AI-driven outcomes if they can see how decisions are made.</li>
</ul><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="model-debugging-and-bias-detection">Model Debugging and Bias Detection<a href="https://ekimetrics.github.io/blog/Counterfactual_Explanations#model-debugging-and-bias-detection" class="hash-link" aria-label="Direct link to Model Debugging and Bias Detection" title="Direct link to Model Debugging and Bias Detection">​</a></h4><p>Counterfactuals also help developers detect biases in machine learning models.
For example, if certain demographic groups need unrealistically high increases in income to get loan approvals, this may indicate a hidden bias in the model. By analyzing these patterns, developers can:</p><ul>
<li>Identify fairness issues</li>
<li>Refine the model to reduce bias</li>
<li>Improve model reliability and robustness</li>
</ul><p>By providing actionable recommendations, ensuring regulatory compliance, and improving model fairness, counterfactual explanations are essential for building trustworthy AI systems.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="iii-generating-counterfactuals-dice-and-mace">III. Generating Counterfactuals: DiCE and MACE<a href="https://ekimetrics.github.io/blog/Counterfactual_Explanations#iii-generating-counterfactuals-dice-and-mace" class="hash-link" aria-label="Direct link to III. Generating Counterfactuals: DiCE and MACE" title="Direct link to III. Generating Counterfactuals: DiCE and MACE">​</a></h2><p>Counterfactual generation is a key step in understanding and improving machine learning models. Two widely used methods for generating counterfactuals are DiCE (Diverse Counterfactual Explanations) and MACE (Model-Agnostic Counterfactual Explanations with Constraints).</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="dice-diverse-counterfactual-explanations">DiCE (Diverse Counterfactual Explanations)<a href="https://ekimetrics.github.io/blog/Counterfactual_Explanations#dice-diverse-counterfactual-explanations" class="hash-link" aria-label="Direct link to DiCE (Diverse Counterfactual Explanations)" title="Direct link to DiCE (Diverse Counterfactual Explanations)">​</a></h4><p><a href="https://arxiv.org/pdf/1905.07697" target="_blank" rel="noopener noreferrer">DiCE</a>, developed by Microsoft Research, is a powerful tool for generating diverse counterfactual explanations in machine learning. Unlike traditional counterfactual methods that focus on finding a single optimal explanation, DiCE emphasizes the importance of providing multiple, diverse alternatives.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="key-features-of-dice">Key Features of DiCE:<a href="https://ekimetrics.github.io/blog/Counterfactual_Explanations#key-features-of-dice" class="hash-link" aria-label="Direct link to Key Features of DiCE:" title="Direct link to Key Features of DiCE:">​</a></h4><ol>
<li>Model-Agnostic Approach: DiCE can work with any black-box model, making it versatile across different machine learning architectures.</li>
<li>Diversity in Explanations: It generates a set of counterfactuals that are meaningfully different from each other, offering a broader perspective on possible changes.</li>
<li>Customizable Constraints: Users can specify feature ranges and immutability, ensuring the generated counterfactuals are realistic and actionable.</li>
<li>Multiple Generation Methods: DiCE employs various techniques including genetic algorithms, random sampling, and KD-tree search to efficiently explore the feature space.</li>
<li>Scalability: It's designed to handle large datasets and complex models efficiently.</li>
</ol><p>DiCE solves a key limitation of single-counterfactual methods by offering multiple alternative solutions, which is especially useful when there are different ways to achieve the same outcome. This variety not only helps users explore more actionable options but also provides a clearer picture of the model’s decision boundaries.
That said, while DiCE is a major step forward in explainable AI, it isn’t perfect without the right constraints, it can sometimes suggest unrealistic changes. That’s why domain expertise is essential for setting meaningful boundaries and making sense of the results.</p><p>To better understand how DiCE works in practice, here is an example in Python demonstrating how to generate counterfactual explanations using a trained machine learning model:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> dice_ml </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> dice_ml </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> Dice</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Initialize a DiCE explainer with dataset details and model constraints</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">d </span><span class="token operator">=</span><span class="token plain"> dice_ml</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">dataframe</span><span class="token operator">=</span><span class="token plain">X_train</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                 continuous_features</span><span class="token operator">=</span><span class="token plain">numerical_features</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                 outcome_name</span><span class="token operator">=</span><span class="token plain">target</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                 permitted_range</span><span class="token operator">=</span><span class="token plain">permitted_ranges</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Wrap the trained model for use with DiCE</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">m </span><span class="token operator">=</span><span class="token plain"> dice_ml</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">model</span><span class="token operator">=</span><span class="token plain">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> backend</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">"sklearn"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Create a DiCE explainer instance</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">exp </span><span class="token operator">=</span><span class="token plain"> Dice</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">d</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> m</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Select a single test instance (excluding the target column)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">query_instance </span><span class="token operator">=</span><span class="token plain"> X_test</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">drop</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">columns</span><span class="token operator">=</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">target</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">iloc</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Generate counterfactual explanations for the selected instance</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">counterfactuals </span><span class="token operator">=</span><span class="token plain"> exp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">generate_counterfactuals</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">query_instance</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                                               total_CFs</span><span class="token operator">=</span><span class="token number">5</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Number of counterfactual examples to generate</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                                               desired_class</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">"opposite"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Flip the predicted class</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                                               features_to_vary</span><span class="token operator">=</span><span class="token plain">actionable_features</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Features allowed to change</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Display counterfactuals, showing only feature differences</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">counterfactuals</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">visualize_as_dataframe</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">show_only_changes</span><span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><br><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="mace-model-agnostic-counterfactual-explanations-with-constraints">MACE (Model-Agnostic Counterfactual Explanations with Constraints)<a href="https://ekimetrics.github.io/blog/Counterfactual_Explanations#mace-model-agnostic-counterfactual-explanations-with-constraints" class="hash-link" aria-label="Direct link to MACE (Model-Agnostic Counterfactual Explanations with Constraints)" title="Direct link to MACE (Model-Agnostic Counterfactual Explanations with Constraints)">​</a></h4><p><a href="https://arxiv.org/pdf/2205.15540" target="_blank" rel="noopener noreferrer">MACE (Model-Agnostic Counterfactual Explanations)</a> is an advanced approach to generating counterfactual explanations in machine learning, with a primary focus on ensuring the realism and feasibility of the generated explanations.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="key-features-of-mace">Key features of MACE<a href="https://ekimetrics.github.io/blog/Counterfactual_Explanations#key-features-of-mace" class="hash-link" aria-label="Direct link to Key features of MACE" title="Direct link to Key features of MACE">​</a></h4><ol>
<li>Constraint Integration: MACE incorporates domain-specific constraints directly into the counterfactual generation process. This ensures that all suggested changes are not only mathematically valid but also practically feasible and logically consistent.</li>
<li>Model Agnosticism: Like other counterfactual methods, MACE can work with any type of machine learning model, making it versatile across different applications and model architectures.</li>
<li>Plausibility Enforcement: MACE prevents the generation of impossible or unrealistic counterfactuals. For instance, it won't suggest changes like reducing age or altering immutable characteristics.</li>
<li>Customizable Rules: Users can define specific constraints tailored to their domain, allowing for fine-grained control over what types of changes are considered acceptable.</li>
<li>Handling Complex Data Types: MACE is designed to work effectively with both numerical and categorical features, addressing the challenges posed by mixed data types in real-world datasets.</li>
</ol><p>MACE stands out for its ability to generate realistic and actionable explanations. However, this realism comes at a cost. MACE is more computationally intensive than simpler counterfactual methods because it solves complex optimization problems with constraints.
Despite this, MACE has proven highly valuable in situations where explanation credibility and regulatory compliance matter most.
That said, the quality of MACE’s explanations still depends on well-defined constraints and high-quality data. Without these, even the most advanced counterfactual method can produce misleading or unhelpful results.</p><p>To better understand how MACE works in practice, here is an example in Python demonstrating how to generate counterfactual explanations using a trained machine learning model:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> omnixai</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">tabular </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> Tabular</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> omnixai</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">preprocessing</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">tabular </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> TabularTransform</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> omnixai</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">explainers</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">tabular </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> MACEExplainer</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Load the dataset into an OmnixAI Tabular object</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">tabular_data </span><span class="token operator">=</span><span class="token plain"> Tabular</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">df</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    feature_columns</span><span class="token operator">=</span><span class="token plain">feature_names</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># List of feature column names</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    categorical_columns</span><span class="token operator">=</span><span class="token plain">categorical_features</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Specify categorical features</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    target_column</span><span class="token operator">=</span><span class="token plain">target  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Specify the target variable</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Fit a tabular transformer for preprocessing (e.g., encoding, scaling)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">transformer </span><span class="token operator">=</span><span class="token plain"> TabularTransform</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">fit</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">tabular_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">class_names </span><span class="token operator">=</span><span class="token plain"> transformer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">class_names  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Get class labels from the transformer</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Define a prediction function that applies the model after transformation</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">predict_function </span><span class="token operator">=</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">lambda</span><span class="token plain"> z</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">predict_proba</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">transformer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">transform</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">z</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Initialize the MACE counterfactual explainer</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">maceexplainer </span><span class="token operator">=</span><span class="token plain"> MACEExplainer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    training_data</span><span class="token operator">=</span><span class="token plain">tabular_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Provide the training dataset</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    predict_function</span><span class="token operator">=</span><span class="token plain">predict_function</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Use the defined prediction function</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ignored_features</span><span class="token operator">=</span><span class="token plain">features_to_ignore  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Specify features that should not be modified</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Select test instances (excluding the target column) for explanation</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">test_instances </span><span class="token operator">=</span><span class="token plain"> tabular_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">remove_target_column</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token number">5</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Generate counterfactual explanations for the selected test instances</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">explanations </span><span class="token operator">=</span><span class="token plain"> maceexplainer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">explain</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">test_instances</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Visualize the counterfactual explanation for the first test instance</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">explanations</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ipython_plot</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">index</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> class_names</span><span class="token operator">=</span><span class="token plain">class_names</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="example-of-mace-application-to-the-give-me-some-credit-dataset">Example of MACE Application to the Give Me Some Credit Dataset<a href="https://ekimetrics.github.io/blog/Counterfactual_Explanations#example-of-mace-application-to-the-give-me-some-credit-dataset" class="hash-link" aria-label="Direct link to Example of MACE Application to the Give Me Some Credit Dataset" title="Direct link to Example of MACE Application to the Give Me Some Credit Dataset">​</a></h4><p>To demonstrate how MACE can be used in a real-world setting, we applied it to the well-known <a href="https://github.com/DrIanGregory/Kaggle-GiveMeSomeCredit" target="_blank" rel="noopener noreferrer">Give Me Some Credit dataset from Kaggle</a>. This dataset was originally used in a 2011 competition to predict the likelihood of a borrower becoming seriously delinquent (90+ days late on a payment) within two years.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="dataset-overview">Dataset Overview<a href="https://ekimetrics.github.io/blog/Counterfactual_Explanations#dataset-overview" class="hash-link" aria-label="Direct link to Dataset Overview" title="Direct link to Dataset Overview">​</a></h4><p>The dataset includes over 250,000 anonymized credit records, split into 150,000 training examples and 101,503 test instances. The target variable is SeriousDlqin2yrs (1 = default, 0 = no default), and each record contains 10 explanatory variables that capture an individual’s credit behavior, income, and financial obligations. Key features include:</p><ul>
<li>RevolvingUtilizationOfUnsecuredLines: Utilization ratio of unsecured lines (e.g., credit cards)</li>
<li>DebtRatio: Proportion of debt relative to income</li>
<li>MonthlyIncome: Self-reported monthly income</li>
<li>NumberOfOpenCreditLinesAndLoans: Total number of credit lines and loans</li>
<li>NumberOfTimes90DaysLate: Frequency of severe delinquency</li>
</ul><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="model-training">Model Training<a href="https://ekimetrics.github.io/blog/Counterfactual_Explanations#model-training" class="hash-link" aria-label="Direct link to Model Training" title="Direct link to Model Training">​</a></h4><p>For this case study, we used XGBoost, a gradient boosting algorithm known for its ability to handle structured data and capture nonlinear relationships. XGBoost was trained on the preprocessed training data to predict the likelihood of loan default (SeriousDlqin2yrs).</p><p>After training, we applied MACE to generate counterfactual explanations for individuals predicted as high risk (i.e., default = 1). The goal was to understand what minimal and realistic changes could flip their classification to non-default.</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Table-35ed507976f3c203ad0b08bbfb6944b0.png" width="604" height="108" class="img_ev3q"></p></div>
<div align="justify"><p>Two counterfactuals were generated for the individual originally predicted to default (label = 1). The original instance had a DebtRatio of 0.803 and a MonthlyIncome of 9120.0. The first counterfactual (CF 1) keeps the DebtRatio constant at 0.803 but increases the MonthlyIncome to 9422.625, resulting in a predicted label of 0 (non-default). The second counterfactual (CF 2) achieves the same label flip by simultaneously lowering the DebtRatio to 0.7993 and raising the MonthlyIncome to 9422.625. This demonstrates that modest, realistic adjustments to financial metrics can alter risk classification, highlighting specific and actionable paths to improve creditworthiness.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="iv-the-future-of-counterfactuals-and-explainable-ai">IV. The Future of Counterfactuals and Explainable AI<a href="https://ekimetrics.github.io/blog/Counterfactual_Explanations#iv-the-future-of-counterfactuals-and-explainable-ai" class="hash-link" aria-label="Direct link to IV. The Future of Counterfactuals and Explainable AI" title="Direct link to IV. The Future of Counterfactuals and Explainable AI">​</a></h2><p>Counterfactual explanations are becoming an increasingly important tool for AI transparency and decision-making. As the field evolves, several key trends are shaping their future.</p><p>One major shift is the integration of causal reasoning into counterfactual methods. Instead of simply answering "what-if" questions, future approaches will identify real cause-and-effect relationships, helping distinguish meaningful factors from random correlations. This will make counterfactual explanations more reliable and actionable.</p><p>Another important development is the move toward automated constraints. Rather than relying on manually defined rules, new techniques will learn realistic constraints directly from data. This will ensure that counterfactual suggestions remain practical and achievable without requiring extensive human oversight.</p><p>Despite these advancements, challenges remain especially in scaling counterfactual methods for complex models like transformers and ensuring they work effectively across different languages and cultures.</p><p>To address these issues, we have been developing a counterfactual generator that prioritizes plausibility and proximity. By ensuring that suggested changes are both realistic and within the distribution, our approach aims to provide users with more trustworthy insights into AI-driven decisions.</p><p>Counterfactuals are playing a key role in shifting AI from an opaque "black box" to a transparent, user-friendly system. By making machine learning more explainable, accountable, and actionable, they will help shape the future of ethical and trustworthy AI.</p></div>]]></content:encoded>
            <category>XAI</category>
            <category>explainability</category>
            <category>Machine Learning</category>
            <category>Counterfactuals</category>
        </item>
        <item>
            <title><![CDATA[MMM Opportunities of Using Causal Inference]]></title>
            <link>https://ekimetrics.github.io/blog/Causal_Inference</link>
            <guid>https://ekimetrics.github.io/blog/Causal_Inference</guid>
            <pubDate>Mon, 31 Mar 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Integrating causal inference into Marketing Mix Models transforms correlation-based marketing analytics into powerful decision engines that reveal the true impact of campaigns.]]></description>
            <content:encoded><![CDATA[<div align="justify"><p>Marketing Mix Modeling (MMM) has served as a foundational methodology for quantifying marketing effectiveness for decades. Its enduring relevance stems from its ability to provide aggregate insights across channels without relying on individual-level tracking. In today's privacy-centric landscape, this advantage has become even more valuable.</p><p>However, as marketing ecosystems grow increasingly complex, traditional MMM frameworks face new challenges in capturing the true dynamics of modern campaigns. This isn't to suggest that classical MMM approaches are obsolete—quite the contrary. Instead, we propose that integrating causal inference methodologies can enhance and extend these proven frameworks, addressing their limitations while preserving their strengths.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-causal-nature-of-marketing-roi">The Causal Nature of Marketing ROI<a href="https://ekimetrics.github.io/blog/Causal_Inference#the-causal-nature-of-marketing-roi" class="hash-link" aria-label="Direct link to The Causal Nature of Marketing ROI" title="Direct link to The Causal Nature of Marketing ROI">​</a></h2><p>When any ad designer or CMO asks, "What was the return on investment of our TV campaign?", they are not simply inquiring about correlation – they're asking a causal question. In statistical terms, they're seeking to measure the effect of TV advertising spend on sales. This is the so-called Average Treatment Effect (ATE) in the causal inference field of study. More specifically, they might want to understand this effect for:</p><ul>
<li>Specific time periods or conditions</li>
<li>Weeks or campaigns</li>
</ul><p>All of these questions are trying to explain the true incremental impact of marketing activities on business outcomes.</p><p>Traditional MMM approaches typically employ regression techniques to model the relationship between marketing activities and business outcomes. These models have delivered substantial value by:</p><ol>
<li><strong>Quantifying Channel Contributions</strong>: Estimating the impact of different marketing channels on sales or other KPIs</li>
<li><strong>Optimizing Budget Allocation</strong>: Providing guidance on how to distribute marketing investments</li>
<li><strong>Forecasting Outcomes</strong>: Predicting the likely impact of planned marketing activities</li>
</ol><p>These capabilities remain valuable, and the established MMM ecosystem offers robust tools for addressing fundamental marketing measurement questions. However, classical approaches often make simplifying assumptions that can limit their accuracy in certain contexts.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-independence-assumption-bait">The Independence Assumption Bait<a href="https://ekimetrics.github.io/blog/Causal_Inference#the-independence-assumption-bait" class="hash-link" aria-label="Direct link to The Independence Assumption Bait" title="Direct link to The Independence Assumption Bait">​</a></h2><p>While traditional regression models might seem sufficient when input variables are independent, real-world marketing data rarely satisfies this condition. Marketing plans are typically optimized to maximize sales, creating inherent correlations between channels and activities. This interdependence introduces several critical biases:</p><ul>
<li><strong>Selection Bias</strong>: Advertisers naturally allocate more budget to channels that historically perform well, creating a self-fulfilling prophecy in the data.</li>
<li><strong>Temporal Confounding</strong>: Seasonal effects and marketing calendar events influence both advertising decisions and sales outcomes.</li>
<li><strong>Channel Interaction Effects</strong>: Marketing channels don't operate in isolation but influence and reinforce each other, creating complex causal pathways.</li>
</ul><p>Rather than replacing traditional MMM, causal inference methodologies can be integrated to address specific limitations while building upon the existing foundation. This integration helps solve several persistent challenges:</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="challenge-1-confounding-variables">Challenge 1: Confounding Variables<a href="https://ekimetrics.github.io/blog/Causal_Inference#challenge-1-confounding-variables" class="hash-link" aria-label="Direct link to Challenge 1: Confounding Variables" title="Direct link to Challenge 1: Confounding Variables">​</a></h3><p>Classical MMM struggles when unobserved factors influence both marketing decisions and outcomes. For example, a company might increase TV advertising during periods of expected high demand (holidays), making it difficult to disentangle the true impact of advertising from the seasonal effect.</p><p>Causal inference explicitly models these confounding relationships:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># Traditional approach might look like:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> statsmodels</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">api </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token plain"> sm</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Simplified regression model treating all variables as independent</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">model </span><span class="token operator">=</span><span class="token plain"> sm</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">OLS</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">sales</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">tv_spend</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> digital_spend</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> search_spend</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> email_spend</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> seasonality</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">fit</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Causal approach explicitly models the confounding structure:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> dowhy </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> CausalModel</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> networkx </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token plain"> nx</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Create a graph showing both direct effects and confounding relationships</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">g </span><span class="token operator">=</span><span class="token plain"> nx</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">DiGraph</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Add nodes for all variables</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">g</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">add_nodes_from</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">'TV_Spend'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'Digital_Spend'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'Search_Spend'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'Email_Spend'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'Seasonality'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'Sales'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Add direct effects on sales</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">g</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">add_edges_from</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'TV_Spend'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'Sales'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'Digital_Spend'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'Sales'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'Search_Spend'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'Sales'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'Email_Spend'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'Sales'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Add confounding relationships (seasonality affects both marketing and sales)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">g</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">add_edges_from</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'Seasonality'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'TV_Spend'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'Seasonality'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'Digital_Spend'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'Seasonality'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'Sales'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Add channel interaction effects (TV drives search behavior)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">g</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">add_edges_from</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'TV_Spend'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'Search_Spend'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Create causal model for a specific treatment (TV_Spend in this example)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">causal_model </span><span class="token operator">=</span><span class="token plain"> CausalModel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data</span><span class="token operator">=</span><span class="token plain">df</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    treatment</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">'TV_Spend'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    outcome</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">'Sales'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    graph</span><span class="token operator">=</span><span class="token plain">g</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>By explicitly modeling this causal structure, we can obtain unbiased estimates of the true effect of TV advertising, isolated from seasonal influences.</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/causal_graph_example-21cb946e859e60771eff0357dfef2bfc.jpg" width="800" height="640" class="img_ev3q"></p></div>
<div align="justify"><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="challenge-2-channel-interactions-and-spillover-effects">Challenge 2: Channel Interactions and Spillover Effects<a href="https://ekimetrics.github.io/blog/Causal_Inference#challenge-2-channel-interactions-and-spillover-effects" class="hash-link" aria-label="Direct link to Challenge 2: Channel Interactions and Spillover Effects" title="Direct link to Challenge 2: Channel Interactions and Spillover Effects">​</a></h3><p>Traditional models often treat marketing channels as independent contributors, but modern marketing ecosystems are characterized by complex interactions. TV advertising might boost search volume, social media campaigns might amplify email effectiveness, and so on.</p><p>Causal modeling allows us to represent these dependencies mathematically:</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Latex_1-ae049261b66397d4acc4773c7331f7c5.png" width="3753" height="83" class="img_ev3q"></p></div>
<div align="left"><p>where</p><p><img decoding="async" loading="lazy" alt="screenshot-app " src="data:image/png;base64,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" width="1198" height="34" class="img_ev3q"></p></div>
<div align="justify"><p>represents the indirect effect of TV advertising on sales via its influence on search behavior. This captures the reality that when TV campaigns run, they often drive consumers to search for the product or brand online, creating an indirect pathway to conversion. By modeling these pathways explicitly, we can attribute effects more accurately and understand both direct and indirect impacts of each marketing channel.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="challenge-3-non-random-media-allocation">Challenge 3: Non-Random Media Allocation<a href="https://ekimetrics.github.io/blog/Causal_Inference#challenge-3-non-random-media-allocation" class="hash-link" aria-label="Direct link to Challenge 3: Non-Random Media Allocation" title="Direct link to Challenge 3: Non-Random Media Allocation">​</a></h3><p>Marketing investments aren't randomly assigned—they're strategically allocated based on past performance, creating a form of selection bias. This can lead to misleading conclusions when using traditional regression approaches.</p><p>The potential outcomes framework from causal inference helps address this by explicitly considering counterfactuals:</p></div>
<div align="left"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="data:image/png;base64,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" width="1211" height="27" class="img_ev3q"></p><p>where</p><p><img decoding="async" loading="lazy" alt="screenshot-app " src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABLAAAAAbCAMAAABba4bDAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAADPUExURQAAAAAAAP////////////////////////////////7+/v7+/v7+/v7+/v7+/v7+/vz8/P////////7+/v7+/v39/f39/f7+/v39/f39/f39/f////////////7+/vz8/P////////7+/v////////39/f39/f////7+/v////////7+/v////////7+/v////39/f////////////////////////7+/v///////+bm5v7+/v7+/v////f39/7+/v7+/v///6qqqv///////1Ozm/UAAABFdFJOUwABEUBHRkhEAWDj+vj0+/xZBmPpspSM5JeNkIIgOsdMAi62C0x+owm0eifnQlj3JJ/RF0H9eAX+WQMKy+44Htu8FAMlGtxpPEoAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAE0SURBVHhe7dprU1JRFIDhE6FSmEUESqZBVnTTTCvvdv3/v8l9zlnjwD9YzDzPB4a19vd3Zm+o7j1YFDuAlCJVRSwAsopaFbEAyCpqVcQCIJmI1KI4AUgmIrWYqaUBII22VkXMxdIAkJlgAStDsIC0Og+73bVusb7R6ZVZsICsHj3u9zef9Iutp88G7aNWHAGk8nz4YjQebe+MJ5OXu696Ta8EC8hob//1tKpmbw5iBkhr9LZ+tnr3ft6OAHnN6o/Bh4+DZmq4EgKJffr8Zdo+XtViCZDR4dHX9tfBRiwBMjr+dhLfapoF5DUfLr25CxaQ1+n3Hz8XroRFHABkMzk7vxAsYBVcXl3fTAULSK/E6fbX7z8aBayEv//+1/92B1g9roRAcu3jVYgdQEqRqlbsANKoqjt6BgvaI+gPRQAAAABJRU5ErkJggg==" width="1200" height="27" class="img_ev3q">
is the Average Treatment Effect</p></div>
<div align="justify"><p>Y(1) is the outcome with treatment, and</p><p>Y(0) is the outcome without treatment.</p><p>This framework forces us to consider what would have happened without a particular marketing intervention, providing a more accurate measure of true incremental impact.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="moving-toward-causal-solutions">Moving Toward Causal Solutions<a href="https://ekimetrics.github.io/blog/Causal_Inference#moving-toward-causal-solutions" class="hash-link" aria-label="Direct link to Moving Toward Causal Solutions" title="Direct link to Moving Toward Causal Solutions">​</a></h2><p>To address these challenges at Ekimetrics, we've developed a hybrid approach that integrates causal inference into the classical MMM pipeline. By introducing a causal graph, we incorporate human expertise into the model, allowing us to explicitly represent and validate the relationships between variables. This step transforms MMM from a correlation-based tool into a causality-driven framework capable of uncovering adjusted marketing effectiveness.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="mmm-preprocessing">MMM Preprocessing<a href="https://ekimetrics.github.io/blog/Causal_Inference#mmm-preprocessing" class="hash-link" aria-label="Direct link to MMM Preprocessing" title="Direct link to MMM Preprocessing">​</a></h3><p>Begin with established MMM preprocessing techniques:</p><ul>
<li>Adstock transformations to capture carryover effects</li>
<li>Saturation functions to model diminishing returns</li>
<li>Seasonality adjustments to account for cyclical patterns</li>
</ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="building-a-causal-graph">Building a Causal Graph<a href="https://ekimetrics.github.io/blog/Causal_Inference#building-a-causal-graph" class="hash-link" aria-label="Direct link to Building a Causal Graph" title="Direct link to Building a Causal Graph">​</a></h3><p>Next, we build a causal graph that visualizes relationships between variables. Unlike traditional approaches, this step embeds domain expertise directly into the model. For instance, media_X might depend on media_Y, media_Z. Our causal graph explicitly encodes this dependency and establishes direct and indirect paths to the outcome variable.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="validating-relationships">Validating Relationships<a href="https://ekimetrics.github.io/blog/Causal_Inference#validating-relationships" class="hash-link" aria-label="Direct link to Validating Relationships" title="Direct link to Validating Relationships">​</a></h3><p>Once the causal structure is in place, we validate it through a combination of rigorous methods to ensure robustness and credibility:</p><ul>
<li>
<p><strong>Independence Tests</strong><br>
<!-- -->We run statistical independence tests, such as the Chi-squared test, to verify whether the proposed relationships in the causal graph hold true in the data. These tests help confirm that the dependencies between variables are statistically significant and align with the causal structure.</p>
</li>
<li>
<p><strong>Overlap Testing</strong><br>
<!-- -->When conditioning on specific variables (e.g., during causal discovery or effect estimation), it's crucial to ensure sufficient data points remain for robust analysis. We carefully assess data quantity and overlap to avoid issues of sparsity that could compromise the validity of the results. This step ensures that the model remains reliable even after stratifying or subsetting the data.</p>
</li>
</ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="quantifying-causal-effects">Quantifying Causal Effects<a href="https://ekimetrics.github.io/blog/Causal_Inference#quantifying-causal-effects" class="hash-link" aria-label="Direct link to Quantifying Causal Effects" title="Direct link to Quantifying Causal Effects">​</a></h3><p>To understand the impact of each media campaign on sales, we use tools like DoWhy to estimate the Average Treatment Effect (ATE) for each media variable. The ATE provides a quantitative measure of how much a given campaign influences sales, both directly and indirectly.</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># For each channel, identify proper adjustment variables</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">identified_estimand </span><span class="token operator">=</span><span class="token plain"> model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">identify_effect</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">treatment</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">'TV_Spend'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                                           outcome</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">'Sales'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Estimate causal effect using a method that leverages traditional MMM strengths</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">causal_estimate </span><span class="token operator">=</span><span class="token plain"> model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">estimate_effect</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">identified_estimand</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                                      method_name</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">"backdoor.linear_regression"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>This analysis also helps uncover synergies and spillover effects between variables, shedding light on how different campaigns interact with each other to drive outcomes.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="ensuring-robustness">Ensuring Robustness<a href="https://ekimetrics.github.io/blog/Causal_Inference#ensuring-robustness" class="hash-link" aria-label="Direct link to Ensuring Robustness" title="Direct link to Ensuring Robustness">​</a></h3><p>Validation is a critical step in ensuring the reliability and practical utility of our causal model. Rather than simply accepting point estimates, we systematically quantify the uncertainty associated with our causal effect measurements—an approach that enhances both analytical rigor and decision-making transparency.</p><p>We employ several complementary techniques to measure uncertainty:</p><ul>
<li><strong>Confidence Intervals</strong>: Using bootstrapping approaches to generate confidence intervals around our Average Treatment Effect (ATE) estimates. This provides stakeholders with a clear understanding of the range within which the true causal effect likely falls.</li>
<li><strong>Hypothesis Testing</strong>: Calculating p-values that indicate the statistical significance of identified effects, helping differentiate between genuine causal relationships and potential statistical noises.</li>
<li><strong>Sensitivity Analysis</strong>: Systematically varying model assumptions to understand how robust our causal estimates are to different specifications and data conditions.</li>
</ul><p>This focus on uncertainty measurement acknowledges the inherent limitations of causal inference in real-world settings with finite data. We recognize that causal estimates may be imperfect when data volume is limited, measurement noise is present, or when important variables might be unobserved. By transparently communicating these uncertainties, we enable more informed decision-making.</p><p>We also reconcile the experiment's results with Business validation. This involves comparing your causal MMM results against actual experimental data to verify that your model's predictions match observed outcomes in controlled settings. This process serves as a reality check that bridges statistical models with empirical evidence.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="https://ekimetrics.github.io/blog/Causal_Inference#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><p>As marketing ecosystems become increasingly complex, the need for robust causal inference in MMM grows more critical. By acknowledging and addressing the causal nature of marketing effectiveness measurement, we can deliver more accurate and actionable insights to decision-makers.</p><p>Understanding MMM as a causal inference problem rather than just a regression task is crucial for modern marketing analytics. This perspective enables more accurate measurement of marketing effectiveness and better-informed decision-making in an increasingly complex digital landscape.</p><hr><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="glossary">Glossary<a href="https://ekimetrics.github.io/blog/Causal_Inference#glossary" class="hash-link" aria-label="Direct link to Glossary" title="Direct link to Glossary">​</a></h2><p><strong>Average Treatment Effect (ATE)</strong><br>
<!-- -->The mean difference in outcomes between units that received a treatment and units that did not. In marketing, this represents the average incremental impact of a marketing intervention (e.g., TV campaign) on a business outcome (e.g., sales).</p><p><strong>Causal Discovery</strong><br>
<!-- -->The process of learning causal relationships from observational data. This involves identifying which variables cause others and constructing a causal graph based on statistical patterns in the data, often combined with domain knowledge.</p><p><strong>Causal Graph</strong><br>
<!-- -->A visual representation of causal relationships between variables using a Directed Acyclic Graph (DAG). In marketing, causal graphs explicitly show how marketing channels influence each other and sales, as well as how external factors like seasonality affect both marketing decisions and outcomes.</p><p><strong>Directed Acyclic Graph (DAG)</strong><br>
<!-- -->A mathematical structure consisting of nodes (variables) and directed edges (causal relationships) with no cycles. DAGs provide a formal way to represent causal assumptions and identify proper adjustment sets for causal estimation.</p><p><strong>Confounding Variables</strong><br>
<!-- -->Variables that influence both the treatment (e.g., marketing spend) and outcome (e.g., sales), potentially creating spurious associations. In marketing, seasonality is a common confounder as it affects both marketing decisions and consumer behavior.</p><p><strong>DoWhy</strong><br>
<!-- -->An open-source Python library for causal inference that implements a four-step process: modeling, identification, estimation, and refutation. DoWhy provides tools to build causal graphs, identify proper estimation techniques, and validate causal assumptions.</p><p><strong>Overlap Testing</strong><br>
<!-- -->A validation technique that ensures sufficient data exists across different values of treatment variables to enable robust causal estimation. This helps avoid extrapolation beyond the support of the data.</p></div>]]></content:encoded>
            <category>Causal AI</category>
            <category>Explainable Marketing</category>
            <category>MMM</category>
        </item>
        <item>
            <title><![CDATA[How We Created Our First Talk-to-Data (Text-to-SQL) Application in Production]]></title>
            <link>https://ekimetrics.github.io/blog/Talk_to_Data_App</link>
            <guid>https://ekimetrics.github.io/blog/Talk_to_Data_App</guid>
            <pubDate>Mon, 17 Feb 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Delving into the R&D journey of creating a pioneering Text-to-SQL tool, exploring the intersection of data governance, business language challenges, and the semantic power of LLMs.]]></description>
            <content:encoded><![CDATA[<div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="1-talk-to-data-a-new-frontier">1. Talk to data: a new frontier<a href="https://ekimetrics.github.io/blog/Talk_to_Data_App#1-talk-to-data-a-new-frontier" class="hash-link" aria-label="Direct link to 1. Talk to data: a new frontier" title="Direct link to 1. Talk to data: a new frontier">​</a></h2><p>As 2023 drew to a close, we made a strategic decision to tackle one of the most promising challenges in data science: enabling natural language interactions with databases, technically known as "Text-to-SQL." Our conviction that 2024 would be the breakthrough year for talk-to-data applications was driven by several key developments: the release of GPT-4-turbo, intensifying competition in the AI space, declining implementation costs, expanded context windows, and unprecedented market momentum.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="2business-value">2.	Business Value<a href="https://ekimetrics.github.io/blog/Talk_to_Data_App#2business-value" class="hash-link" aria-label="Direct link to 2.	Business Value" title="Direct link to 2.	Business Value">​</a></h2><p>The business case for talk-to-data solutions extends beyond technical innovation. We identified 3 primary value propositions that resonated with our first customer:</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Use_Cases-efbb03f778bb37a399a94274a2c46013.png" width="1917" height="1078" class="img_ev3q"></p></div>
<div align="justify"><p>Market validation has since strengthened our conviction: for instance, Uber reports a 70% reduction in SQL query authoring time through such tools, translating to an estimated 140,000 hours saved monthly (source: <a href="https://medium.com/wrenai/how-uber-is-saving-140-000-hours-each-month-using-text-to-sql-and-how-you-can-harness-the-same-fb4818ae4ea3" target="_blank" rel="noopener noreferrer">https://medium.com/wrenai/how-uber-is-saving-140-000-hours-each-month-using-text-to-sql-and-how-you-can-harness-the-same-fb4818ae4ea3</a>).</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="3technical-challenges">3.	Technical Challenges<a href="https://ekimetrics.github.io/blog/Talk_to_Data_App#3technical-challenges" class="hash-link" aria-label="Direct link to 3.	Technical Challenges" title="Direct link to 3.	Technical Challenges">​</a></h2><p>You might wonder: since RAGs have proven effective with LLMs like GPT-3.5, why wouldn't they suffice for database interactions? After all, databases are structured in tabular formats, seemingly eliminating the need for parsing, chunking, or preprocessing.</p><p>However, despite the apparent simplicity of structured databases, four significant challenges make this task particularly complex:</p><ol>
<li>Ambiguity in Natural Language: varies from one user to another, and one question can have several answers</li>
<li>Complex SQL syntax: You may need nested queries, aggregations, filters and conditions</li>
<li>Schema &amp; Naming Understanding: Alignment between the user question, the format of data, the naming of fields, and then the SQL... are not trivial</li>
<li>Error Sensitivity: small errors in SQL lead to invalid queries or incorrect results, unlike natural language, where minor errors are often tolerated.</li>
</ol></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Timeline_Performance-ce7a3c2eb524c131856e470e1ade7a93.png" width="3300" height="948" class="img_ev3q"></p></div>
<div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="4-strategic-approach">4. Strategic Approach<a href="https://ekimetrics.github.io/blog/Talk_to_Data_App#4-strategic-approach" class="hash-link" aria-label="Direct link to 4. Strategic Approach" title="Direct link to 4. Strategic Approach">​</a></h2><p>Our initial analysis identified four potential implementation strategies, evaluated against specificity and robustness:</p><ol>
<li>A pure text-to-sql LLM</li>
<li>A text-to-sql LLM enhanced by various controls &amp; assistance</li>
<li>Aggregation of data in automatically generated small tables</li>
<li>A very laborious pattern extraction model to try to match natural language pieces with predefined SQL pieces</li>
</ol></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Potential_Methodologies-6166cf735a5de0aa003a74849d5b0ff0.png" width="1914" height="1074" class="img_ev3q"></p></div>
<div align="justify"><p>To fill in the gaps between these two axes, we chose the “text-to-SQL + controls” solution, and used the Vanna framework as our foundation to accelerate our developments.</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/LLM_SQL-770a17ab3892336ad2e61d2c7bdf8905.png" width="1918" height="1078" class="img_ev3q"></p></div>
<div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="5-implementation-journey">5. Implementation Journey<a href="https://ekimetrics.github.io/blog/Talk_to_Data_App#5-implementation-journey" class="hash-link" aria-label="Direct link to 5. Implementation Journey" title="Direct link to 5. Implementation Journey">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="architecture--initial-setup">Architecture &amp; Initial Setup<a href="https://ekimetrics.github.io/blog/Talk_to_Data_App#architecture--initial-setup" class="hash-link" aria-label="Direct link to Architecture &amp; Initial Setup" title="Direct link to Architecture &amp; Initial Setup">​</a></h3><p>To begin with, we created an architecture fitting with the client environment &amp; the architects’ requirements.  While the complete technical details are beyond this document's scope, here's a simplified overview:</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Components-71d1386d0c9f2350c55aca74ca520c56.png" width="1279" height="661" class="img_ev3q"></p></div>
<div align="justify"><p>To launch our work, we reframed the business need: the customer only wanted an application that would enable them to query their database: one question, one answer, directly in the form of data.</p><p>We quickly identified a critical challenge: how could business users without SQL expertise or familiarity with the available tables verify the accuracy and relevance of the answers?</p><p>To address this issue, we immediately incorporated a natural language reformulation of the SQL query - which only addresses part of the problem, however. And to improve regularly the tool, we integrated human feedback loops through example pairs (question and SQL answer).</p><p>Our initial implementation included:</p><ul>
<li>A dockerised architecture connecting back and front via FastAPI</li>
<li>Vanna 0.0.3x</li>
<li>A GPT 3.5 turbo (to limit costs on iterations, which proved useful at first)</li>
<li>Access to 2-3 tables in an initially fixed format</li>
</ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="implementation-challenges--solutions">Implementation Challenges &amp; Solutions<a href="https://ekimetrics.github.io/blog/Talk_to_Data_App#implementation-challenges--solutions" class="hash-link" aria-label="Direct link to Implementation Challenges &amp; Solutions" title="Direct link to Implementation Challenges &amp; Solutions">​</a></h3><p>Our work initially consisted of testing and identifying problems, the better to reflect on them and devise solutions. Working with limited-capability LLMs highlighted the importance of robust code and precise prompting. Here are the key challenges we encountered:</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Steps_Q-169b2d92b7ac934ac6e3aba47dc7f7bd.png" width="1908" height="1071" class="img_ev3q"></p></div>
<div align="justify"><p>We developed several solutions to address these challenges. Below are three key challenges and the solutions we developed:</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="a-problems-linked-to-the-database">a/ Problems linked to the database:<a href="https://ekimetrics.github.io/blog/Talk_to_Data_App#a-problems-linked-to-the-database" class="hash-link" aria-label="Direct link to a/ Problems linked to the database:" title="Direct link to a/ Problems linked to the database:">​</a></h3><p>To manage problems linked to a change in the data (change of name, addition of field, modality, etc.), we created 3 types of referential that enabled us to observe and integrate table changes as automatically as possible, even when they were not communicated:</p><ol>
<li>A replica of the data dictionary (Excel) used by the data team, listing the table name, field, type, description and an example of a value.</li>
<li>A configuration automatically generated from the data available at the time of generation, listing the table schemas (CREATE TABLE ...), and examples of SQL queries automatically translated into natural language, the main purpose of which is to link a word with a field in a table (list the countries France, GB and Germany // SELECT country FROM table_A WHERE country in (‘FRA’, ‘GBR’, ‘DEU’))</li>
<li>A repository of field values in STRING format</li>
</ol><p>These repositories do not solve the fundamental problem of data instability and communication problems between teams, but they do provide a clear picture of what is being used, enable alerts to be raised quickly, and allow the most automatic possible adaptation to changes in the databases. Because the best response is above all to have real data governance, quality controls, bodies and processes for validating and communicating changes.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="b-problems-linked-to-the-lack-of-correspondence-between-the-business-language-and-the-database-language">b/ Problems linked to the lack of correspondence between the business language and the database language<a href="https://ekimetrics.github.io/blog/Talk_to_Data_App#b-problems-linked-to-the-lack-of-correspondence-between-the-business-language-and-the-database-language" class="hash-link" aria-label="Direct link to b/ Problems linked to the lack of correspondence between the business language and the database language" title="Direct link to b/ Problems linked to the lack of correspondence between the business language and the database language">​</a></h3><p>This is the major difficulty with TTD.</p><p>There are several factors, and therefore several solutions:</p><ul>
<li>
<p><strong>Unclear questions</strong></p>
<ul>
<li>
<p>Example: what is the price of XXX -&gt; no date, no distinction between new and used, etc.</p>
</li>
<li>
<p>Solution:</p>
<ul>
<li>Add default filters (e.g. most recent date)</li>
<li>Choose one of several possible tables, and specify this in the SQL query reformulation in natural language.</li>
<li>Solutions based on a multi-turn discussion with LLM:<!-- -->
<ul>
<li>LLM points out the ambiguities of the formulation and asks for clarification</li>
<li>LLM shows what related data are available, and asks for clarification</li>
<li>LLM suggests a reformulation, and asks for validation before generating the SQL</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Questions that are too specific</strong></p>
<ul>
<li>
<p>Example: ‘What is the price of X on 1 January at 1.03pm according to website Y in region Z?’ when the information is only available on average on a monthly basis.</p>
</li>
<li>
<p>Solution:</p>
<ul>
<li>In general, the LLM responds as best it can, and rephrasing allows the user to be informed of the level of detail in the response. But be careful, too much detail can lead the LLM to add aberrant filters - which again is visible in the reformulation.</li>
<li>Solutions based on a multi-turn discussion with LLM (see above)</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>An implicit context</strong></p>
<ul>
<li>
<p>Example:</p>
<ul>
<li>User A, living in France: what is the price of XXX? (implied: in France)</li>
<li>User B, living in the UK: what is the price of XXX? (implied: in the UK)</li>
</ul>
</li>
<li>
<p>Solutions (not all integrated):</p>
<ul>
<li>Personalise the TTD with a simple prompt presenting the user's context</li>
<li>Add default filters based on country, entity, etc.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Unclear field names</strong></p>
<ul>
<li>
<p>Example: field names such as ‘DL_MKT_TIME’. The description of the field allows the LLM to understand it, but the name of the field is very important.</p>
</li>
<li>
<p>Solutions:</p>
<ul>
<li>Create views dedicated to TTD</li>
<li>Use a business name in the TTD code, which will be replaced on the fly by its technical name when the request is made.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Jargon in user questions</strong></p>
<ul>
<li>
<p>Example: business abbreviations (‘bb’ for ‘buyback’, etc.).</p>
</li>
<li>
<p>Solution:</p>
<ul>
<li>Specific jargon must necessarily be given to the LLM, either upstream (FT) or in the document base (which feeds the prompt).</li>
<li>Hybrid search can be used to retrieve specific business vocabulary that is not well represented in the latent vector space.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Table formats (aggregation, filter, etc.) of which the user is unaware, leading to complex or impossible queries</strong></p>
<ul>
<li>
<p>Example: ‘what is the average selling price of X’ on a table aggregated by shop (requiring a weighted average).</p>
</li>
<li>
<p>Solutions:</p>
<ul>
<li>
<p>Information &amp; instructions:</p>
<ul>
<li>The description of the tables and fields must be clear about their level of aggregation.</li>
<li>More examples</li>
</ul>
</li>
<li>
<p>Safeguards:</p>
<ul>
<li>Identify certain patterns in the query, then raise an error with a corrective message for the LLM</li>
<li>Even on-the-fly replacement of certain operations</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Specific field formats</strong></p>
<ul>
<li>Example: two fields for a single word, several words in one field value, etc.</li>
<li>Solution: in this case, the best solution is still to change the data. If this isn't possible and TTD doesn't work as expected, you can use ad-hoc prompts and fairly open SQL queries (e.g. ‘LIKE’ instead of ‘=’).</li>
</ul>
</li>
</ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="c-performance-drift--speed-of-development">c/ Performance drift &amp; speed of development<a href="https://ekimetrics.github.io/blog/Talk_to_Data_App#c-performance-drift--speed-of-development" class="hash-link" aria-label="Direct link to c/ Performance drift &amp; speed of development" title="Direct link to c/ Performance drift &amp; speed of development">​</a></h3><p>To test the performance, a dedicated team of users asked regularly new questions (not the golden questions) to the tool on various tables &amp; topics, the metric being simply here the rate of correct answers on answerable questions.</p><p>The final difficulty, linked to the high error sensitivity of text-2-sql, was its initial instability during development.
To remedy this, two minimum solutions quickly became apparent.</p><ol>
<li>Any new change must be accompanied by a drift measurement (loss of reliability on questions that have already been mastered).</li>
<li>By adding tables, new prompt elements can be confused with older ones, which were not designed at the same time. To counter this, a table assignment mechanism upstream of the chain should enable the LLM to be directed towards a list of candidate tables.</li>
</ol><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="6future-outlook">6.	Future Outlook<a href="https://ekimetrics.github.io/blog/Talk_to_Data_App#6future-outlook" class="hash-link" aria-label="Direct link to 6.	Future Outlook" title="Direct link to 6.	Future Outlook">​</a></h2><p>The landscape of talk-to-data solutions continues to evolve rapidly, with major players like Databricks and Google introducing interesting offerings. The Bird SQL benchmark continues to fill up with new records, and LLMs have greatly improved their code generation, thanks in particular to the ‘test time compute’ approach.</p><p>But our experience shows that all these use cases will require the following fundamental elements: excellent data governance, a solid business sense on the part of the players responsible for maintaining text-to-SQL-based products, and well-parametrized tools to ensure robust responses - expertise and solutions that we've developed and proven to be consistently valuable.</p><p>The future of text-to-SQL applications depends not just on technological advancement, but on the thoughtful integration of these fundamental elements.</p></div>]]></content:encoded>
            <category>Innovation</category>
            <category>R&amp;D</category>
            <category>AI</category>
            <category>GenAI</category>
            <category>Text-to-SQL</category>
            <category>NLP</category>
            <category>Data Science</category>
        </item>
        <item>
            <title><![CDATA[Where LLMs still fail]]></title>
            <link>https://ekimetrics.github.io/blog/LLMs_fail</link>
            <guid>https://ekimetrics.github.io/blog/LLMs_fail</guid>
            <pubDate>Mon, 25 Nov 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[An exploration of current limitations in Large Language Models (LLM) through concrete examples, revealing their surprising struggles with seemingly simple tasks.]]></description>
            <content:encoded><![CDATA[<div align="justify"><p>The field of Natural Language Processing (NLP) has experienced an unprecedented explosion in recent years, and at the heart of this revolution are Large Language Models (LLMs). These powerful tools have transformed our understanding of what machines can achieve, rivalling human cognition in many areas. In just a few short years, LLMs have evolved from generating pseudo-coherent text in English (GPT-2; 2019) to enormous models that possess knowledge besting most human experts in specific domains (o1, claude 3.5; 2024).</p><p>At Ekimetrics, we're thrilled to be working with clients to harness the potential of these new technologies, exploring exciting applications such as Retrieval-Augmented Generation (RAG), KPI extraction, or social media content generation. We use LLMs to manage and format knowledge in all its forms.</p><p>Our team has witnessed first-hand the incredible capabilities of LLMs, and we're eager to continue pushing the boundaries of what's possible. However, despite their impressive abilities, even the best LLMs are still not without weaknesses.</p><p>In this article, we'll delve into concrete examples of LLMs struggling with seemingly trivial tasks and attempt to understand the underlying reasons for these failures. By exploring the limitations of LLMs, we hope to gain a deeper appreciation for their capabilities and develop strategies to overcome their weaknesses, ultimately unlocking their full potential.</p><p>The observations and insights shared in this article are purely my personal perspective, drawn from extensive experience working with Large Language Models (LLMs) in both research and production environments. As someone deeply immersed in this field at Ekimetrics, I recognize that LLMs are fundamentally complex "black box" systems. The examples and analysis that follow represent my professional observations and interpretations, but they are not definitive scientific conclusions.</p><p>All the examples below are generated using the latest version of ChatGPT available as of October 21, 2024. I chose ChatGPT because it is the most widely used and well-known access to an LLMs for most people, and it hosts state-of-the-art models from OpenAI. All messages you see are the first answers I got from ChatGPT, there are no previous messages, and no system prompt.</p><p>As a bonus, see o1 answers at the end of the article. It only get 1 out of the 3 examples right.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="misguided-attention">Misguided Attention<a href="https://ekimetrics.github.io/blog/LLMs_fail#misguided-attention" class="hash-link" aria-label="Direct link to Misguided Attention" title="Direct link to Misguided Attention">​</a></h2><p>In the realm of large language models (LLMs), one intriguing phenomenon that often leads to errors is what we could refer to as <a href="https://github.com/cpldcpu/MisguidedAttention" target="_blank" rel="noopener noreferrer">misguided attention</a>. This occurs when a LLM encounters a problem or prompt that closely resembles a well-known scenario it has encountered during its training. The model, recognizing the familiar pattern, may prematurely jump to conclusions without fully processing the nuances of the current situation. This is akin to a human cognitive bias known as the <a href="https://en.wikipedia.org/wiki/Einstellung_effect" target="_blank" rel="noopener noreferrer">Einstellung effect</a>, where prior experience with similar problems causes individuals to apply previously learned solutions inappropriately.</p><p>A classic example of this is the "Dead Schrödinger's cat" scenario. In this twist on <a href="https://en.wikipedia.org/wiki/Schr%C3%B6dinger's_cat" target="_blank" rel="noopener noreferrer">the famous thought experiment</a>, a dead cat is placed into a box alongside a nuclear isotope, a vial of poison, and a radiation detector. If the detector picks up radiation, it releases the poison. The question is: "What is the probability of the cat being alive when the box is opened a day later?"</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Schrodinger_cat-133e0c6b0aed6cf20c81577cc5a7c8a5.jpg" width="800" height="426" class="img_ev3q"></p></div>
<p>Now watch what happens if we ask ChatGPT this simple question:</p>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Chat_GPT_question-c43eda3ecc0acab4d8f9f484795624fb.jpg" width="696" height="800" class="img_ev3q"></p></div>
<div align="justify"><p>Almost all LLMs, upon encountering this problem, default to their learned response from Schrödinger's original paradox, which involves a living cat in a superposition of states. They fail to consider the critical twist—that the cat starts out dead. This oversight occurs because the model's attention latches onto the familiar elements of the problem and neglects to process the new information. Some kind of "Oh! I already know this one!" effect.</p><p>This phenomenon is intricately tied to the way these models are trained. LLMs are trained on massive datasets comprising diverse text inputs from the internet, books, articles, and other sources. During this training process, the models learn to identify patterns, structures, and associations within the data. They use these patterns to generate predictions about the next token in a sequence, based on the context provided by preceding tokens. This ability to predict and generate text is largely driven by statistical correlations rather than true understanding or reasoning.</p><p>As a result, LLMs develop a keen sense for recognizing familiar patterns and scenarios that resemble those they have encountered during training. This can lead them to quickly latch onto well-known narratives or problem templates, even when subtle differences or new information require a different approach. The reliance on pattern recognition means that when a LLM encounters a prompt that closely mirrors a familiar scenario, it may default to the response it has learned for that scenario, without noticing new elements in the context. Even when new elements can be as critical as in this example. This is akin to how humans sometimes rely on heuristics or mental shortcuts based on past experiences, potentially leading to errors in judgment when faced with novel situations. Understanding this aspect of LLM training helps illuminate why misguided attention occurs and underscores the importance of developing strategies to mitigate its effects.</p><p>The challenge of misguided attention highlights both the potential and the pitfalls of LLMs. While their power lies in pattern recognition, enabling them to draw from vast reserves of knowledge, this same reliance can sometimes become their worst enemy when they encounter novel twists on familiar themes. By understanding these limitations and refining how we use LLMs, we can continue to enhance their reasoning capabilities, all the while mitigating risks such as hallucinations for our clients, thus unlocking even more sophisticated applications in natural language processing.</p><br><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="following-instructions">Following instructions<a href="https://ekimetrics.github.io/blog/LLMs_fail#following-instructions" class="hash-link" aria-label="Direct link to Following instructions" title="Direct link to Following instructions">​</a></h2><p>LLMs are fundamentally constrained by their next-token prediction mechanism, generating text one token at a time without the ability to revise previous output. Unlike humans, who can reflect on and adjust their thoughts during writing, LLMs are locked into a linear, sequential process. This limitation can lead to inconsistencies or unintended outputs, especially when handling specific formatting requirements.</p><p>In the next example, I ask the model to write sentences that end with a particular word.</p><div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Ask_sentence-3d17ab22776dd61e9a3daf82ade431ba.jpg" width="800" height="468" class="img_ev3q"></p></div><p>To give you an idea of how bad LLMs are at this task, I couldn’t even use one to reflect on the error, because they do not see any problem and consider the task completed with success.</p><div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Grade_task-f7f9c0aa56383b38519f60368482ac5a.jpg" width="800" height="441" class="img_ev3q"></p></div><p>We can only try to guess why LLMs fail so spectacularly at this task. Here is my guess. While writing the sentence, the LLMs can only predict the next token. They “know” they must include “houses” in the sentence but the sense of writing nice sentences seems to overtake their will to respect the user instructions. Often the sentence could have ended after the token “houses”, as if the model had planned to stop, but it can’t help but adding a few more words for the sentence to sound nice. When using ChatGPT through the API, we can observe this behavior but cannot directly modify the model's output process. However, if we were to work with a local model where we have more control, we could potentially address this by forcing the model to stop at the desired word by manually inserting end-of-sentence tokens like ".\n" immediately after the target word. We could also investigate on the learned probabilities to select the next tokens in those situations, to estimate the willingness of the model to follow instructions over writing nice sentences.</p><br><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="numbers">Numbers<a href="https://ekimetrics.github.io/blog/LLMs_fail#numbers" class="hash-link" aria-label="Direct link to Numbers" title="Direct link to Numbers">​</a></h2><p>LLMs perceive language as a sequence of tokens, rather than words and numbers. This tokenization process can lead to difficulties when processing numbers or proper nouns, as the model struggles to understand the context and relationships between these entities. For instance, when faced with mathematical equations, LLMs may falter, as they're forced to rely on statistical patterns rather than true comprehension of the operations to make.</p><div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Numbers-1580f35c35e2f01189208290bb9acf00.jpg" width="800" height="297" class="img_ev3q"></p></div><p>The numerical comparison error in the image illustrates an interesting limitation of LLMs. In this instance, the model incorrectly concludes that 3.11 is greater than 3.9. The reason for this error may lie in how software version numbers are typically formatted - for example, Python releases follow a sequence like 2.0, 2.1, ..., 2.9, 2.10. Since the model has frequently encountered version numbers where 3.9 precedes 3.11, and commonly sees phrases like "version 3.9 or higher" (which includes 3.10 and 3.11), this pattern may have influenced its numerical reasoning. The model has thus learned this relationship between these numbers but struggles to discern that context matters and that by default the number 3.9 remains larger than the number 3.11. This also helps explain why it seems confused in its explanation "since 90 is bigger than 11, 3.9 is smaller than 3.11". It's almost like the model "knows" his first take was wrong but is too afraid to look bad by correcting itself.</p><p>This example is perfect to explain why chain-of-thoughts -before answering- are crucial for LLMs. Asking the model to explain its reasoning after it already gave its answer is just glorified post-hoc rationalization. However, prompting the model to write a justification before its answer can help it navigate the complexity of the task more effectively, leading to a more accurate final response.</p><br><p>Despite their remarkable achievements, LLMs continue to exhibit vulnerabilities in seemingly simple tasks. Through our exploration of misguided attention, next-token prediction limitations, and tokenization challenges, we've seen how these powerful models can still stumble in unexpected ways. These limitations remind us that LLMs process language very differently from humans. Understanding these shortcomings is crucial not just for academic interest, but for practical applications in business settings. At Ekimetrics, this knowledge helps us design more robust solutions by anticipating potential pitfalls and implementing appropriate guardrails, such as chain-of-thought prompting. As the field continues to evolve, with models like OpenAI's o1 showing promising improvements, we remain both optimistic about the future potential of LLMs and mindful of their current limitations. This balanced perspective is essential for anyone working to harness these powerful tools effectively.</p><br><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="bonus-o1-answers">Bonus: o1 answers<a href="https://ekimetrics.github.io/blog/LLMs_fail#bonus-o1-answers" class="hash-link" aria-label="Direct link to Bonus: o1 answers" title="Direct link to Bonus: o1 answers">​</a></h2><p><a href="https://openai.com/index/introducing-openai-o1-preview/" target="_blank" rel="noopener noreferrer">o1</a> is the latest model by OpenAI and n°1 LLM on most -and especially logical- benchmarks. It uses chain-of-thought techniques to process user requests, decomposing and analyzing information before giving its final answer. One might think this state-of-the-art model, made to think, reflect, and perform complex tasks, would ace those examples. This is also what I thought, before testing the model on those same prompts. Of the three tricky examples I chose, they only got 3.9 &gt; 3.11 correct.</p><p>Below are o1-preview answers.</p><div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/o1_answer_1-8f58c033e9afe9ea641490c11990dad1.jpg" width="800" height="540" class="img_ev3q"></p></div><div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/o1_answer_2-da82975d052366266e86ee0a14905250.jpg" width="592" height="404" class="img_ev3q"></p></div><div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/o1_answer_3-57e035afbe789b68e623747c7b6cbe02.jpg" width="754" height="653" class="img_ev3q"></p></div></div>]]></content:encoded>
            <category>Generative AI</category>
            <category>LLM</category>
            <category>NLP</category>
        </item>
        <item>
            <title><![CDATA[Optimizing Chatbot Performance: A Two-Phase Methodology for Enhanced Performance in HR Policy Responses]]></title>
            <link>https://ekimetrics.github.io/blog/Wombat_HR</link>
            <guid>https://ekimetrics.github.io/blog/Wombat_HR</guid>
            <pubDate>Tue, 15 Oct 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[At Ekimetrics, we've tackled the challenge of ensuring the relevance and accuracy of chatbot responses by developing a cutting-edge chatbot system designed to navigate the complex landscape of HR policies.]]></description>
            <content:encoded><![CDATA[<div align="justify"><p>In the ever-evolving landscape of AI-driven solutions, ensuring the relevance and accuracy of chatbot responses is paramount, especially when dealing with sensitive and critical knowledge bases such as HR policies. At Ekimetrics, we've tackled this challenge head-on by developing a cutting-edge chatbot system designed to navigate the complex landscape of HR policies. Our approach leverages Retrieval Augmented Generation (RAG) and employs a rigorous two-phase optimization process. In this post, we'll walk you through our methodology, sharing insights that can be applied to enhance chatbot performance across various domains.</p><p>HR policies form the backbone of any organization, guiding everything from leave policies to workplace conduct. However, the sheer volume and complexity of these policies can make it challenging for employees to quickly find the information they need. Our goal was to create a chatbot that could provide accurate, context-aware responses to HR queries, effectively serving as a 24/7 HR assistant. To achieve this, we developed a sophisticated optimization methodology divided into two distinct phases: enhancing source retrieval and fine-tuning answer generation. Let's dive into each phase, illustrated by our custom-designed infographics.</p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="phase-1-optimizing-source-retrieval">Phase 1: Optimizing Source Retrieval<a href="https://ekimetrics.github.io/blog/Wombat_HR#phase-1-optimizing-source-retrieval" class="hash-link" aria-label="Direct link to Phase 1: Optimizing Source Retrieval" title="Direct link to Phase 1: Optimizing Source Retrieval">​</a></h2>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Source_Retrieval-2d488c7929b96509d6ca8a321d4fa0b5.png" width="604" height="340" class="img_ev3q"></p></div>
<div align="justify"><p>The first phase of our methodology was dedicated to optimizing the retrieval of relevant documents that the chatbot uses to generate answers.</p><p>This phase is crucial as the quality of the final answer is directly influenced by the relevance of the retrieved sources.</p><p>As shown in the "Sources Retrieval Pipeline" infographic, our process involves several key steps:</p><ol>
<li><strong>Document Processing</strong>:</li>
</ol><ul>
<li>Parsing PDF documents into raw content</li>
<li>Cleaning and slicing the content using GPT-4</li>
<li>Embedding the document chunks with OpenAI ADA</li>
<li>Indexing the embeddings in a vector database</li>
</ul><ol start="2">
<li><strong>Query Processing</strong>:</li>
</ol><ul>
<li>Reformulating the user's question using GPT-4</li>
<li>Retrieving relevant sources from the vector database</li>
<li>Incorporating conversation history for context</li>
</ul><p>Key parameters we targeted for optimization included:</p><ul>
<li><strong>k</strong>: The number of documents retrieved in response to a query</li>
<li><strong>min_similarity</strong>: The minimum similarity threshold required for a source document to be considered relevant</li>
</ul><p>Process: We began by defining the parameters k and min_similarity, which directly impacted the chatbot’s ability to fetch the most relevant sources from the HR policy knowledge base. To optimize these parameters, we employed Optuna, a powerful hyperparameter optimization framework. The objective was to fine-tune k and min_similarity to maximize the relevance of retrieved documents, thereby setting a solid foundation for the subsequent answer generation phase.</p><p>Here is the code snippet that demonstrates the optimization process:</p></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">import mlflow</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">import optuna</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">import gc</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">def objective(trial):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  with mlflow.start_run(nested=True):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    gc.collect()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    k = trial.suggest_int("k", 1, 4)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    min_similarity = trial.suggest_float("min_similarity", 0, 0.04)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    chunk_size = trial.suggest_int('chunk_size', 200, 2000)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    metrics = get_metrics(k, min_similarity, indexes_names_template='hr-dev-{chunk_size}')</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    mlflow.log_params({"k": k, "min_similarity": min_similarity, "chunk_size": chunk_size})</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    log_metrics(metrics)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    gc.collect()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return -1*metrics['f1_score']</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">with mlflow.start_run(run_name="tuned_wombat_chatbot"):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  study = optuna.create_study()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  study.optimize(objective, n_trials=200)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">best_params = study.best_params</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">print(f"best_params : {best_params} with final f_score of : {-1* study.best_value} on trial {study.best_trial}")</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div align="justify"><p>To create a realistic testbed, we engaged our Head of HR to formulate answers to 30 representative questions that the chatbot might encounter. These questions then served as our benchmark to systematically evaluate different configurations of k and min_similarity. By iterating through various combinations, we identified the optimal settings that resulted in the highest precision and recall rates during the retrieval process.</p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="phase-2-fine-tuning-answer-generation">Phase 2: Fine-tuning Answer Generation<a href="https://ekimetrics.github.io/blog/Wombat_HR#phase-2-fine-tuning-answer-generation" class="hash-link" aria-label="Direct link to Phase 2: Fine-tuning Answer Generation" title="Direct link to Phase 2: Fine-tuning Answer Generation">​</a></h2>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Answer_Generation-c6bb9c061846ce00cab480aee58f5eb1.png" width="604" height="340" class="img_ev3q"></p></div>
<div align="justify"><p>The second phase focused on refining the chatbot's ability to generate final answers that are not only accurate, but also contextually appropriate, based on the retrieved documents.</p><p>Key components of this phase include:</p><ol>
<li><strong>Question Reformulation</strong>: Using GPT-4 to enhance the original user question</li>
<li><strong>Source Retrieval</strong>: Fetching the most relevant documents based on the reformulated question</li>
<li><strong>Answer Generation</strong>: Combining retrieved sources, conversation history, and the enhanced question to generate a final answer using GPT-4</li>
</ol><p>Key focus areas for optimization included:</p><ul>
<li><strong>Temperature</strong>: Controls the randomness of the language model’s predictions</li>
<li><strong>Prompts</strong>: Customizable templates that guide the chatbot in generating responses</li>
<li><strong>chunk_size</strong>: Optimizing the size of text chunks processed by the system</li>
</ul><p>Process: With the retrieval pipeline optimized, the next step was to fine-tune the parameters governing the language model’s behavior during answer generation. We adjusted the temperature parameter to balance creativity with accuracy, ensuring that the chatbot's responses remained relevant without deviating from the core content of the HR policies.</p><p>In addition to temperature, we experimented with different prompts that directed the chatbot on how to structure its responses. This step involved crafting and testing various prompt versions to determine which formulations yielded the most accurate and contextually aligned answers.</p><p>Similar to the retrieval optimization, we used the same set of 30 HR-related questions and answers for evaluation. By comparing the chatbot's generated answers against the expected outcome, we employed a bespoke similarity metric to identify the most effective parameter configurations. These similarity metrics were computed by an external model called an LLM Judge that assessed the semantic similarity between the chatbot's responses and the expected answers.</p><h1>Answer Similarity Assessment</h1><p>This involved using a custom prompt to assess the semantic similarity between the chatbot's responses and the expected answers. By scoring this similarity and providing justifications, we gained additional insights into the chatbot's ability to produce responses that align closely with the intended content.</p><details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Here is the prompt that we used for the similarity assessment!</summary><div><div class="collapsibleContent_i85q"><p><strong>Task</strong>:</p><p>You must return the following fields in your response in two lines, one below the other:
score: Your numerical score for the model's answer_similarity based on the rubric
justification: Your reasoning about the model's answer_similarity score</p><p>You are an impartial judge. You will be given an input that was sent to a machine learning model, and you will be given an output that the model produced. You may also be given additional information that was used by the model to generate the output.</p><p>Your task is to determine a numerical score called answer_similarity based on the input, output, and target.
A definition of answer_similarity and a grading rubric are provided below.
You must use the grading rubric to determine your score. You must also justify your score.</p><p>Examples could be included below for reference. Make sure to use them as references and to understand them before completing the task.</p><p>Input:
<code>{input}</code></p><p>Output:
<code>{output}</code></p><p>Target:
<code>{target}</code></p><p><strong>Metric definition</strong>:
Answer similarity is evaluated on the degree of semantic similarity of the provided output to the provided targets, which is the ground truth. Scores can be assigned based on the gradual similarity in meaning and description to the provided targets, where a higher score indicates greater alignment between the provided output and provided targets.</p><p><strong>Grading rubric</strong>:
Answer similarity: Below are the details for different scores:</p><ul>
<li>Score 1: The output has little to no semantic similarity to the provided targets.</li>
<li>Score 2: The output displays partial semantic similarity to the provided targets on some aspects.</li>
<li>Score 3: The output has moderate semantic similarity to the provided targets.</li>
<li>Score 4: The output aligns with the provided targets in most aspects and has substantial semantic similarity.</li>
<li>Score 5: The output closely aligns with the provided targets in all significant aspects.</li>
</ul><p><strong>Examples</strong>:</p><p>Example Input:
What is MLflow?</p><p>Example Output:
MLflow is an open-source platform for managing machine learning workflows, including experiment tracking, model packaging, versioning, and deployment, simplifying the ML lifecycle.</p><p>Example Target:
MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.</p><p>Example score: 4
Example justification: The definition effectively explains what MLflow is, its purpose, and its developer. It could be more concise for a 5-score.</p><p>You must return the following fields in your response in two lines, one below the other:
score: Your numerical score for the model's answer_similarity based on the rubric
justification: Your reasoning about the model's answer_similarity score
Do not add additional new lines. Do not add any other fields.</p></div></div></details><p><a href="https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_similarity" target="_blank" rel="noopener noreferrer">https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_similarity</a></p><h1>Hyperparameter Optimization and Evaluation</h1><p>Both phases utilized Optuna for hyperparameter optimization, with a focus on minimizing the negative F1 score for improving the retrieval quality, and minimizing the negative similarity score of the final answer relevance. Throughout the optimization process, all relevant metrics and parameters were logged using MLflow, ensuring comprehensive tracking and enabling systematic comparisons between different experimental runs.</p><h1>Potential Biases and Limitations</h1><p>The obvious bias in the above process stems from the fact that the benchmark answers are written or vetted by a single individual, although this individual, as head of HR, has the final authority over what constitutes an acceptable answer. Nonetheless, any idiosyncrasies in how a single individual crafts sentences and structures answers are likely to have been picked up when computing similarity scores. Further, any development of chatbots for general usage will likely be optimized for sounding friendly and empathetic in a way that may transcend a single individual’s writing style.</p><p>For instance, a user may expect interjections and exclamations such as “Congratulations” or “Commiserations” in output answers warranted by a given situation, which may deviate from sample answers that may be more factual. Our team had to balance such factors in their quest to achieve high similarity scores.</p><h1>Conclusion</h1><p>The methodology outlined here presents a robust, two-phase approach to optimizing a chatbot system designed to navigate the complex and sensitive domain of HR policies. By systematically refining both the source retrieval process and the answer generation mechanism, we have developed a chatbot that is both precise and contextually aware. The integration of advanced hyperparameter optimization techniques, combined with rigorous evaluation and tracking through MLflow, underscores the scientific rigor of this approach. As a result, the chatbot is well-equipped to deliver high-quality, relevant answers, enhancing user experience and ensuring that critical HR-related queries are addressed with the utmost accuracy. Finally, this approach holds relevance for topics that transcend HR – the methodology is just as applicable to building RAGs in any area or domain.</p></div>]]></content:encoded>
            <category>GenAI</category>
            <category>HR</category>
            <category>Chatbot</category>
            <category>Q&amp;A</category>
            <category>HR policies</category>
            <category>Automation</category>
        </item>
        <item>
            <title><![CDATA[Bayesian Neural Networks]]></title>
            <link>https://ekimetrics.github.io/blog/Bayesian_NN</link>
            <guid>https://ekimetrics.github.io/blog/Bayesian_NN</guid>
            <pubDate>Fri, 07 Jun 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Bayesian neural networks (BNNs) is a pioneering approach that embraces uncertainty as a strength.]]></description>
            <content:encoded><![CDATA[<div align="justify"></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/bayesian_NN-76183d22af78ff37820dfeeb29354336.jpg" width="4256" height="2832" class="img_ev3q"></p></div>
<div align="justify"><p>Artificial neural networks (ANNs) have revolutionized countless fields, from powering self-driving cars to aiding medical diagnoses. Yet, these marvels often lack a crucial human trait: the ability to say "I don't know." While ANNs excel at making predictions, they struggle with uncertainty, confidently providing answers even when data is limited or unreliable. This can lead to misleading results, especially in critical areas where trust is paramount.</p><p>Enter Bayesian neural networks (BNNs), a pioneering approach that embraces uncertainty as a strength. Inspired by Bayesian statistics, BNNs confidently acknowledge "I don't know" when the data gets hazy. This seemingly simple addition unlocks a treasure trove of benefits:</p><ul>
<li><strong>Confidence, not certainty</strong>: BNNs don't just predict, they provide confidence intervals, allowing users to understand the level of trust they can place in each answer. This leads to more informed decision-making, especially in critical situations.</li>
<li><strong>Real-world robustness</strong>: Built to handle the noise and outliers inherent in real-world data, BNNs are more reliable in uncertain environments, offering more trustworthy results.</li>
<li><strong>Active learning</strong>: BNNs can pinpoint their areas of highest uncertainty, actively seeking out the most informative data points to refine their knowledge. This continuous learning loop makes them even more powerful.</li>
</ul><p>This fascinating world of BNNs holds immense potential, not just for improving traditional AI applications, but for building trust and transparency in a future increasingly reliant on artificial intelligence. As we delve deeper into their inner workings, explore their diverse applications, and discuss the exciting challenges and opportunities they present, be prepared to witness how "I don't know" becomes the secret weapon of the future's AI.</p></div>
<h1>Neural Networks Essentials</h1>
<div align="justify"><p>In order to understand the way bayesian neural networks, we will have to dive into what bayesian neural networks are based on : artificial neural networks.</p><p>Artificial neural networks are part of the Artificial Intelligence that aims at replicating the way human brain works, this field is also commonly called “Deep Learning”.</p><p>An artificial neural network can be used for 2 main tasks :</p><div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Regression-34c24414298e318144bdf0779fe4ecf5.png" width="1028" height="578" class="img_ev3q"></p></div><ol>
<li>
<p>Classification: task where the network learns to map input data to discrete categories or classes. This is achieved through the training process, where the network adjusts its weights and biases to minimize the difference between predicted class probabilities and the actual class labels in the training data.</p>
</li>
<li>
<p>Regression: task where the network is trained to predict a continuous output variable based on input data. In regression tasks, the network learns to capture the underlying patterns and relationships in the data, allowing it to make predictions on new, unseen data points. The training process involves minimizing the difference between predicted and actual continuous values.</p>
</li>
</ol><p>An artificial neural network will work this following way :</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Artificial_NN-4e522d3c6ed1ea902c12734109ad8ab0.png" width="808" height="558" class="img_ev3q"></p></div>
<div align="justify"><p>The artificial neural network (ANNs) can be considered as a mathematical function where you put in input the features which will be useful to predict an output.</p><p>ANNs are made up of interconnected nodes, called neurons, which process information and transmit it to other neurons. The neurons are arranged in layers, with each layer performing a different function.</p><p>We can split layers into 3 categories :</p><ol>
<li>Input layer is the first layer, it will receive the input data . The input layer neurons then process the data and transmit it to the hidden layers.</li>
<li>The hidden layers are the layers of an ANN that are not directly connected to the input or output layers. The hidden layers are responsible for learning complex patterns in the data. The hidden layers are also responsible for making predictions about the data.</li>
<li>Output layer is the last layer, the one that predicts the desired output.</li>
</ol><p>Neurons (rounds in our schema) in ANNs work by:</p><ol>
<li><strong>Receiving input signals</strong>: From other neurons.</li>
<li><strong>Multiplying input signals by weights</strong>: The weights determine the influence of each input signal on the neuron's output signal.</li>
<li><strong>Adding the weighted inputs</strong>: To produce a sum.</li>
<li><strong>Passing the sum to an activation function</strong>: Which produces the neuron's output signal.</li>
</ol><p>ANNs are trained by adjusting the weights of the neurons. The goal of the training process is to minimize the error between the predicted output of the ANN and the actual output of the ANN.</p><p>Once the ANN is trained, it can be used to make predictions on new data. The new data can be anything from images and sounds to text and sensor readings. The ANN will then produce an output for the new data.</p><p>Now that we have a basic understanding of artificial neural networks (ANNs), let's discuss Bayesian neural networks (BNNs). BNNs are a type of ANN that uses Bayesian probability theory to represent uncertainty in the model's parameters. This allows BNNs to learn more robust models and make more accurate predictions, especially when dealing with limited or noisy data.</p></div>
<h1>Bayesian Neural Networks : "I know that I know nothing"</h1>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Standard_vs_Bayesian-6cbb18167321f035a39657b32d2fac40.png" width="605" height="284" class="img_ev3q"></p></div>
<div align="justify"><p>Bayesian Neural Networks (BNNs) have emerged as a powerful solution to address a fundamental limitation of traditional Artificial Neural Networks (ANNs): the ability to express uncertainty.</p><p>While ANNs are proficient at making predictions, they often lack the capability to quantify uncertainty or admit when they don't have enough information. BNNs, on the other hand, introduce a Bayesian framework that inherently accounts for uncertainty in the model parameters.</p><p>Indeed, one key difference between ANNs and BNNs is that ANNs learn a single point estimate of the model's parameters, while BNNs learn a probability distribution over the model's parameters.</p><p>By treating weights as probability distributions rather than fixed values, BNNs can express a range of possibilities for each weight, allowing them to convey a degree of uncertainty in their predictions. This inherent probabilistic nature makes BNNs more flexible and generic in handling situations where an ANN might simply provide a confident but potentially misguided answer. BNNs excel in scenarios where acknowledging uncertainty is crucial, providing a more nuanced and cautious approach compared to the deterministic nature of traditional ANNs. In essence, BNNs offer a valuable advancement by enabling neural networks to say, "I don't know," thereby enhancing their applicability in real-world scenarios that demand a nuanced understanding of uncertainty.</p></div>
<h1>Conclusion</h1>
<div align="justify"><p>In conclusion, while Bayesian neural networks (BNNs) present exciting possibilities in the realm of artificial intelligence, addressing challenges such as computational complexity and the need for efficient training algorithms is imperative for unlocking their full potential. The computational efficiency of BNNs, often demanding more resources than traditional ANNs, necessitates advancements in both hardware and algorithms to make them practical for real-world applications. Ongoing research focused on developing scalable training algorithms for BNNs is a key endeavor, with the promise of making these networks more accessible across various domains. Additionally, the interpretability of BNN outputs, despite their transparency in uncertainty quantification, requires further exploration to enhance widespread acceptance.</p><p>On the flip side, exploring the applications of BNNs reveals their transformative impact across diverse fields. In medical diagnoses, BNNs offer transparency and caution, providing not only accurate predictions but also confidence intervals that empower healthcare professionals to make informed decisions. In the financial sector, BNNs excel at predicting market trends, offering confidence intervals that assist investors and analysts in navigating the inherently uncertain landscape. The cautious approach of BNNs is particularly valuable in the realm of autonomous vehicles, where real-time quantification of uncertainty enhances safety in unpredictable environments. Furthermore, BNNs play a crucial role in robotics, allowing robots equipped with these networks to navigate complex environments with sophistication, adapting to unforeseen obstacles and uncertainties in ways traditional neural networks cannot.</p><p>As we navigate the evolving landscape of AI, Bayesian neural networks emerge as a beacon of progress, not just in making better predictions but in fostering trust, transparency, and adaptability. By embracing the notion of "I don't know," BNNs illuminate a path toward collaborative interactions between AI systems and humans, providing nuanced and cautious insights in the face of uncertainty. The ongoing research and development in this field underscore the pivotal role that Bayesian neural networks are poised to play in shaping the next generation of intelligent systems.</p></div>]]></content:encoded>
            <category>data science</category>
            <category>deep learning</category>
        </item>
        <item>
            <title><![CDATA[Multimodal Deep Learning]]></title>
            <link>https://ekimetrics.github.io/blog/Multimodal_fusion</link>
            <guid>https://ekimetrics.github.io/blog/Multimodal_fusion</guid>
            <pubDate>Mon, 29 Apr 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Understand why multimodal deep learning models are more accurate than assembled unimodal models.]]></description>
            <content:encoded><![CDATA[<div align="justify"></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Introduction-0c5bea6b0953aca320c988247d219f80.png" width="767" height="607" class="img_ev3q"></p></div>
<br>
<div align="justify"><p>Data science (and specifically machine learning) is often seen as made up of different fields of expertise and research: « classical » machine learning (on tabular data), time series analysis &amp; forecasting, computer vision, natural language processing… Indeed, each kind of (input or output) data, which we call <strong>modality</strong> – e.g. image, text, time series –, has specific properties, raises specific challenges, and therefore requires dedicated solutions and evaluation methods. A pastry chef may ignore how to cook a black truffle risotto; a renowned ophthalmologist may deny any knowledge on kidney function; as well, in a siloed view of data science, a computer vision expert might pay no attention to the latest advances in gradient boosting or large language models.</p><p>At <strong>Ekimetrics</strong>, we have abandoned that siloed view: useful and meaningful data often is multimodal, and leveraging the predictive or generative power of several modalities cannot just be a matter of stacking or superficially connecting each field’s solutions. Let us see why this is important.</p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="multimodal-data-is-everywhere">Multimodal data is everywhere<a href="https://ekimetrics.github.io/blog/Multimodal_fusion#multimodal-data-is-everywhere" class="hash-link" aria-label="Direct link to Multimodal data is everywhere" title="Direct link to Multimodal data is everywhere">​</a></h2>
<div align="justify"><p>In many fields of economy and science, data is multimodal, and each modality may contain useful information. Multimodality is not just a theoretic use case! Let us provide three examples:</p><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Ad_Picture-bea3732d610bfa9db711ee0282bbeb13.png" width="650" height="746" class="img_ev3q"></p><ul>
<li>Product databases (manufacturing, retail, tourism, real estate…) may contain tabular data, product descriptions or other textual information, product photos, time series for price or sales history… Each product has multimodal data. Each modality contains potential predictive information, for instance to forecast future demand or sales.</li>
<li>Medical records include textual documents, tabular data, images, regular or irregular time series. Here again, for each patient, every modality may contain precious information to help diagnosis or prognosis.</li>
<li>Advertisements, e-mails, newspaper covers and articles and social media posts are usually bimodal: texts combined with one or several images (and sometimes, texts within images). Both modalities surely have an influence on the impact (clicks, “likes”, sales…) of these signals, and should be addressed when predicting this impact.</li>
</ul><p>This is why, in our job as data scientists, we must design and implement the best solutions to harness combined data modalities.</p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="a-multimodal-brain-is-better-than-several-unimodal-brains">A multimodal brain is better than several unimodal brains<a href="https://ekimetrics.github.io/blog/Multimodal_fusion#a-multimodal-brain-is-better-than-several-unimodal-brains" class="hash-link" aria-label="Direct link to A multimodal brain is better than several unimodal brains" title="Direct link to A multimodal brain is better than several unimodal brains">​</a></h2>
<div align="justify"><p>Every minute or even second, our human brain reacts, evaluates, and makes decisions after combining and processing several signals of different kinds, gathered from several senses. Each sense (or each modality) is correctly interpreted <strong>thanks to the knowledge of all other signals</strong>:</p><ul>
<li>To know if the noise I heard on my right reveals a danger, I must concentrate my vision sense to the same direction (maybe turn my head) and look at the source of the noise.</li>
<li>Imagine that someone shows me a document and gives oral explanations. Something sounds weird in these explanations. To know if my interlocutor is serious or joking, I must stop looking at the document, and look at his/her facial expression.</li>
<li>To select what to pay attention to in this <a href="https://www.youtube.com/watch?v=KB_lTKZm1Ts" target="_blank" rel="noopener noreferrer">famous awareness test</a>, I must know and understand the written instructions (or game rule).</li>
<li>As a specialist physician, to avoid missing weak signals in exam results (imagery, blood test history…) and symptoms, I must understand why the patient was sent to me, which means reading the cover letter or medical report, so as to know what to focus on.</li>
<li>In many bimodal documents, for instance scientifical articles, understanding the images (diagrams, charts) help understanding the text, and reading some texts (e.g. captions) can be necessary to understand the images or to deduce what is important to be watched in the image.</li>
</ul><p>If all these examples, suppose each modality is observed by a different person (a different unimodal brain), without any communication with the other persons, and afterwards, only afterwards, these persons can communicate and decide, without any access to the modalities. What will happen? They will probably miss something, because they did not look in the right direction, or did not focus enough on some detail, or misinterpreted a signal.</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Monkeys-572eccaee87d0f6a45c2c5291997e81d.png" width="395" height="270" class="img_ev3q"></p><p>What if the neurons are artificial ones?</p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="unimodal-embeddings-are-not-all-you-need">Unimodal embeddings are not all you need<a href="https://ekimetrics.github.io/blog/Multimodal_fusion#unimodal-embeddings-are-not-all-you-need" class="hash-link" aria-label="Direct link to Unimodal embeddings are not all you need" title="Direct link to Unimodal embeddings are not all you need">​</a></h2>
<div align="justify"><p>In simple (or maybe naive) multimodal deep learning architectures, unimodal state-of-the-art (SoTA) neural networks work independently. Each unimodal brain classically produces a <strong>unimodal embedding</strong>, which is usually a vector (1D-array) or a sequence of vectors (2D-array) of numerical values, which is supposed to concentrate all useful information on the modality, in a reduced mathematical space. Then, a prediction is built from all these gathered embeddings, thanks to the addition of a few more trained neural layers (a “head” block), or with numerical machine learning solutions such as tree boosting. It looks SoTA, but it often induces poor performance.</p><p>The first reason is that each unimodal embedding is given in a different latent space: it means that it uses a language which is completely different from the other unimodal embeddings. Thus, the image embedding does not help guessing what is important to check in the text embedding, and vice versa. In the human scenario (unimodal brains), it would be as if the involved persons could not communicate correctly together: making a good decision would be quite unlikely.</p><p>Fortunately, public pre-trained models and architectures called <strong>dual encoders</strong>, like CLIP<sup><a href="https://ekimetrics.github.io/blog/Multimodal_fusion#user-content-fn-1-2e81d8" id="user-content-fnref-1-2e81d8" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup> , can produce text embeddings and image embeddings in the same latent space (same language), which significantly helps bridging the gap between modalities, and improve predictive performance.</p><p>But this is not enough. You already know the second reason: even with dual encoders, each unimodal brain (network) does not communicate with the other brains when analyzing its modality, and therefore, does not focus enough on what is important in it. This is why a new family of multimodal models has been designed: <strong>fusion encoders</strong>. They allow, in each modality analysis, to progressively introduced useful information from other modalities. Thus, useful information is jointly extracted from all modalities of a same data sample.</p><p>Pretrained image &amp; text (bimodal) fusion encoders, ingeniously assembling SoTA expertise of both computer vision and natural language processing, are now publicly available: ViLT<sup><a href="https://ekimetrics.github.io/blog/Multimodal_fusion#user-content-fn-2-2e81d8" id="user-content-fnref-2-2e81d8" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup> , FLAVA<sup><a href="https://ekimetrics.github.io/blog/Multimodal_fusion#user-content-fn-3-2e81d8" id="user-content-fnref-3-2e81d8" data-footnote-ref="true" aria-describedby="footnote-label">3</a></sup> , BLIP<sup><a href="https://ekimetrics.github.io/blog/Multimodal_fusion#user-content-fn-4-2e81d8" id="user-content-fnref-4-2e81d8" data-footnote-ref="true" aria-describedby="footnote-label">4</a></sup> , BLIP-2<sup><a href="https://ekimetrics.github.io/blog/Multimodal_fusion#user-content-fn-5-2e81d8" id="user-content-fnref-5-2e81d8" data-footnote-ref="true" aria-describedby="footnote-label">5</a></sup> … They can output <strong>bimodal embeddings</strong>, which contain rich relevant information from a pair <code>{image, text}</code>, analyzed together. They are excellent candidates for “transfer learning” strategies: adding a trained “head” network on top of a fine-tuned fusion encoder, to make the desired prediction or forecast.</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/BLIP-a4096d65e4dda89ff8514e24d609aeb2.png" width="480" height="217" class="img_ev3q"></p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="a-demonstration-by-ekimetrics">A demonstration by Ekimetrics<a href="https://ekimetrics.github.io/blog/Multimodal_fusion#a-demonstration-by-ekimetrics" class="hash-link" aria-label="Direct link to A demonstration by Ekimetrics" title="Direct link to A demonstration by Ekimetrics">​</a></h2>
<div align="justify"><p>Huge product databases sometimes suffer from insufficient or irregular quality, due to historical or heterogeneous (sometimes manual) feeding processes. For instance, an irrelevant picture (product photo) may have been associated to an unmatching product. Manual thorough detection of errors, on millions of products, would be extremely costly. At Ekimetrics Innovation Lab (Eki.Lab), we have designed, built, evaluated, and validated a deep learning solution to efficiently semi-automate that task: it can detect most mismatches between an image and the known text fields (name, properties, description, summary…) of the same product.</p><p>Our most efficient solution mainly relies on a fine-tuned fusion encoder and trained head. Even with artificially generated hard-to-find image permutations (e.g. replacing an image with a similar but slightly different one) in test database, our solution can detect 87% (recall) of existing mismatches, with a tolerated precision of 50% (i.e. allowing that that only 1 suspicious image out of 2 is actually incorrect). On top of that, we have designed and validated a method to adapt the model to any unlabeled product database (self-supervision): costly human labelling is unnecessary.</p><p>Our experiments confirm that without a fusion encoder, and even using CLIP as a dual encoder, results are much worse: recall remains under 30% with the same constraint on precision.</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Fusion_encoder-761c366ece8019ab9debcfc288ef235d.png" width="499" height="263" class="img_ev3q"></p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-about-tabular-data-and-time-series">What about tabular data and time series?<a href="https://ekimetrics.github.io/blog/Multimodal_fusion#what-about-tabular-data-and-time-series" class="hash-link" aria-label="Direct link to What about tabular data and time series?" title="Direct link to What about tabular data and time series?">​</a></h2>
<div align="justify"><p>Now, what would be the best solution for a predictive task in which multimodal input includes tabular data and/or time series? In most cases, there is no relevant pre-trained fusion encoder on these modalities, since there is no universal knowledge to deduce from pre-training on time series or tabular data. However, a few recent research papers, such as the Perceiver family (Perceiver<sup><a href="https://ekimetrics.github.io/blog/Multimodal_fusion#user-content-fn-6-2e81d8" id="user-content-fnref-6-2e81d8" data-footnote-ref="true" aria-describedby="footnote-label">6</a></sup> , Perceiver IO<sup><a href="https://ekimetrics.github.io/blog/Multimodal_fusion#user-content-fn-7-2e81d8" id="user-content-fnref-7-2e81d8" data-footnote-ref="true" aria-describedby="footnote-label">7</a></sup> , Hierarchical Perceiver<sup><a href="https://ekimetrics.github.io/blog/Multimodal_fusion#user-content-fn-8-2e81d8" id="user-content-fnref-8-2e81d8" data-footnote-ref="true" aria-describedby="footnote-label">8</a></sup> ), show promising ways to build and train an accurate unique (fusion) deep learning model based on any combination of various modalities.</p><p>Combining such a “multimodal-by-design” architecture with pre-trained models (including fusion encoders) for both text and images, which may still bring useful knowledge with small-sized training datasets, is an active field of research<sup><a href="https://ekimetrics.github.io/blog/Multimodal_fusion#user-content-fn-9-2e81d8" id="user-content-fnref-9-2e81d8" data-footnote-ref="true" aria-describedby="footnote-label">9</a></sup> , in which Ekimetrics is fully involved. A high range of disruptive use cases can be designed: in the near future, efficient multimodal models will help optimize media communication, forecast demand or sales to optimize product design or supply chain, forecast or optimize prices, help diagnosis or prognosis, perform impactful social media analysis… Please stay connected for future news on the subject!</p></div>
<!-- -->
<section data-footnotes="true" class="footnotes"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5 sr-only" id="footnote-label">Footnotes<a href="https://ekimetrics.github.io/blog/Multimodal_fusion#footnote-label" class="hash-link" aria-label="Direct link to Footnotes" title="Direct link to Footnotes">​</a></h2>
<ol>
<li id="user-content-fn-1-2e81d8">
<p>« Learning Transferable Visual Models From Natural Language Supervision », A. Radford et al., 2021 <a href="https://ekimetrics.github.io/blog/Multimodal_fusion#user-content-fnref-1-2e81d8" data-footnote-backref="" aria-label="Back to reference 1" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-2-2e81d8">
<p>« ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision », W. Kim &amp; al., 2021 <a href="https://ekimetrics.github.io/blog/Multimodal_fusion#user-content-fnref-2-2e81d8" data-footnote-backref="" aria-label="Back to reference 2" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-3-2e81d8">
<p>« FLAVA: A Foundational Language And Vision Alignment Model », A. Singh, 2022 <a href="https://ekimetrics.github.io/blog/Multimodal_fusion#user-content-fnref-3-2e81d8" data-footnote-backref="" aria-label="Back to reference 3" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-4-2e81d8">
<p>« BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation », J. Li et al., 2022 <a href="https://ekimetrics.github.io/blog/Multimodal_fusion#user-content-fnref-4-2e81d8" data-footnote-backref="" aria-label="Back to reference 4" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-5-2e81d8">
<p>« BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models », J. Li et al., 2023 <a href="https://ekimetrics.github.io/blog/Multimodal_fusion#user-content-fnref-5-2e81d8" data-footnote-backref="" aria-label="Back to reference 5" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-6-2e81d8">
<p>« Perceiver: General Perception with Iterative Attention », A. Jaegle et al., 2021 <a href="https://ekimetrics.github.io/blog/Multimodal_fusion#user-content-fnref-6-2e81d8" data-footnote-backref="" aria-label="Back to reference 6" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-7-2e81d8">
<p>« Perceiver IO : a general architecture for structured inputs &amp; outputs », A. Jaegle et al., 2021 <a href="https://ekimetrics.github.io/blog/Multimodal_fusion#user-content-fnref-7-2e81d8" data-footnote-backref="" aria-label="Back to reference 7" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-8-2e81d8">
<p>« HiP: Hierarchical Perceiver », J. Carreira et al., 2022 <a href="https://ekimetrics.github.io/blog/Multimodal_fusion#user-content-fnref-8-2e81d8" data-footnote-backref="" aria-label="Back to reference 8" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-9-2e81d8">
<p>For instance, see « Flamingo: a Visual Language Model for Few-Shot Learning », J.B. Alayrac et al., 2022 <a href="https://ekimetrics.github.io/blog/Multimodal_fusion#user-content-fnref-9-2e81d8" data-footnote-backref="" aria-label="Back to reference 9" class="data-footnote-backref">↩</a></p>
</li>
</ol>
</section>]]></content:encoded>
            <category>data science</category>
            <category>deep learning</category>
            <category>multimodal</category>
        </item>
        <item>
            <title><![CDATA[Uncertainty in time series forecasting]]></title>
            <link>https://ekimetrics.github.io/blog/Uncertainty_TS</link>
            <guid>https://ekimetrics.github.io/blog/Uncertainty_TS</guid>
            <pubDate>Fri, 23 Feb 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Discover uncertainty quantification for time series forecasting.]]></description>
            <content:encoded><![CDATA[<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Uncertainty_TS-f4b33f9c07724504284127dee3524d01.jpg" width="5434" height="3623" class="img_ev3q"></p></div>
<br>
<div align="justify"><p>Uncertainty in time series forecasting refers to the unpredictability and variability associated with predicting future values in a sequence of data points over time. Time series forecasting involves analyzing historical data to make informed predictions about future trends, patterns, or values. However, due to various factors such as randomness, external influences, and incomplete information, forecasting models often encounter uncertainty. Managing and quantifying uncertainty in time series forecasting is crucial for decision-making processes across diverse domains such as finance, economics, weather forecasting, and supply chain management.</p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="why-prefer-uncertainty">Why prefer uncertainty?<a href="https://ekimetrics.github.io/blog/Uncertainty_TS#why-prefer-uncertainty" class="hash-link" aria-label="Direct link to Why prefer uncertainty?" title="Direct link to Why prefer uncertainty?">​</a></h2>
<div align="justify"><p>Accuracy methods, otherwise known as point forecasting, are commonly used to address issues confronted in times series. Attempting to estimate the closest value to reality without considering all risk-related factors refers to accuracy. This approach leans more toward a retrospective examination of past errors.</p><p>Considering the stochastic nature of the universe, we aim to approach interval within which the true values will fall, considering potential future risks and potential future errors. We're no longer referring to point forecasting but rather interval forecasting.</p><p>The need to quantify uncertainty can arise in various ways, for example:</p><ul>
<li>
<p>Uncertainty forecasting helps energy companies make decisions about buying and selling energy in real-time markets, considering fluctuations in demand and supply</p>
</li>
<li>
<p>Forecasting uncertainty in demand and supply can help retail businesses optimize their production, distribution processes and stock management</p>
</li>
<li>
<p>Hospitals can use uncertainty forecasts to anticipate patient admissions, enabling better resource allocation, staffing decisions and pharmaceutical orders</p>
</li>
</ul></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/EarthFromSpace-239895b591f3b6601a8a93c29fde5683.jpg" width="1386" height="473" class="img_ev3q"></p></div>
<div align="justify"><p><strong>The most important thing to keep in mind is that even a good estimator can make important errors due to the volatility that a time series can present.  To prevent this problem, the use of prediction intervals is recommended.</strong></p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="popular-methods-for-uncertainty-quantification">Popular methods for uncertainty quantification<a href="https://ekimetrics.github.io/blog/Uncertainty_TS#popular-methods-for-uncertainty-quantification" class="hash-link" aria-label="Direct link to Popular methods for uncertainty quantification" title="Direct link to Popular methods for uncertainty quantification">​</a></h2>
<div align="justify"><p>There are a huge number of methods capable of creating prediction intervals that quantify the uncertainty surrounding an observation. However, it is possible to group most of them into five methods.</p></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="bootstrap">Bootstrap<a href="https://ekimetrics.github.io/blog/Uncertainty_TS#bootstrap" class="hash-link" aria-label="Direct link to Bootstrap" title="Direct link to Bootstrap">​</a></h3>
<div align="justify"><p>Bootstrap is a statistical technique used to estimate the variability of a statistic . It involves generating multiple bootstrap samples through resampling, with replacement, from  the original dataset. Subsequently, these samples undergo analysis employing the same statistical model.  The results provide a range of values that represent the uncertainty associated with your forecast or analysis, offering a more nuanced understanding of potential fluctuations in outcomes. This approach provides a flexible and data-driven method to assess the precision of statistical estimates.</p><div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/Bootstrap-20851c749e5bd9b16eaa51a8ea37312e.png" width="506" height="189" class="img_ev3q"></p></div><br><p>Advantages of this approach include its utilization of non-parametric methods, requiring a minimal amount of data, and ease of implementation. However, the method has disadvantages such as demanding intensive computing resources, being highly dependent on input data, and posing challenges in terms of interpretation.</p></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="asymmetric-regression">Asymmetric regression<a href="https://ekimetrics.github.io/blog/Uncertainty_TS#asymmetric-regression" class="hash-link" aria-label="Direct link to Asymmetric regression" title="Direct link to Asymmetric regression">​</a></h3>
<div align="justify"><p>In machine learning, altering the loss function can reshape a model's predictions by influencing how it handles errors. During the training phase, a machine learning model learns to adjust its parameters to minimize the overall loss  , but in the case of asymmetric loss functions, penalties for overestimations and underestimations are different. The optimization process involves balancing the penalties associated with overestimating and underestimating based on parameter values. Fine-tuning these parameters allows to adapt the model's behavior. This introduces a nuanced response to errors. Among other things, this is the principle behind quantile regression.</p><div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/AssymetricRegression-dac8b6542211b054dbc87ff3355f7f39.png" width="749" height="189" class="img_ev3q"></p></div><p>Advantages of the approach include its compatibility with non-parametric methods . However, it comes with disadvantages such as the absence of mathematical guarantees, uncalibrated forecasts, and the complexity of interpretation associated with the method.</p></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="probabilistic-models">Probabilistic models<a href="https://ekimetrics.github.io/blog/Uncertainty_TS#probabilistic-models" class="hash-link" aria-label="Direct link to Probabilistic models" title="Direct link to Probabilistic models">​</a></h3>
<div align="justify"><p>Probabilistic models are favored for their capacity to output distributions instead of single points estimates.  Numerous probabilistic models exist for time series forecasting, and among them, ARIMA models are notably popular.</p><p>The process begins with the model fitting the time series data, capturing the autoregressive (AR) and moving average (MA) components, and ensuring stationarity through differencing (integrated component). After generating point forecasts, the model assesses the residuals, which represent the differences between the observed and predicted values. ARIMA utilizes the standard deviation of the normally distributed residuals to construct prediction intervals around the point forecasts. Essentially, the wider the prediction interval, the greater the uncertainty associated with the forecast. This technical methodology not only refines the accuracy of point forecasts but also provides a statistically sound measure of the range within which future observations are likely to fall.</p><div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/Probabilistic-ee0c8ad2bf822f5fa79f424fe51ee035.png" width="520" height="189" class="img_ev3q"></p></div><p>Advantages of this method include its simplicity and interpretability, along with the absence of a requirement for exogenous variables (even if it could be also a disadvantage). However, the method comes with disadvantages such as reliance on assumptions about the data and the need for parameter selection like most probabilistic methods.</p></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="bayesian-approach">Bayesian approach<a href="https://ekimetrics.github.io/blog/Uncertainty_TS#bayesian-approach" class="hash-link" aria-label="Direct link to Bayesian approach" title="Direct link to Bayesian approach">​</a></h3>
<div align="justify"><p>Bayesian time series forecasting employs a technically advanced approach, involving probabilistic modeling, Bayesian inference, and iterative parameter estimation. During modeling, prior distributions are specified, which are updated through Bayesian inference with observed data to obtain posterior distributions. Techniques such as Markov Chain Monte Carlo or variational inference optimize these distributions for parameter estimation. It allows you to quantify uncertainty in your forecasts thanks to posterior distributions.</p><div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/Bayesian-92853115c2370535ab40c2646c2308d9.png" width="745" height="189" class="img_ev3q"></p></div><p>Advantages of this approach include the ability to incorporate business knowledge and the possibility of extracting insights despite weakly informative datasets. However, a notable disadvantage is that incorrect priors may lead to inaccurate results.</p></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conformal-prediction">Conformal prediction<a href="https://ekimetrics.github.io/blog/Uncertainty_TS#conformal-prediction" class="hash-link" aria-label="Direct link to Conformal prediction" title="Direct link to Conformal prediction">​</a></h3>
<div align="justify"><p>Conformal prediction offers finite-sample conformal intervals applicable to any model and dataset without additional costs. It is particularly valuable for black box models, requiring no modification to their analysis or training. In addition to the usual testing and training datasets, the methodology also relies on a calibration set, which aids in generating conformalized intervals by rectifying errors introduced by the regressor. A crucial aspect in the design of conformal prediction is adaptivity. The objective is to ensure that the procedure yields larger intervals for more challenging inputs and smaller intervals for simpler inputs, tailoring the predictive uncertainty to the inherent difficulty of each input.</p><div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/ConformalPrediction-d03b13f13bab4b37e5eeff7c6a4dd274.png" width="940" height="189" class="img_ev3q"></p></div><p>Advantages of this method encompass mathematical coverage guarantees, consistent and robust performance, and applicability to any model or dataset. However, there are drawbacks, such as certain methods requiring intensive computing, which may pose challenges in a business application context, and a dependency on a good regressor.</p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="evaluation-key-points">Evaluation key points<a href="https://ekimetrics.github.io/blog/Uncertainty_TS#evaluation-key-points" class="hash-link" aria-label="Direct link to Evaluation key points" title="Direct link to Evaluation key points">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="coverage">Coverage<a href="https://ekimetrics.github.io/blog/Uncertainty_TS#coverage" class="hash-link" aria-label="Direct link to Coverage" title="Direct link to Coverage">​</a></h3>
<div align="justify"><p>Coverage is a critical aspect in uncertainty prediction, particularly when considering an error level α (0 &lt; α &lt; 1). The anticipated outcome is that, for a given confidence level of (1 - α), we expect that (1 - α)% of the observations fall within the predicted uncertainty intervals. In other words, this statement underscores the importance of the model accurately capturing and quantifying uncertainty, ensuring that the specified percentage of observations is encompassed by the predicted uncertainty range, aligning with the desired level of confidence.</p><p>This concept becomes especially crucial in scenarios where robust and reliable uncertainty estimates are required.</p></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="interval">Interval<a href="https://ekimetrics.github.io/blog/Uncertainty_TS#interval" class="hash-link" aria-label="Direct link to Interval" title="Direct link to Interval">​</a></h3>
<div align="justify"><p>In uncertainty quantification, a brief interval is commonly seen as advantageous, suggesting improved precision in predictions with constrained uncertainty. While a narrow uncertainty interval signifies heightened confidence in the model's predictive capabilities, it is imperative to remain watchful of another critical factor: variance. Although low variance can indicate prediction stability, an excessively low variance may raise concerns. This could suggest that the model oversimplifies the complexity of the problem, potentially overlooking crucial nuances in the data.</p><p>Thus, achieving a balance between pursuing tight uncertainty intervals, indicative of precision, and considering variance is vital to ensure the resilience of the uncertainty quantification model, aligning it with the characteristics of the data.</p></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="adaptiveness">Adaptiveness<a href="https://ekimetrics.github.io/blog/Uncertainty_TS#adaptiveness" class="hash-link" aria-label="Direct link to Adaptiveness" title="Direct link to Adaptiveness">​</a></h3>
<div align="justify"><p>Adaptability is a crucial aspect in evaluating uncertainty quantification, focusing on temporal and sectional adaptiveness. Temporal adaptiveness ensures that the model can dynamically adjust its uncertainty estimation over time, maintaining relevance amid evolving data trends. Similarly, sectional adaptiveness underscores the need for adaptability within different segments or subsets of a dataset. An adaptive model should effectively address variations in uncertainty within these sections, acknowledging potential diversity in conditions or characteristics.</p><p>In summary, within the realm of uncertainty quantification evaluation, adaptability, exemplified through temporal and sectional adaptiveness, represents a comprehensive and nuanced approach that return larger sets for harder inputs and smaller sets for easier inputs.</p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="https://ekimetrics.github.io/blog/Uncertainty_TS#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<div align="justify"><p>In conclusion, the exploration of time series forecasting underscores the shift from traditional accuracy methods to uncertainty quantification. Various methods, such as Bootstrap, Asymmetric Regression, Probabilistic Models, Bayesian Approach, and Conformal Prediction, offer diverse approaches, each with their respective advantages and disadvantages. Conformal prediction, a set of recent methods, still under research and development,  holds the potential to outperform other approaches in the field. That's why, at Eki.Lab, we're pushing ahead with many studies linked to Conformal Prediction.</p><p>The main points of evaluation  we need to look at when evaluating these methods are coverage and adaptivity. We want our predictions to be able to evolve over time and across different dimensions  of the dataset to ensure a good coverage rate whatever the particularities of the predicted observation. It's important to find a balance between keeping our uncertainty ranges narrow enough to be precise but also considering how much our predictions might vary. This is all the key to building a strong model that can handle the complexities of real-world data.</p></div>]]></content:encoded>
            <category>Uncertainty Quantification</category>
            <category>Interval Forecasting</category>
            <category>Time Series Modeling</category>
            <category>Demand Sensing</category>
        </item>
        <item>
            <title><![CDATA[Maths Men: The Economics of Online Advertising]]></title>
            <link>https://ekimetrics.github.io/blog/Economics_online_ad</link>
            <guid>https://ekimetrics.github.io/blog/Economics_online_ad</guid>
            <pubDate>Fri, 01 Sep 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Willing to learn about the online ad market, cross-media effects and contextual advertising? This article wraps up the content of a PhD thesis on the economics of online advertising, defended by Rémi Devaux, Senior Consultant at Ekimetrics.]]></description>
            <content:encoded><![CDATA[<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/economics_online_ad-6ebd119792714bda9f9c002c0bb7f669.jpg" width="4096" height="2730" class="img_ev3q"></p></div>
<br>
<div align="justify"><p>This article wraps up the content of a PhD thesis in economics defended on April 4<sup>th</sup> 2023, by Rémi Devaux, now senior consultant in data science at Ekimetrics. This PhD thesis was part of a 3-year academic partnership between Ekimetrics and Mines Paris PSL.</p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-is-new-with-online-advertising">What is New with Online Advertising<a href="https://ekimetrics.github.io/blog/Economics_online_ad#what-is-new-with-online-advertising" class="hash-link" aria-label="Direct link to What is New with Online Advertising" title="Direct link to What is New with Online Advertising">​</a></h2>
<div align="justify"><p>Accounting for 67% of the 930 billion media spend worldwide<sup><a href="https://ekimetrics.github.io/blog/Economics_online_ad#user-content-fn-1-f3baf9" id="user-content-fnref-1-f3baf9" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup>, online advertising is an obsessive presence in our online life. Search engines, video-sharing platforms, social media and now streaming services: ads are present in many formats on the Internet. In fact, most online platforms are partially or totally financed by advertising. Ad spending by brands subsidies online services, allowing platforms to charge no price to users. This is not new: classified ads financed free newspapers, just as brand sponsoring subsidized radio broadcast. So, what’s new with online advertising?</p><p>Economists pointed out that online advertising decreased the cost of targeting consumers<sup><a href="https://ekimetrics.github.io/blog/Economics_online_ad#user-content-fn-2-f3baf9" id="user-content-fnref-2-f3baf9" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup>. This is a major change in the marketing industry. While offline, advertisers can only target their ads toward a general context (newspaper title, TV show, billboard location), online ads can be addressed on a consumer-level basis using personal data (socio-demographics, web history, online behaviors). This decrease in targeting cost acted as a game-changer on many parts of the advertising industry:</p><ul>
<li>
<p><strong>Pricing and auctions</strong>: finer targeting increases the quantity of ad impressions available to advertisers. As a result, such a massive volume of impressions cannot be sold at a flat price, just as in the offline advertising world. On the Internet, ad impressions are thus often allocated by auctions. Advertisers compete for a given ad impression based on the characteristics of the impression (user data, website…). The brand associated with the higher bid then wins the impression and displays its ad. This allows ad spaces to be priced at their right price, making advertisers and publishers better off.</p>
</li>
<li>
<p><strong>Intermediaries</strong>: this complex system of auctions induces that brands and publishers should be able to manage their ad opportunities and bidding strategy on an impression-per-impression basis. Naturally, given the high volume of ad impressions traded every minute, this is not feasible. That is why, online, many ad intermediaries manage the trading of ad spaces on the supply-side (publishers) as well as on the demand-side (advertisers).</p>
</li>
<li>
<p><strong>Data and Measurement</strong>: the technical nature of online advertising emphasizes the power of data. Views, clicks, conversions: low-targeting costs allow advertisers to observe individual responses to their ads, producing a huge amount of data. Machine learning and data analysis are used at every stage of the ad-serving to optimize advertisers’ bid, click-through-rate, publisher revenues, post-campaign attribution…
That is why observers often made the claim that with the Internet, the advertising industry went from <em>Mad Men to Maths Men</em>.</p>
</li>
</ul><p>This PhD thesis focuses on two questions: (i) how online advertising is distinct from traditional advertisement and (ii) how context plays a role in online ads effectiveness. These questions are answered using econometric techniques applied to advertising data.</p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="old-world-new-world">Old World, New World?<a href="https://ekimetrics.github.io/blog/Economics_online_ad#old-world-new-world" class="hash-link" aria-label="Direct link to Old World, New World?" title="Direct link to Old World, New World?">​</a></h2>
<div align="justify"><p>The first part of the thesis studies whether online advertising is a complement or a substitute to traditional advertisements (the one defined by offline mediums such as TV, radio, billboards, newspapers, or cinema). As we saw in the introduction, online advertising works in a very different way compared to offline media where ad buying involves human negotiation, discounts and where ad effectiveness cannot be observed directly. However, many economists claimed that offline and online advertising were close substitutes<sup><a href="https://ekimetrics.github.io/blog/Economics_online_ad#user-content-fn-3-f3baf9" id="user-content-fnref-3-f3baf9" data-footnote-ref="true" aria-describedby="footnote-label">3</a></sup>.</p><p>One can investigate this question according to two viewpoints: advertising supply and demand.</p><p>On the supply-side, we can determine that two types of advertising media are substitutes if brands tend to use both interchangeably. Conversely, two advertising media types are complements when advertisers tend to use them together, with the goal of generating synergies. Substitutability between two media types can be investigated using the concept of <em>cross-price elasticity</em>: when the price of a media type increases, how does it impact the demand for another media type?</p><p>Using data from 10 big national advertisers in three different industries, I investigate how firm’s spending on offline, internet display<sup><a href="https://ekimetrics.github.io/blog/Economics_online_ad#user-content-fn-4-f3baf9" id="user-content-fnref-4-f3baf9" data-footnote-ref="true" aria-describedby="footnote-label">4</a></sup>  and search ads react to each media price. Using an econometric model derived from microeconomic theory, I find that offline and online advertising media types are not close substitutes.<br>
<!-- -->Intuitively, offline and search do not substitute for each other. Display and offline ad media types seem to be substitutes, but mostly for non-audiovisual media such as newspapers, magazines, and billboards. Indeed, internet display formats substitutes more easily to print advertisement than to TV or radio ads.<br>
<!-- -->Finally, the model shows that display and search ads are substitutable for advertisers. This can be natural as it seems certain types of display ads (such as retargeting) can be used to generate conversions, just as search ads.</p><p>We can also determine whether offline and online ads are complements or substitutes by looking at the demand side: i.e., how consumers respond to ads. Marketing researchers showed that advertising campaigns on different media may generate <em>synergies</em><sup><a href="https://ekimetrics.github.io/blog/Economics_online_ad#user-content-fn-5-f3baf9" id="user-content-fnref-5-f3baf9" data-footnote-ref="true" aria-describedby="footnote-label">5</a></sup>. That is, the return of investing on two media may be greater than the sum of the two returns taken individually. This kind of synergy typically happens when branding and conversion media are played together. For example, a national TV campaign may raise awareness and desirability for a product, increasing online search volume and thus the effectiveness of sponsored links.</p><p>In another study of the thesis, I try to draw a line between offline ad spending and the effectiveness of search ads. Using data from a panel of brands in the hotel sector, I build an econometric model where clicks on sponsored search ads depend on the brand’s own offline ad spending, other ad spending, seasonality, search ad exposure and market-specific factors. Because the effect of offline ad campaigns may be inter-temporal, the investments of period t-1 enters period t after being discounted by a factor <em>λ</em>. I find that, on average, increasing the discounted stock of offline ad investments by 10% increases the volume of clicks recorded on search ads by more than 9%. Thus, the effectiveness of online ads also depends on the investment offline, making both types of media complementary.</p><div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/Picture1-41598bbb139ac23ca96028e34acc0cbf.png" width="1279" height="895" class="img_ev3q">
</p><div align="center">Figure 1: Relationship between Clicks on Online Ads and Offline Advertising</div><p></p></div><br><p>Overall, my results suggest that offline and online advertisements are not close substitutes. Looking at how both types of media work, the way advertisers use them and how consumers react to them, offline and internet advertising are different, and sometimes complementary, markets.</p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="context-matters">Context Matters!<a href="https://ekimetrics.github.io/blog/Economics_online_ad#context-matters" class="hash-link" aria-label="Direct link to Context Matters!" title="Direct link to Context Matters!">​</a></h2>
<div align="justify"><p>The ability to finely-target advertisements has allowed brands to better match their message to consumers. Targeted advertising thus generates gains by reducing ad ‘waste’<sup><a href="https://ekimetrics.github.io/blog/Economics_online_ad#user-content-fn-6-f3baf9" id="user-content-fnref-6-f3baf9" data-footnote-ref="true" aria-describedby="footnote-label">6</a></sup> (which occurs whenever the ad is distributed to a consumer who has no preference for the product advertised).</p><p>However, focusing solely on individual targeting obliterates an important factor: context. In many cases, internet advertising as been subject to placement issues: an ad may be placed on an unviewable part of the page or worse, inside a bad context. The latter case includes ads displayed on controversial websites (e.g., conspiracy-related blogs, pornographic websites), or inside a content that is irrelevant to the ad: in 2012, an ad for cruise vacations run on a YouTube showing the sinking of the Costa Concordia<sup><a href="https://ekimetrics.github.io/blog/Economics_online_ad#user-content-fn-7-f3baf9" id="user-content-fnref-7-f3baf9" data-footnote-ref="true" aria-describedby="footnote-label">7</a></sup>. There are many other examples.</p><p>This kind of issue arises from an information asymmetry between the advertiser and the publisher. Online advertising features so many possible ad placements and the process of matching ads to placements is so complex that, eventually, advertisers rarely know where their ad appears. Without carefully considering contextual effects, advertisers may waste their ad budget, and even harm their brand image by appearing in bad contexts. Two works of my studies try to shed light on such contextual effects.</p><p>In a first study, I use data from a viewability provider to investigate the determinant of ad viewability. The data showed that more than 20% of video ads for a healthcare advertiser were never seen. I try to investigate the determinant of ad viewability. Using an econometric model, I show that when the ad network did not have incentives to make ads viewable, it reduced the overall viewability rate of the campaign. Buying ads on a cost-per-view instead of cost-per-impressions basis induces the ad network to make the ad viewable, increasing viewability by almost 20%. Programmatic advertising also deters ad viewability by introducing opacity between the advertiser and the publisher.</p><p>In a second study, I investigate how controversial content on socials impact advertising effectiveness. I use the 2020 Facebook Ad Boycott as a natural experiment. In July 2020, more than 1,000 advertisers pulled out their ad spending from Facebook and Instagram as both platforms were hosting controversial content. Using data from an advertiser in the skincare industry that continued to advertise, I estimate how Facebook ad effectiveness changed compared to the brand’s ads on other platforms. Controlling for platform-specific effectiveness and seasonality, I find that during the boycott, Facebook Ads experienced a decrease of up to 10,000 clicks compared to the expected effectiveness of the ads.</p><div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/Picture2-e913d29bdfa63e5fff1150162fdd325f.png" width="1358" height="988" class="img_ev3q">
</p><div align="center">Figure 2: Effect of the boycott (week 0) on clicks recorded on Facebook Ads. Estimates are reported with 95% confidence interval.</div><p></p></div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="https://ekimetrics.github.io/blog/Economics_online_ad#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<div align="justify"><p>Online advertising is evolving at an unprecedent pace and so is regulation. Since 2020, political initiatives foster consumer protection, privacy, and abuse of dominance in the digital economy. This has at least two implications for online advertising.</p><p>First, for antitrust purposes market regulators need to define the relevant market for advertising. Does regulation on the online ad market affect offline advertising? Does Google Ads compete with national TV networks? Such questions are crucial to understand the market power of online advertising players. This thesis brings some insight from this perspective.</p><p>Second, the strengthening of personal data regulation, along the end of third-party cookies will emphasize the importance of ad context. As targeting individual-level characteristics will be more and more difficult, brands will gain in leveraging context to target their ads. For example, targeting hotel ads on travel blog pages is a way to reach relevant consumers.</p><p>Finally, cookies limitation will impact advertiser’s ability to finally measure the return of their advertisement. More than never, holistic econometric measurement with Marketing Mix Modeling is a key tool to measure advertising effectiveness.</p><br></div>
<!-- -->
<section data-footnotes="true" class="footnotes"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5 sr-only" id="footnote-label">Footnotes<a href="https://ekimetrics.github.io/blog/Economics_online_ad#footnote-label" class="hash-link" aria-label="Direct link to Footnotes" title="Direct link to Footnotes">​</a></h2>
<ol>
<li id="user-content-fn-1-f3baf9">
<p>eMarketer (2023). Worldwide Digital Ad Spending 2023. <em>Insider Intelligence</em>. <a href="https://www.insiderintelligence.com/content/worldwide-digital-ad-spending-2023" target="_blank" rel="noopener noreferrer">https://www.insiderintelligence.com/content/worldwide-digital-ad-spending-2023</a> <a href="https://ekimetrics.github.io/blog/Economics_online_ad#user-content-fnref-1-f3baf9" data-footnote-backref="" aria-label="Back to reference 1" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-2-f3baf9">
<p>Goldfarb, A. (2014). What is different about online advertising? <em>Review of Industrial Organization, 44</em>, 115-129. <a href="https://ekimetrics.github.io/blog/Economics_online_ad#user-content-fnref-2-f3baf9" data-footnote-backref="" aria-label="Back to reference 2" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-3-f3baf9">
<p>Goldfarb, A., &amp; Tucker, C. (2011). Substitution between offline and online advertising markets. <em>Journal of Competition Law and Economics, 7</em>(1), 37-44. <a href="https://ekimetrics.github.io/blog/Economics_online_ad#user-content-fnref-3-f3baf9" data-footnote-backref="" aria-label="Back to reference 3" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-4-f3baf9">
<p>Here, Internet display is defined as banners, videos, and social media ads. <a href="https://ekimetrics.github.io/blog/Economics_online_ad#user-content-fnref-4-f3baf9" data-footnote-backref="" aria-label="Back to reference 4" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-5-f3baf9">
<p>Naik, P. A., &amp; Raman, K. (2003). Understanding the impact of synergy in multimedia communications. <em>Journal of marketing research, 40</em>(4), 375-388. <a href="https://ekimetrics.github.io/blog/Economics_online_ad#user-content-fnref-5-f3baf9" data-footnote-backref="" aria-label="Back to reference 5" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-6-f3baf9">
<p>Iyer, G., Soberman, D., &amp; Villas-Boas, J. M. (2005). The targeting of advertising. <em>Marketing Science, 24</em>(3), 461-476 <a href="https://ekimetrics.github.io/blog/Economics_online_ad#user-content-fnref-6-f3baf9" data-footnote-backref="" aria-label="Back to reference 6" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-7-f3baf9">
<p>Heilpern, W. (2016, 22 mai). 26 of the most hilarious, unfortunate online ad placements. Business Insider. <a href="https://www.businessinsider.com/here-are-the-most-hilarious-unfortunate-online-ad-placements-ever-2016-5?r=US&amp;IR=T" target="_blank" rel="noopener noreferrer">https://www.businessinsider.com/here-are-the-most-hilarious-unfortunate-online-ad-placements-ever-2016-5?r=US&amp;IR=T</a> <a href="https://ekimetrics.github.io/blog/Economics_online_ad#user-content-fnref-7-f3baf9" data-footnote-backref="" aria-label="Back to reference 7" class="data-footnote-backref">↩</a></p>
</li>
</ol>
</section>]]></content:encoded>
            <category>Online Advertising</category>
            <category>Economics</category>
            <category>Econometrics</category>
            <category>Cross-media</category>
            <category>Contextual advertising</category>
            <category>Academic Research</category>
        </item>
        <item>
            <title><![CDATA[ClimateQ&A]]></title>
            <link>https://ekimetrics.github.io/blog/ClimateQ&amp;A</link>
            <guid>https://ekimetrics.github.io/blog/ClimateQ&amp;A</guid>
            <pubDate>Thu, 13 Jul 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[AI to make climate and biodiversity science more accessible.]]></description>
            <content:encoded><![CDATA[<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/ClimateQA-dc6ccd7ee02461ffd516b5d71680d886.jpg" width="1280" height="720" class="img_ev3q"></p></div>
<div align="justify"><p>ClimateQ&amp;A is an AI-powered tool that has been specifically developed to help address the challenges associated with understanding and accessing climate change and biodiversity-related literature. The tool aims to democratize access to the scientific literature of climate change and biodiversity, making it easier for researchers, policymakers, and the public to understand and use this critically important information. This article aims to provide users with information regarding the tool, encompassing its contextual framework, technical operation, as well as its inherent limitations.</p><p>Disclaimer : for simplicity, we use “climate” as an umbrella term to designate the phenomena of climate change &amp; biodiversity loss. We are currently thinking about more inclusive names.</p><p>For a shorter version of this article, please refer to the <a href="https://www.youtube.com/watch?v=DwGm0-53iTQ" target="_blank" rel="noopener noreferrer">conference ClimateQ&amp;A</a> by Datacraft and Ekimetrics (only in French).</p><p>Click <a href="https://www.climateqa.com/" target="_blank" rel="noopener noreferrer">here</a> to access the latest version of the tool, now featuring a kids version.</p><p>Click <a href="https://fr.linkedin.com/posts/th%C3%A9o-alves-da-costa-09397a82_bilan-2-mois-apr%C3%A8s-le-lancement-de-la-activity-7067146668965519361-AUgW" target="_blank" rel="noopener noreferrer">here</a> to access a preliminary analysis of the questions asked to the tool.</p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="i-introduction-why-climateqa">I. Introduction: Why ClimateQ&amp;A<a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#i-introduction-why-climateqa" class="hash-link" aria-label="Direct link to I. Introduction: Why ClimateQ&amp;A" title="Direct link to I. Introduction: Why ClimateQ&amp;A">​</a></h2>
<div align="justify"><p>Against the backdrop of rising global temperatures and the alarming decline of biodiversity, human societies are not catching up on action. Across geographies, widespread climate literacy remains low, which limits and hinders actions to mitigate and adapt to these pressing challenges. This lack of actionable knowledge is due to several factors, including the increasing demand for climate-related information from all groups of society (including the public sector, students, corporates, policymakers, investors, and local communities, among many others), and rising fake news &amp; climate skepticism. Another important issue is that scientific reports of IPCC and IPBES present some inherent limitations regarding accessibility, language, and understanding. Indeed, at a Side Event at COP21 in Paris, in the lead-up to the Oslo Expert Meeting on Communication in 2016, IPCC Chair Hoseung Lee asked :</p><blockquote>
<p>“<em>What use are the IPCC reports if many of the intended users cannot understand them, do not know where to find them, or cannot use them in their work?”</em></p>
</blockquote><p>In this part we explore these issues, which are vital to inform the development of our tool ClimateQ&amp;A.</p></div>
<br>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="aclimate-knowledge-and-literacy-are-still-lagging">A.	Climate knowledge and literacy are still lagging<a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#aclimate-knowledge-and-literacy-are-still-lagging" class="hash-link" aria-label="Direct link to A.	Climate knowledge and literacy are still lagging" title="Direct link to A.	Climate knowledge and literacy are still lagging">​</a></h3>
<div align="justify"><p>In order to act against climate change, citizens need to be climate-literate:</p><blockquote>
<p><em>"Climate literacy is a subset of the broader science literacy that refers to the knowledge, skills, and attitudes that individuals, communities, and societies need to understand and address climate change effectively. It draws on climate science, the quantitative and geospatial technologies by which it is understood, and the interconnectedness of human beings with their environment . Literacy on climate change is vital for informed decision-making, emissions reduction, and community resilience."</em> <sup><a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fn-1-8d0ebe" id="user-content-fnref-1-8d0ebe" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup></p>
</blockquote><p>Despite the importance of climate literacy, studies find that in advanced economies, ~70% of people are aware of climate change and its consequences, but only 20% are climate-literate<sup><a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fn-2-8d0ebe" id="user-content-fnref-2-8d0ebe" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup> <sup><a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fn-3-8d0ebe" id="user-content-fnref-3-8d0ebe" data-footnote-ref="true" aria-describedby="footnote-label">3</a></sup>.
Additionally, climate awareness is unequally distributed: in 2015, 40% of adults in the world had never heard of climate change – roughly 2Bn people. In some developing countries like Egypt, Bangladesh, or India, this represents more than 65% of the adult population<sup><a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fn-4-8d0ebe" id="user-content-fnref-4-8d0ebe" data-footnote-ref="true" aria-describedby="footnote-label">4</a></sup>.</p></div>
<br>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="bthe-demand-for-climate-related-information-is-increasing">B.	The demand for climate-related information is increasing<a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#bthe-demand-for-climate-related-information-is-increasing" class="hash-link" aria-label="Direct link to B.	The demand for climate-related information is increasing" title="Direct link to B.	The demand for climate-related information is increasing">​</a></h3>
<div align="justify"><p>Driven by the global and far-reaching implications of climate change - across geographies, populations, sectors of activity, organizations- the demand for accurate, timely, and relevant climate information is increasing. Businesses and investors seek to understand their exposure to climate and biodiversity-related risks. Educators need to train a new generation of students, who will live with the consequences of climate change. Policymakers require reliable information for effective strategies. The media plays a vital role in disseminating factual information to the general public. As climate change shapes our world, this demand will continue to grow, requiring informed decision-making across sectors.</p><p>Another significant catalyst for information demand stems from the targeting of the IPCC by misinformation campaigns. The surge in media coverage surrounding climate change has witnessed a simultaneous surge in influential campaigns disseminating misinformation. These campaigns employ tactics such as selective media exposure, contrived controversies, alternative facts, and distorted media balance. Adding to this, social media algorithms amplify existing social circles and reinforce pre-existing opinions, while individuals increasingly rely on these platforms as their primary sources of information. Consequently, people find themselves trapped in echo chambers, unaware of the consensus or under the false impression that substantial uncertainty exists<sup><a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fn-5-8d0ebe" id="user-content-fnref-5-8d0ebe" data-footnote-ref="true" aria-describedby="footnote-label">5</a></sup>. Frequently, socio-economic narratives and a sense of loss resulting from climate change policies direct individuals toward disbelief and denial. These narratives create a clash between scientific communications on climate change and one's familiar ideas and way of life, often intertwined with automobiles, industry, consumption, and a carbon-centric economy. As climate skepticism is based on narratives that do not even revolve around climate itself<sup><a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fn-6-8d0ebe" id="user-content-fnref-6-8d0ebe" data-footnote-ref="true" aria-describedby="footnote-label">6</a></sup>, climate science communications have often achieved limited success.</p><div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/Picture1-b412ea7d57e7a93ea09abf2ec6efa1fb.png" width="449" height="304" class="img_ev3q"></p></div><p>In this context, it becomes paramount for climate scientists to multiply their communication endeavors and generate tailored content that caters to diverse audiences. By doing so, they would ensure that each segment of society has access to information that is relevant to their specific needs and concerns. This would enable users to not only access the desired information but also empower them to fact-check the information they come across. While the IPCC has made significant efforts to improve its communication strategy (e.g.; the organization held The Expert Meeting on Communication of 2016 which led to a number of recommendations to enhance IPCC communications activities, strategy and capacity), there are a number of limitations to this strategy stemming from how these organizations operate.</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/Screenshot_Twitter_ClimateQA-312a1bc148ecfb2c621262e7c0760339.png" width="598" height="679" class="img_ev3q"></p></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="cthe-role-of-the-ipcc-and-ipbes--practical-limitations-and-consequences">C.	The role of the IPCC and IPBES : practical limitations and consequences<a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#cthe-role-of-the-ipcc-and-ipbes--practical-limitations-and-consequences" class="hash-link" aria-label="Direct link to C.	The role of the IPCC and IPBES : practical limitations and consequences" title="Direct link to C.	The role of the IPCC and IPBES : practical limitations and consequences">​</a></h3>
<div align="justify"><p>IPCC and IPBES reports are the Gold Standard of the science of climate and nature, as they present a scientific consensus and synthetize in great detail, coverage, and historical depth the key considerations regarding living and countering with climate change, and the trends of biodiversity and nature’s contribution to people. Despite the unquestionable thoroughness and quality of these assessments, challenges to their true adoption by policymakers and civil society persist. Some practical challenges include:</p><ul>
<li>
<p><strong>Language.</strong> One significant limitation is the language barrier. Due to their affiliation with the United Nations, the IPCC and IPBES reports are only available in the UN's official languages, namely Arabic, Chinese, French, Russian, Spanish, and English. While these languages are spoken by approximately four billion people as either a first or second language, which accounts for 50% of the global population, the remaining 50% are unable to access climate science information from official sources due to this language restriction.</p>
</li>
<li>
<p><strong>Audience.</strong> Another challenge lies in the intended audience of these reports. Originally designed to provide a comprehensive review and recommendations on the state of climate science, the main recipients of these reports are technical audiences, such as scientists and policymakers. The IPCC and IPBES produce various types of documents, including Summary for Policymakers (SPMs), Full Reports, and Technical Summaries. These reports are often lengthy, exceeding 15,000 pages for the main IPCC and IPBES reports, and are filled with specialized terminology, such as confidence levels and uncertainty. Additionally, these reports are accessible only in PDF format through the respective organizations' websites.</p>
</li>
</ul><p>The level of complexity of this publications is such that at the 5th Assessment Report, senior policymakers called for a “Summary for Citizens” to complement the Summary for Policymakers<sup><a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fn-7-8d0ebe" id="user-content-fnref-7-8d0ebe" data-footnote-ref="true" aria-describedby="footnote-label">7</a></sup>.</p><p>Moreover, as the organizations' mandates emphasize neutrality and a "policy-relevant" but not "policy-prescriptive" approach, their ability to communicate and engage with stakeholders is limited. They are constrained from publishing any information that has not undergone rigorous multilateral validation or that lacks contextualization. With the general purpose of bridging climate and nature science to policymaking, both organizations are built on three emergent principles: holding the line between policy relevance and prescription, enlisting geographically diverse participants, and evolving a thicket of procedures to guard scientific credibility<sup><a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fn-8-8d0ebe" id="user-content-fnref-8-8d0ebe" data-footnote-ref="true" aria-describedby="footnote-label">8</a></sup>. The first principle aims to provide policy-relevant information without prescribing specific policies<sup><a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fn-9-8d0ebe" id="user-content-fnref-9-8d0ebe" data-footnote-ref="true" aria-describedby="footnote-label">9</a></sup>, thus upholding neutrality and respecting the mandates of multilateral environmental agreements<sup><a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fn-10-8d0ebe" id="user-content-fnref-10-8d0ebe" data-footnote-ref="true" aria-describedby="footnote-label">10</a></sup>. In an official statement, the IPCC declares :</p><p><em>“By endorsing the IPCC reports, governments acknowledge the authority of their specific content. The work of the organization is therefore policy-relevant and yet policy-neutral, never policy-prescriptive<sup><a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fn-11-8d0ebe" id="user-content-fnref-11-8d0ebe" data-footnote-ref="true" aria-describedby="footnote-label">11</a></sup>.”</em></p><p>What this means, is that in order to publish approved, adopted, and accepted reports, these organizations avoid technocratically mandating a particular policy solution when a range of options are feasible (even to different degrees)<sup><a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fn-12-8d0ebe" id="user-content-fnref-12-8d0ebe" data-footnote-ref="true" aria-describedby="footnote-label">12</a></sup>. While this neutrality reinforces the credibility that is derived from their strict processes, it gives rise to two major improvement areas:</p><ol>
<li>
<p><strong>The need to consider reports collectively</strong><sup><a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fn-13-8d0ebe" id="user-content-fnref-13-8d0ebe" data-footnote-ref="true" aria-describedby="footnote-label">13</a></sup>.  To avoid missing important information, it is essential to consider the entire body of reports collectively. The IPCC reports undergo a formal review process involving multiple drafts. Comments and feedback are provided by scientific experts, as well as representatives from governments and appointed consultants. These comments are taken into account during the approval of the Summary for Policymakers (SPM) and acceptance of the full report. Given that SPMs are subject to a high degree of compression, each sentence requires approval from the parties involved (scientists and government representatives), which often leads to rigorous discussions<sup><a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fn-14-8d0ebe" id="user-content-fnref-14-8d0ebe" data-footnote-ref="true" aria-describedby="footnote-label">14</a></sup>. While this dialogue-based approach strengthens the IPCC assessment, it is important to make other relevant documents accessible to interested audiences.</p>
</li>
<li>
<p><strong>That scientific knowledge must be accessible to non-technical audiences</strong>. Building upon the previous point, the strict rules governing report texts restrict their modification. The IPCC faces challenges in presenting reports to non-specialists in a language that is more accessible, avoiding jargon such as confidence levels and likelihood language<sup><a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fn-15-8d0ebe" id="user-content-fnref-15-8d0ebe" data-footnote-ref="true" aria-describedby="footnote-label">15</a></sup>, as well as reducing their length. While the IPCC may face criticism for delivering messages in a clearer, shorter, and potentially less rigorous manner, collaboration with other stakeholders can help bridge this gap.</p>
</li>
</ol></div>
<br>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="ii-implications-for-the-tool--desired-features--outcomes">II. Implications for the tool : desired features &amp; outcomes<a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#ii-implications-for-the-tool--desired-features--outcomes" class="hash-link" aria-label="Direct link to II. Implications for the tool : desired features &amp; outcomes" title="Direct link to II. Implications for the tool : desired features &amp; outcomes">​</a></h2>
<div align="justify"><p>Stemming from 1. The need for increased climate-related information in a relevant and fast manner and 2. The practical limitations stemming from IPCC and IPBES operating principles, we derive some features to inform the product development of ClimateQ&amp;A. The tool must be, and provide answers that are:</p><ul>
<li><strong>Truthful</strong>: Our tool aims to present climate change and biodiversity information as accurately as it is documented in scientific reports. It serves as a reliable resource for fact-checking fake news, and its content can be easily fact-checked with the original sources. It should also follow the structure of IPCC reports, from SPMs to Full Reports.</li>
<li><strong>Clear</strong>: With a focus on catering to diverse audiences, our tool avoids technical jargon and provides concise answers that are easily understandable by both technical and non-technical users. It ensures that complex concepts, such as uncertainty and confidence levels, are explained in a user-friendly manner.</li>
<li><strong>Comprehensive</strong>: The tool compiles information from a wide range of available reports, supplementing individual report findings with additional sources and relevant discoveries. This comprehensive approach ensures that users gain a holistic understanding of the subject matter.</li>
<li><strong>Accessible</strong>: Our tool is designed to be freely accessible to all users, transcending language barriers by providing multilingual support. It is optimized for compatibility across various devices, allowing users to access the tool seamlessly.</li>
<li><strong>Fast &amp; Intuitive</strong>: By utilizing advanced search functionalities, our tool enables users to navigate through reports swiftly, eliminating the need for cumbersome manual searches (e.g., Ctrl + F). It automatically identifies and retrieves relevant words and topics related to the user's query, streamlining the information retrieval process.</li>
<li><strong>Collaborative</strong>: The development of our tool is a collaborative effort that incorporates feedback and input from stakeholders across different domains, including non-technical users, technical experts, and scientists. This collaborative approach ensures that the tool meets the needs and expectations of a diverse user base.</li>
</ul><p>We have designed the technical core of the tool to fit the aforementioned specifications.</p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="iii-technical-core">III. Technical core<a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#iii-technical-core" class="hash-link" aria-label="Direct link to III. Technical core" title="Direct link to III. Technical core">​</a></h2>
<div align="justify"><p>ClimateQ&amp;A is built of several technical algorithmic modules, ChatGPT being the last one (the generation of an answer). The core of Climate Q&amp;A consists of three steps, summarized in Figure 1: creation of a structured dataset, question enrichment and sourcing, and generation &amp; display in an interface.</p><div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/Picture2-b1aa91178ecc7f67c433bbc28fb1a8df.svg" width="565" height="318" class="img_ev3q"></p></div><div align="center">Figure 1: Climate Q&amp;A global architecture. First, a structure dataset is created from a set of heterogeneous documents in an offline mode (1.Structured dataset creation).
Then, user questions  are enriched and sources (step 2). Finally, the answers with its sources are displayed in a web application.</div><br><p>All the documents used for Climate Q&amp;A are listed in the application page (section Sources) : <a href="https://huggingface.co/spaces/Ekimetrics/climate-question-answering" target="_blank" rel="noopener noreferrer">https://huggingface.co/spaces/Ekimetrics/climate-question-answering</a></p><ins> Step 1: Creation of a structured dataset from a set of heterogeneous documents</ins><br><p>The first step parses a heterogeneous set of documents (PDFs, Words etc) and extracts information, which is stored in a structured database.</p><p>The algorithm uses OCR techniques for all the documents stored in any given folder. The document structure is preserved, meaning that paragraphs are linked to section titles, figures are linked to the passages that cite them, etc. A structured database is thus created, containing all the information extracted from the documents and the relationships between the different entries in the database. These entries are then represented as a vector of finite dimension. In ClimateGPT, we use SentenceBert, but any type of representation is possible. This will serve as the basis for a quick search in the database for step 3.</p><ins>Step 2: Enrichment of the user's LLM query</ins><br><p>In the second step, the user's query is reformulated and enriched by searching for similar passages in the structured database. The relevant question and passages are encapsulated in a prompt, which also serves to limit the scope of acceptable answers for ClimateQ&amp;A.</p><p>Within the step of enriching the user's query, the goal is to create a query on an LLM from the question, whose answer will be formulated from the documents of interest (the structured database from step 1). First, the user's question is reformulated in a more intelligible way for a LLM by asking the model to do it. Then, the new question is compared to all entries in the database using the Faiss algorithm developed by Facebook, known for its execution speed on data corpora up to billions of entries. The most relevant entries, meaning those with the most significant similarity to the question, are selected, filtered by a thematic classification model (in this case, does the entry discuss climate?) and encapsulated in a prompt, necessary for querying an LLM (in our case, GPT-3.5 Turbo). The prompt contains other information, such as not going beyond the scope of its knowledge and formulating its answer based on the selected entries. The prompt created is then used to query an LLM.</p><ins>Step 3: Displaying the results : generative part</ins><p>The query result (i.e.; the question) is displayed on an interface. The answer is generated using the 10 most relevant references found in the corpus of documents. The sources are displayed alongside the answer, so that the user can verify the information and extract it for a report if necessary.
The display is based on two parts: the LLM's answer with the notes serving as a reference, as well as the sources used to formulate the answer. For ClimateQ&amp;A, we use the OpenAI Azure environment to send the request and get the response. Just like the ClimateQ&amp;A interface, the user can continue their search by asking several questions in a row like a real ChatBot.</p><ins>Comments and future work</ins><p>We use GPT-3.5 Turbo for Climate Q&amp;A, but our methodology can be applied to any type of LLM. We have chosen this tool for the generative part of our model because it has been exceptionally optimized for dialogue using Reinforcement Learning with Human Feedback (RLHF) – a method that uses human demonstrations and preference comparisons to guide the model towards desired behaviors. In the future we will test other models like Bard and Llama.</p><p>The hierarchy of the documents are not taken into account in the first version. Summary reports can be used in a first time to a concise answer. If the user wants more details, global and longer reports can then served as references to add additional facts, numbers to the answer. Looking for passages or references in the most recent documents could also make our approach more relevant. This feature will be added in the next version of Climate Q&amp;A.</p><p>Qdrant is an excellent alternative to FAISS algorithm for search similarity, as it allows natively metadata featuring ( e.g. : search for a specific set of documents, or creating filters on passages), whereas FAISS can not. Moreover, Qdrant is faster for a small amount of data and is easier to use and maintain in production. We will add Qdrant as an option of similarity search for our next version.</p><p>The interface of the first version of Climate Q&amp;A displays textual information only. In the future, we will integrate images and graphics to enrich the answer. Building a specific graph for the answer would also be a notable improvement. For example, a graph which shows the evolution of the carbon footprint for a country per year is of a great interest to the question : “Did the carbon footprint of France increase the last five years?”</p><p>Textual data is now the main source of references for Climate Q&amp;A, but a lot of information is contained in tabs or graphics (like checked numbers etc). One of the main improvements for Climate Q&amp;A is to better parse and retrieve information from graphics and tabs. It will include novels features in OCR techniques and information retrieval techniques.</p></div>
<br>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="ivalgorithm-performance">IV.	Algorithm performance<a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#ivalgorithm-performance" class="hash-link" aria-label="Direct link to IV.	Algorithm performance" title="Direct link to IV.	Algorithm performance">​</a></h2>
<div align="justify"><p>In order to assess the performance of our model, we evaluate the individual performances of each of the elements that form the generation of an answer.
<ins>Retrieval</ins>: evaluating the retrieval process specifically on our data is a difficult task without a custom labialized dataset. Our approach to quantitatively evaluate it was to sample a thousand paragraphs from our knowledge base, and for each ask a LLM to generate a question that could be answered with each paragraph. Then, we did the inverse process and tried retrieving the documents using the questions. For the model we currently use, the paragraph was in the top 10 documents retrieved in 3 cases out of 4. There are lots of biases to this method and we are still working to refine it, but the results allow us to test and compare performance for different retrieval models.</p><br><br><p><ins>Generation</ins>: answers provided by ClimateQ&amp;A are subject to a human evaluation whose evaluation criteria are outlined below.</p><br><br><p>We analyse the performance of our model by comparing the answers of ChatGPT and ClimateQ&amp;A to those provided by the IPCC, assumed to be a “ground of truth”. We evaluate the answers based on the below criteria, i.e. relevance, accuracy and completeness, structure, quoting, and consistency. These serve as a qualitative measure of performance for the algorithm.</p><div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/Picture3-6296bf595cc1ef3cb0a95109b789a3cd.png" width="718" height="814" class="img_ev3q"></p></div></div>
<br>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="vlimitations">V.	Limitations<a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#vlimitations" class="hash-link" aria-label="Direct link to V.	Limitations" title="Direct link to V.	Limitations">​</a></h2>
<div align="justify"><p>ClimateQ&amp;A has been built to answer to climate and biodiversity-related questions by finding 10 relevant paragraphs in the original IPCC and IPBES documents and generating an answer from those paragraphs. There are inherent limitations to this configuration and the underlying assets and tools that we use to parse the documents, retrieve relevant sources, and generate answers. In this part, we elaborate on these limitations, which we are planning to address in the following versions of the tool. These limitations are related to the content of the answers, biases inherent to the configuration of the model and the generative module, and the environmental footprint of AI.</p><p><strong>Limitations regarding the answers</strong> : ClimateQ&amp;A performs well in questions that are somewhat specific and whose answers can be found in the reports.</p><ul>
<li>ClimateQ&amp;A does not work well when it comes to summarizing entire documents. For example, a question such as “What are the conclusions of the last IPCC report on climate mitigation?” will not yield a high quality answer. The questions need to be more precise and concise in order for the retriever to find relevant passages and produce a better answer.</li>
<li>ClimateQ&amp;A cannot directly answer questions whose answers cannot be found in the reports, but that are relevant to understand and fight against global warming in specific contexts, for example “What is the IPCC’s position on the use of megadams as a solution to climate change?”, “Which French political party has the strongest measures against climate change?”, etc. These questions might not be answered precisely and could even be blocked by our moderation feature.</li>
<li>For other limitations of GPT-3 models regarding text synthesis and structural &amp; algorithmic limitations, please refer to (Brown, et al. 2020).</li>
</ul><p><strong>Bias</strong>: there are some bias associated to how the model is built and the underlying assets.</p><p><ins>Bias from the generation of answers linked to ChatGPT</ins>: GPT-3 was trained on a massive dataset of text from the open internet, including the entire public internet toughly every month since its inception (60% of weight in training mix, also called CommonCrawl), a crowd-sourced curated selection of the internet most popular page referrals from Reddit and Social Media (with a weight of 22%, also called the WebText2 dataset), portions of books that are available online (16%), and Wikipedia in English (3%).</p><p>Though sources are diverse, the biases of the data that the model has been trained on are retained. Biases favor language &amp; culture (predominantly English and Western-centric) and men. Conversely, negative sentiments are recorded for black people and Islam in GPT-3<sup><a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fn-16-8d0ebe" id="user-content-fnref-16-8d0ebe" data-footnote-ref="true" aria-describedby="footnote-label">16</a></sup>. These biases represent an important aspect to assess the broader societal impacts of tools like ClimateQ&amp;A, especially as those biases translate in prejudices and views about climate change, which are, as seen in Part 1, very context-specific. In general, we find that the Anglo-Saxon single-materiality approach, i.e. to consider climate change risks that might have a financial impact on businesses (and not also the impacts of businesses on climate change), might produce some biases in our model. ChatGPT has also been finetuned via human feedback and user feedback. These might represent additional biases to the model.</p><br><p><ins>Bias in structure and selection</ins> :  The developers of ClimateQ&amp;A are European data scientists, engineers and climate / biodiversity consultants. There may be some biases related to the background of these persons, as well as the fact that none are climate or biodiversity scientists. While we have received feedback from expert scientists, most users that have given feedback are non-technical regarding climate change. These might influence how the model is built and the answers that are generated.</p><br><br><p><ins>Positive bias in the questions that are asked to the algorithm</ins>: we have introduced a positive bias in the questions asked to the model by providing a reformulation feature that softens the contents of the answers. For example, questions that read “Should we eat the rich?” were reformulated as “What are the most effective ways to reduce the carbon footprint of the wealthiest people in society?”.</p><br><br><p><strong>Limitations regarding the environmental impact of AI</strong>: training and using large AI models requires substantial amounts of resources. The progress driving large models has been achieved through increasingly large and computationally-intensive deep learning models and in general, improving accuracy of models is at the expense of economic, environmental, and social costs<sup><a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fn-17-8d0ebe" id="user-content-fnref-17-8d0ebe" data-footnote-ref="true" aria-describedby="footnote-label">17</a></sup>. In order to evaluate the environmental impact of our model, we need indicators that measure the environmental impact of each module (given that we don’t train the model ourselves), and that can be aggregated for total. As the environmental impacts of AI depend on the local energy infrastructure, hardware used, running time, and the number of parameters, there are still few measures available to perform this task and inform users.</p><p><ins>Carbon footprint</ins>:</p><p>to estimate the carbon footprint of our model, we use CodeCarbon, a Python package that estimates the power consumption of hardware (GPU, CPU and RAM) and applies the carbon intensity of the region where the computing is done<sup><a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fn-18-8d0ebe" id="user-content-fnref-18-8d0ebe" data-footnote-ref="true" aria-describedby="footnote-label">18</a></sup>. We estimate the following emissions for each phase of the project at Ekimetrics.</p><br><br><div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/Picture4-bccede718c7194ba56700afbae8b3210.png" width="742" height="219" class="img_ev3q"></p></div><p>*Link for <a href="https://medium.com/@chrispointon/the-carbon-footprint-of-chatgpt-e1bc14e4cc2a" target="_blank" rel="noopener noreferrer">The carbon footprint of ChatGPT</a></p><br><p>Carbon Emissions are <strong>relatively low but not negligible</strong> compared to other usages: one question asked to ClimateQ&amp;A is around 0.482gCO2e - equivalent to 2.2m by car and 1ml of water<sup><a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fn-19-8d0ebe" id="user-content-fnref-19-8d0ebe" data-footnote-ref="true" aria-describedby="footnote-label">19</a></sup>.
In this calculation we do not include the carbon footprint of training ChatGPT (done by OpenAI) that has been estimated to 1,287MWh, corresponding to 552 tons of CO2 equivalent, which is similar to 1.2M liters of water and 2.5M km by car, 63x the Earth’s circumference. These calculations are estimates and should be considered carefully<sup><a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fn-20-8d0ebe" id="user-content-fnref-20-8d0ebe" data-footnote-ref="true" aria-describedby="footnote-label">20</a></sup>.</p><div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/Picture5-9ff3cf64645e3d04670bd7a174b457e4.png" width="605" height="338" class="img_ev3q"></p></div></div>
<!-- -->
<section data-footnotes="true" class="footnotes"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5 sr-only" id="footnote-label">Footnotes<a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#footnote-label" class="hash-link" aria-label="Direct link to Footnotes" title="Direct link to Footnotes">​</a></h2>
<ol>
<li id="user-content-fn-1-8d0ebe">
<p>Lesley-Ann L. et Cole 2018 <a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fnref-1-8d0ebe" data-footnote-backref="" aria-label="Back to reference 1" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-2-8d0ebe">
<p>Oliver and Adkins 2020 <a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fnref-2-8d0ebe" data-footnote-backref="" aria-label="Back to reference 2" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-3-8d0ebe">
<p>Bell, et al. 2021 <a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fnref-3-8d0ebe" data-footnote-backref="" aria-label="Back to reference 3" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-4-8d0ebe">
<p>Lee, et al. 2015 <a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fnref-4-8d0ebe" data-footnote-backref="" aria-label="Back to reference 4" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-5-8d0ebe">
<p>Fake news threatens a climate literate world 2017) <a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fnref-5-8d0ebe" data-footnote-backref="" aria-label="Back to reference 5" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-6-8d0ebe">
<p>Lejano 2019 <a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fnref-6-8d0ebe" data-footnote-backref="" aria-label="Back to reference 6" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-7-8d0ebe">
<p>Lynn, Araya, et al. 2016) <a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fnref-7-8d0ebe" data-footnote-backref="" aria-label="Back to reference 7" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-8-8d0ebe">
<p>De Pryck and Hulme 2022 <a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fnref-8-8d0ebe" data-footnote-backref="" aria-label="Back to reference 8" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-9-8d0ebe">
<p>IPCC, Appendix A to the Principles Governing IPCC Work 2013 <a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fnref-9-8d0ebe" data-footnote-backref="" aria-label="Back to reference 9" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-10-8d0ebe">
<p>IPBES, Functions, operating principles and institutional arrangements of the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services 2012 <a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fnref-10-8d0ebe" data-footnote-backref="" aria-label="Back to reference 10" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-11-8d0ebe">
<p>IPCC, Organization n.d. <a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fnref-11-8d0ebe" data-footnote-backref="" aria-label="Back to reference 11" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-12-8d0ebe">
<p>Havstad and Brown 2017 <a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fnref-12-8d0ebe" data-footnote-backref="" aria-label="Back to reference 12" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-13-8d0ebe">
<p>Havstad and Brown 2017 <a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fnref-13-8d0ebe" data-footnote-backref="" aria-label="Back to reference 13" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-14-8d0ebe">
<p>Broome 2014 <a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fnref-14-8d0ebe" data-footnote-backref="" aria-label="Back to reference 14" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-15-8d0ebe">
<p>Lynn and Peeva, Communications in the IPCC’s Sixth Assessment Report cycle 2021 <a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fnref-15-8d0ebe" data-footnote-backref="" aria-label="Back to reference 15" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-16-8d0ebe">
<p>Brown, et al. 2020 <a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fnref-16-8d0ebe" data-footnote-backref="" aria-label="Back to reference 16" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-17-8d0ebe">
<p>Schwartz, et al. 2019 <a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fnref-17-8d0ebe" data-footnote-backref="" aria-label="Back to reference 17" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-18-8d0ebe">
<p>BCG Gamma 2020 <a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fnref-18-8d0ebe" data-footnote-backref="" aria-label="Back to reference 18" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-19-8d0ebe">
<p>ADEME n.d. <a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fnref-19-8d0ebe" data-footnote-backref="" aria-label="Back to reference 19" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-20-8d0ebe">
<p>Patterson, et al. 2022 <a href="https://ekimetrics.github.io/blog/ClimateQ&amp;A#user-content-fnref-20-8d0ebe" data-footnote-backref="" aria-label="Back to reference 20" class="data-footnote-backref">↩</a></p>
</li>
</ol>
</section>]]></content:encoded>
            <category>Climate</category>
            <category>biodiversity</category>
            <category>generative AI</category>
            <category>genAI</category>
            <category>IPCC</category>
            <category>IPBES</category>
            <category>OCR</category>
        </item>
        <item>
            <title><![CDATA[Does your company level with your car in terms of Analytics?]]></title>
            <link>https://ekimetrics.github.io/blog/2023/03/07/level_car_analytics</link>
            <guid>https://ekimetrics.github.io/blog/2023/03/07/level_car_analytics</guid>
            <pubDate>Tue, 07 Mar 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[See what ABS and ESP do in a car and what it corresponds to in a company. This article advocates real-time analytics for ML insights. ]]></description>
            <content:encoded><![CDATA[<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/level_car_analytics-6c2b9d1a859a0af66c1a2fe9d626982a.jpg" width="2992" height="2000" class="img_ev3q"></p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="introduction">Introduction<a href="https://ekimetrics.github.io/blog/2023/03/07/level_car_analytics#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h2>
<div align="justify"><p>Data Engineers at Ekimetrics use Spark to execute data ingestion and data preparation tasks, and Spark can process incoming data in batches as well as data in real-time streams. We also work on GCP and Cloud Dataflow, where Apache Beam provides the same flexibility to process streams as well as batches.</p><p>But IT teams ans Business teams are so used to batch analytics that it is hard to convey the value of real-time or up-to-date analytics, even though we now have platforms to handle the sources that can stream data. And with the emergence of micro-services, we also have more and more of these sources.</p><p>Based on current market evolutions and on our previsions:</p><ul>
<li><strong>Companies should create real-time views</strong> of customers, products, suppliers and production plans on an easy-to-grow platform with the adequate governance,</li>
<li><strong>Companies should assemble a real-time analytical view of their whole business</strong> to be able to steer it appropriately,</li>
</ul><p>Because this article is about real-time analytics, it starts with the definition of these terms. We use the example of driving a car as parallel for running a company. We all drive cars, we all see cars change, for example from “manual gearbox” to “automatic gearbox” to “no more gearbox”, and we all have a lot of knowledge about cars. Car technology will be used in this article to illustrate the following questions:</p><ul>
<li>What is the minimal data available to drive a car and what is the benefit of additional data ?</li>
<li>How to transpose for example the ABS of a car into a platform to help drive a company ?</li>
</ul><br><p>Below are the steps in this article, as they will be presented and explained:</p><div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Global_Table_1-9067e1c766fae75a98d1cc3069117889.png" width="605" height="725" class="img_ev3q"></p></div><br></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="definitions">Definitions<a href="https://ekimetrics.github.io/blog/2023/03/07/level_car_analytics#definitions" class="hash-link" aria-label="Direct link to Definitions" title="Direct link to Definitions">​</a></h2>
<div align="justify"><p>Let’s first introduce definitions of the two topics of this article (independent of the car illustration):</p><p><strong>Analytics</strong>, as separated from <strong>Operational</strong>, is related to <strong>data</strong>:</p><ul>
<li><strong>Operational data</strong> is internal data from the business operations,</li>
<li><strong>Analytical data</strong> is operational data that is aligned, consolidated, reprocessed and possibly augmented with external data.</li>
</ul><br><p><strong>Real-time</strong>, as separated from <strong>Batch</strong>, is related to <strong>processing</strong>:</p><ul>
<li>
<p><strong>Batch processing</strong> is about processing data on a schedule, between every few hours and every month. Users gets an image that is always a bit old, but for many businesses and operations this is seen as acceptable,</p>
</li>
<li>
<p><strong>Real-time processing</strong> is about reflecting the current status of the operations. The latency depends of the source rather than the processing, it is between a few seconds and a few hours depending on the capability of the source system.</p>
</li>
</ul><p>When analyzing a business process, it is always a very interesting exercise to diagnose if a certain metric is operational or analytic. And if the data is analytic, the next question is to wonder if the business could benefit from having that metric in real-time.</p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="car-wise-lets-start-in-the-1950s">Car-wise, let’s start in the 1950s<a href="https://ekimetrics.github.io/blog/2023/03/07/level_car_analytics#car-wise-lets-start-in-the-1950s" class="hash-link" aria-label="Direct link to Car-wise, let’s start in the 1950s" title="Direct link to Car-wise, let’s start in the 1950s">​</a></h2>
<div align="justify"><p>I work in Paris, and tourists love to tour the city in Citroën 2CV, a French car that was produced between the 50s and the 80s, like the picture below from one of the numerous companies renting these cars.</p><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/Block_figures_1-b6db335c04027e9ef0a8beb8703d3b46.png" width="2492" height="649" class="img_ev3q"></p><p>No electronics in there, this is pure mechanics and a few electrical wires, an ideal starting point for our journey !</p><p>What data do you have to drive a Citroën 2CV? Speed, battery level, gas levels, total mileage, that’s it.
And the engine is so noisy that there is no need for an engine RPM meter !</p><p>According to the definitions above, the data is the direct readings of the mechanical operations of the car, for example from the speed of the wheels, so this is operational data.</p><p>No further processing here, so no analytical data at all.</p><br><p>Finally, let’s introduce this diagram of the car with no Analytics at all:</p><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/Diagram_1-60f3d7b918b8194ac8b89e4cc869f4d0.png" width="945" height="158" class="img_ev3q"></p><br><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-does-driving-a-2cv-transpose-to-driving-a-company">How does driving a 2CV transpose to driving a company?<a href="https://ekimetrics.github.io/blog/2023/03/07/level_car_analytics#how-does-driving-a-2cv-transpose-to-driving-a-company" class="hash-link" aria-label="Direct link to How does driving a 2CV transpose to driving a company?" title="Direct link to How does driving a 2CV transpose to driving a company?">​</a></h3><p>When we transpose to driving a company, operational data is the data that resides in the operational systems, such a CRMs and ERPs. It contains all the referential and transactional information necessary to run the company, from HR to Sales to Finance. All reports and dashboards that are directly embedded in these applications or close to them are also considered operational data.</p><p>Since there is enough information to run the business, we can jump to our first conclusion:</p><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/Diagram_2-2b6413adb9435c83218636d55314faac.png" width="1322" height="223" class="img_ev3q"></p><br></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="born-in-the-80s-and-mandatory-in-eu-in-2003-the-abs">Born in the 80s and mandatory in EU in 2003, the ABS<a href="https://ekimetrics.github.io/blog/2023/03/07/level_car_analytics#born-in-the-80s-and-mandatory-in-eu-in-2003-the-abs" class="hash-link" aria-label="Direct link to Born in the 80s and mandatory in EU in 2003, the ABS" title="Direct link to Born in the 80s and mandatory in EU in 2003, the ABS">​</a></h2>
<div align="justify"><p>The ABS, or Anti-lock Braking System, measures the speed of each wheel and reduces the pressure on the brakes to avoid the wheel locking up when losing grip.</p><p>To brake efficiently, it applies “threshold braking” and “cadence braking” that are techniques of the most skillful drivers, taken here at a much faster rate than what these drivers can perform.</p><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/Diagram_3-540eed6d65cce9d3f553a6ea992c35b7.png" width="1323" height="266" class="img_ev3q"></p><br><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-is-real-time-analytical-processing-of-operational-data-in-a-business">What is real-time analytical processing of operational data in a business?<a href="https://ekimetrics.github.io/blog/2023/03/07/level_car_analytics#what-is-real-time-analytical-processing-of-operational-data-in-a-business" class="hash-link" aria-label="Direct link to What is real-time analytical processing of operational data in a business?" title="Direct link to What is real-time analytical processing of operational data in a business?">​</a></h3><p>If we change the wheels of the cars by the BUs of a company, we get to a functional diagrams like the one below, where each BU includes efficient controls applied in real-time on the data from the operations:</p><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/Diagram_4-1fbf05bf4351f9e1586c4c3fb5f7f47a.png" width="1323" height="238" class="img_ev3q"></p><br><p>Data Science is applied locally on the operational data, for purposes like segmentation, scoring, pricing and other applications of Machine Learning. The purpose here is to get value from investments in each BU, <strong>but there are no insights to make the whole bigger than the sum of its parts.</strong> It is like in the example of the ABS, we want to replicate the techniques of the best players, locally in each domain.</p><p>It is important to note that local AI initiatives do not require a dedicated platform for analytical data and that improving the operations locally can be achieved in the operational applications. For example, SAP and Salesforce include more and more predictive features. And since these platform work on up-to-date information, <strong>embedding Machine Learning in operational systems is the closest to what ABS is in a car.</strong></p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="in-2014-the-esp-becomes-mandatory-in-all-european-cars">In 2014, the ESP becomes mandatory in all European cars<a href="https://ekimetrics.github.io/blog/2023/03/07/level_car_analytics#in-2014-the-esp-becomes-mandatory-in-all-european-cars" class="hash-link" aria-label="Direct link to In 2014, the ESP becomes mandatory in all European cars" title="Direct link to In 2014, the ESP becomes mandatory in all European cars">​</a></h2>
<div align="justify"><p>The ESP (Electronic Stability Program) is an anti-skid system. It leverages four wheel-speed sensors, the steering angle sensor and an inertial measurement unit to act on the break pressure of each wheel and engine acceleration command. Depending on the situation, it can reduce the speed of the engine and brake all wheels or activate the brake on only one wheel only to alter the heading of the car.</p><p>The diagram changes, since all insights are now consolidated before they are processed:</p><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/Diagram_5-c8a535f563ec37ed3f13c1fff4522610.png" width="1323" height="278" class="img_ev3q"></p><br><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-does-an-esp-looks-like-in-a-company">What does an ESP looks like in a company?<a href="https://ekimetrics.github.io/blog/2023/03/07/level_car_analytics#what-does-an-esp-looks-like-in-a-company" class="hash-link" aria-label="Direct link to What does an ESP looks like in a company?" title="Direct link to What does an ESP looks like in a company?">​</a></h3><p>In your company, we are leaving the territory of ERPs, because even SAP does not cover all aspects that must be monitored and given sense together.<br>
<!-- -->In addition, ERPs have many limitations regarding Advanced Analytics:</p><ul>
<li>ERPs are complex to query and it may impact production,</li>
<li>ERPs don’t need and don’t keep history,</li>
<li>ERPs don’t care about weak signals and don’t make sense of unstructured data,</li>
<li>ERPs are not designed to consolidate data with other ERPs or other operational platforms,</li>
<li>And lastly, when a change is interesting, it takes ages to impact ERPs.</li>
</ul><br><p>Because consolidation cannot occur in the operational systems, <strong>we introduce an additional data storage and processing layer called the analytical data plane</strong> . We assemble data platforms that can host data assets of different Business Units, with some local autonomy and some global governance, so insights can be derived from many elements. This is where the Lakehouse and the Federated Governance patterns apply.</p><p>This is also where we believe we must now work in real-time, like in the ESP in the car analogy. The driving force is that <strong>the analytical plane should be up-to-date with the data that is shared from the operational systems</strong>, it should not add an hourly or a daily delay between the times it processes data.</p><p>Most use-cases that we currently in batch will benefit from being executed with up-to-date data from the operations. Let’s take a two examples:</p><ul>
<li>
<p>The purpose of a Customer 360° use-case is to consolidate Transaction history, Social media presence, Customer support interactions, Segmentation history, Sensitivity to the promotions and prices, etc. into a single view of the customers.</p>
<p>Having that information always up-to-date could let you, for example:</p>
<ul>
<li>Target, or restrain from targeting, in certain situations,</li>
<li>Send messages at the right moment,</li>
<li>Propose Next Best Action when the customer calls,</li>
<li>etc.</li>
</ul>
</li>
<li>
<p>Expanding on that up-to-date Customer 360 and adding up-to-date supply-chain information and up-to-date suppliers information will enable new use-cases to better serve the customers needs.</p>
</li>
</ul><p>In addition to use cases, and if we consider a company as a complex system, <strong>it is important to monitor all its moving parts together to better understand its reactions.</strong> And when there are days or weeks between an action and the monitoring of its effect, it is more difficult to steer.</p><p>For these reasons, we want to propose real-time as the new standard, today:</p><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/Diagram_6-211461493b954a49a017803de5ebd7e2.png" width="1324" height="237" class="img_ev3q"></p><br></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="mandatory-in-2022-aeb-is-the-current-state-of-the-art">Mandatory in 2022, AEB is the current state-of-the-art<a href="https://ekimetrics.github.io/blog/2023/03/07/level_car_analytics#mandatory-in-2022-aeb-is-the-current-state-of-the-art" class="hash-link" aria-label="Direct link to Mandatory in 2022, AEB is the current state-of-the-art" title="Direct link to Mandatory in 2022, AEB is the current state-of-the-art">​</a></h2>
<div align="justify"><p>In addition to sensors that focus on the inside of the car, AEB (Automatic Emergency Braking) uses sensors that look outside of the car, like Front radar sensor, multi-purpose dashboard camera(s) and corner radar sensors. All these components embark object detection and tracking with deep neural networks and share their insights to a central unit that can act on the car brakes.</p><p>This is the most complex system that we study here and we can anticipate that the next step will be cars exchanging information together about conditions of the road. In the projects we currently participate in on connected cars, no data is currently shared between cars.</p><p>So our last diagram is the most complete:</p><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/Diagram_7-75a42aaebbe8167b49183711dd738429.png" width="1325" height="321" class="img_ev3q"></p><br><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="do-you-have-aeb-like-system-in-your-company">Do you have AEB-like system in your company?<a href="https://ekimetrics.github.io/blog/2023/03/07/level_car_analytics#do-you-have-aeb-like-system-in-your-company" class="hash-link" aria-label="Direct link to Do you have AEB-like system in your company?" title="Direct link to Do you have AEB-like system in your company?">​</a></h3><p>AEB is about adding data sources to better anticipate market conditions, to better understand your customers, to better interact with your suppliers and your distributors, to gather data about your competitors, etc.</p><p>And by now you have understood that in the context of the AEB, the purpose is to consolidate these insights in real-time and leverage these to take the best decisions.</p><p>While buying or trading data is good, sharing data on platforms like Snowflake, BigQuery or Databricks Delta Sharing is better, because it is maintained in real-time by the other parties and you don’t have to replicate it in batches.</p><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/Diagram_8-4c937e00cd3427775844bd983709534d.png" width="1324" height="280" class="img_ev3q"></p><br></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="https://ekimetrics.github.io/blog/2023/03/07/level_car_analytics#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<div align="justify"><p>While the focus of this article has been on the value of up-to-date analytics, we’ll close this article with a preview of a next topic : the costs of the streaming datasets compared to traditional batch approaches.</p><p>Ekimetrics is investing a lot in making sure we manage the environmental aspects of the solutions we build, so this last point is critical! We currently work with Spark streams that have the .trigger(availableNow=True) option so they only process the newly arrived rows when the pipelines are executed every hour for example.<br>
<!-- -->So that it remains cost- and emission- efficient.</p><p>And as a closing remark, as I was looking for the dashboards of the Renault 4L, Peugeot 205 and other 2CV, I found that in the latest small car from Citroën, called the AMI, it is nearly as frugal as its predecessors !</p><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/Block_figures_2-36f628f6186899118cc13dfe1dd18e5b.png" width="2451" height="650" class="img_ev3q"></p><p>And it weighs only 500kg, which is frugal too.</p></div>]]></content:encoded>
            <category>Thought Leadership</category>
            <category>Data Processing</category>
            <category>Real-Time</category>
            <category>Analytics</category>
        </item>
        <item>
            <title><![CDATA[Building a datalake - Part 2 - Smart storage & computing strategies for better usability and usefulness]]></title>
            <link>https://ekimetrics.github.io/blog/2023/02/28/building_datalake_part_2</link>
            <guid>https://ekimetrics.github.io/blog/2023/02/28/building_datalake_part_2</guid>
            <pubDate>Tue, 28 Feb 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[For this second part of datalake building, we’ll go deeper into the journey of data, more specifically expand on storage and compute strategies.]]></description>
            <content:encoded><![CDATA[<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/overview_datalake_part_2_v2-2239a71ec2a5454c6e79ec8ed9c3ad48.png" width="5464" height="3640" class="img_ev3q"></p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="article-scope">Article Scope<a href="https://ekimetrics.github.io/blog/2023/02/28/building_datalake_part_2#article-scope" class="hash-link" aria-label="Direct link to Article Scope" title="Direct link to Article Scope">​</a></h2>
<div align="justify"><p>In a previous article - <a href="https://ekimetrics.github.io/blog/2022/02/07/building_datalake_part_1/" target="_blank" rel="noopener noreferrer">Building a datalake - Part 1 - Usable, Useful, Used, or how to avoid dataswamp and empty shell traps | Eki.Lab</a> - we took a look at the foundation architecture used at Ekimetrics when building a datalake. Its focus was to present design elements to ensure your datalake is useful and usable at its core, as well as best practices to avoid falling into the so-called data swamp and empty shell traps.</p><p>For this second part, we’ll go deeper into the journey of data, more specifically expand on storage and compute strategies, and see how the organisation of data and the way it is transformed impact a datalake’s usability &amp; usefuleness.</p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="introduction">Introduction<a href="https://ekimetrics.github.io/blog/2023/02/28/building_datalake_part_2#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h2>
<div align="justify"><p>The overview of data’s journey through a datalake or data platform can be broken down into five steps, represented below from left to right.</p><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/img1_data_journey-b88b0c7ad94bd8652615a4bdb7ae128a.png" width="1803" height="693" class="img_ev3q"></p><div align="center">Data’s journey, from source to usage</div><br><p>A data platform is highly versatile in providing technical options the five steps above. Getting your data from one step to the next means applying a strategy for how the data input is stored, how it is processed and how the output is exposed for the next step.</p><p>These strategies will vary depending on the use case and platform, taking business as well as technical constraints in consideration. Designing your datalake’s strategies to be versatile and homogeneous is essential. It allows your data platform to grow fast, in terms of data content as well as use case possibilities. It also ensures that the datalake is under control with a common way of treating data, where its only varying specifities are the entry point (data sources) and output (serving layer).</p><p>At Ekimetrics, we’ve developed versatile strategies that are applicable to most common use cases, easily reproducible. These strategies help build new capabilities and provide a better understanding of your data platform.</p></div>
<br>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="where-designing-storage--compute-strategies-really-matters">Where designing storage &amp; compute strategies really matters<a href="https://ekimetrics.github.io/blog/2023/02/28/building_datalake_part_2#where-designing-storage--compute-strategies-really-matters" class="hash-link" aria-label="Direct link to Where designing storage &amp; compute strategies really matters" title="Direct link to Where designing storage &amp; compute strategies really matters">​</a></h2>
<div align="justify"><p>In most datalake architectures, the data journey’s step where the most impactful design decisions can be made is <em>Data storage and processing</em>. The other steps are more straightforward:</p><ul>
<li>Data sources are usually out of the architect or data engineer’s control, as they often sit outside of the data platform (e.g. on a third-party server).</li>
<li>Ingestion is a step for which design questions around data validation &amp; organization processes may be worth considering. We’ve talked about these processes in the first part of our “Building a datalake” articles. As for streaming vs batch, it is only dependent on the source: if the source system is streaming data, a resource to ingest that is necessary ; otherwise, recurrent batch ingestion is the go-to.</li>
<li>Serving will depend on the target use case, so even if there are important design decisions to make, they will only be impactful in the scope of their use case, not for the whole datalake.</li>
<li>Usage will most of the time be outside the datalake and depend on the use case. The few design decisions that may be necessary here won’t be as impactful to the datalake’s usability and usefulness either.</li>
</ul><p>Of course, this is not to say that designing relevant strategies and architecture for these steps doesn’t matter: they must be tailored to the business case and technical constraints.</p><p>On the flipside, the <em>Data storage and processing</em> step is the central piece in the datalake puzzle. It is where storage and compute strategies will be the most impactful.</p><p>What could it look like, then ? Inside this central step, data transits through four zones, from its raw form to fully processed for a particular use case, ready to serve to your businesses. These four zones are detailed below, in between ingestion and serving.</p><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/img2_data_storage-905c99d1d5a0c6e5421ba02084e2063e.png" width="1803" height="500" class="img_ev3q"></p><div align="center">Data storage and processing - storage zones</div><br><p>These zones can be found under various names: Landing - Bronze - Silver - Gold, or Temp - Raw - Cleaned - Conformed, etc. The intent is the same, where data becomes more and more usable and business use case oriented with each zone.</p><p>In between each storage zone, organization and transformation processes are applied to organize and extract insights out of data. This is where our storage and computing strategies come in.</p></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="going-from-source-to-raw-storage-strategy">Going from source to raw: storage strategy<a href="https://ekimetrics.github.io/blog/2023/02/28/building_datalake_part_2#going-from-source-to-raw-storage-strategy" class="hash-link" aria-label="Direct link to Going from source to raw: storage strategy" title="Direct link to Going from source to raw: storage strategy">​</a></h3>
<div align="justify"><p>An ingested source will usually be exposed in one of two possible ways:</p><ul>
<li>Incremental changes, where only what is new or updated is exposed.</li>
<li>Full datasets, where all of the up-to-date data (or a new timeframe of data) is exposed.</li>
</ul><p>In these two cases, the ingestion strategy we recommend results in the same outcome. Here the strategy is to historize all received data, adding metadata about its reception date (or validity date) through organization in the storage architecture or in the dataset itself. The RAW storage zone becomes a source of historical knowledge about all data points and their changes through time. Doing this, we’re also able to add a “slowly changing dimension” / “change data capture” aspect to our data, where we can easily find a data point’s values at any moment in time.</p><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/img3_computing_1-cef3f81042eb0c848b50f0d4c4451404.png" width="1803" height="1297" class="img_ev3q"></p><div align="center">Fig. 1: Computing &amp; storage strategies in between Landing and Raw zones</div><br><div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>There are limitations to historizing all received data, notably storage costs: this is where the data’s lifecycle must be discussed. Keeping all older versions of a single dataset may start to become expensive as the data piles up, outweighing the pros if only the latest values are used.</p><p>In this case, a solution can be to keep only the latest values of the source’s data points, or just one version of the source every N periods on top of the latest ones. We can then archive older and unused versions in cold, less costly storage resources. The archived data enters a different lifecycle, where it could be removed at some point in the future if it doesn’t serve any business purpose.</p></div></div><p>The “historize everything” strategy ensures RAW storage is the most useful it can be: use case agnostic, your businesses can use and explore the data at its full potential. It also helps unify the way RAW storage is meant to be read by your later processes, improving usability while allowing for costs optimization without sacrificing the underlying principle.</p></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="going-from-raw-input-to-refined-output-computing-strategy">Going from raw input to refined output: computing strategy<a href="https://ekimetrics.github.io/blog/2023/02/28/building_datalake_part_2#going-from-raw-input-to-refined-output-computing-strategy" class="hash-link" aria-label="Direct link to Going from raw input to refined output: computing strategy" title="Direct link to Going from raw input to refined output: computing strategy">​</a></h3>
<div align="justify"><p>By applying this highly inclusive storage strategy for our RAW storage, we’re then able to fetch the necessary data for our business cases, whether it’s using the full history of a source, its latest version or the latest changes only.</p><p>The most common computing strategy will be to generate an up-to-date view of the data: at the time of computing, what are the insights’ state ? We’re also able to use older data to track changes and generate insights from these.</p><p>Refined data can then be exposed in two ways, acting as the source for another system:</p><ul>
<li>Exposing the full dataset, where all data is up-to-date</li>
<li>Exposing incremental changes, where only what is new or updated is sent to the serving layer</li>
</ul><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/img4_computing_2-dcc7a76dc859f9bae73bb079498fd3eb.png" width="1803" height="1561" class="img_ev3q"></p><div align="center">Fig. 2: computing and storage strategies in between Raw, Trusted and Refined zones</div><br><p>For some use cases, you can generate “frozen in time” views of the data, only updating the current timeframe’s view. For instance, we could update the current month’s exposed insights each day, then stop updating it at the end of the month, writing a new one for the next month’s computed insights, and so on.</p><div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>Historizing our transformations and results is useful for debugging and business exploration, but iterations may start to pile up, so defining a lifecycle for Trusted (TRD) and Refined (RFD) storage is important as well: do our technical processes or business use cases need all the iterations history stored in trusted TRD ? Can our business use cases work with a simplification of RFD storage, similar to the Delta historization for RAW zone in figure 1 ?</p></div></div><p>You may have noticed, our exposed Refined storage looks strangely similar to what we had in the data source: we’re exposing either the full up-to-date version of our insights, or just updates and new ones, just like our sources. The datalake is now a source for your business’ use cases, so it makes sense that it would be able to expose data in a similar way.</p><p>The “up-to-date view” strategy is highly useful for most use cases, and through smart use of historization, still allows for your businesses to get insights on the data’s evolution through time. It also ensures versatility in making your datalake a usable source for other systems when exposing data.</p><p>From there, the serving layer can leverage this source in a wide range of solutions, be it a database, reporting, CRM, AI models, etc.</p></div>
<br>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="https://ekimetrics.github.io/blog/2023/02/28/building_datalake_part_2#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<div align="justify"><p>In essence, storage &amp; compute strategies can be sumarized in three questions:</p><ul>
<li>Why are these strategies necessary: to ensure versatility for businesses and technical processes, all the while improving usability.</li>
<li>How do they do that: by capturing and organizing data’s history efficiently, unifying the way we look at data while allowing for versatility in its usage.</li>
<li>What they mean: a unified, highly useful and usable storage, and computing that help the datalake become the source for a wide range of systems and use cases.</li>
</ul><p>As data engineers and architects, we’re always looking for ways to improve our data products. In the context of building a datalake, this means finding ways to make data easy to find, explain and extract insights from. The strategies we’ve talked are key answers to these challenges ; as we use them to grow data platforms and apply them to new use cases, these storage &amp; compute strategies have proved themselves to be highly useful for other data challenges, providing new capabilities and solutions for our clients.</p></div>]]></content:encoded>
            <category>Datalake</category>
            <category>Data Engineering</category>
            <category>Architecture</category>
            <category>Data Governance</category>
            <category>Data Mesh</category>
        </item>
        <item>
            <title><![CDATA[Exploring the links between creative execution and marketing effectiveness - Part V: Key Paths to Success and Common Pitfalls to Avoid]]></title>
            <link>https://ekimetrics.github.io/blog/2023/02/21/creative_execution_and_marketing_effectiveness_part_V</link>
            <guid>https://ekimetrics.github.io/blog/2023/02/21/creative_execution_and_marketing_effectiveness_part_V</guid>
            <pubDate>Tue, 21 Feb 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[ In this last part, we outline the key learnings from this project, including key paths to success and common pitfalls to avoid.]]></description>
            <content:encoded><![CDATA[<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Eki_Meta_part_V-cb6cceedb9c505b6b88b6f86ad0ca88e.png" width="3041" height="4055" class="img_ev3q"></p></div>
<div align="justify"><p>This article is the last part of a set of five technical articles that accompany a <a href="https://ekimetrics.com/news-and-events/exploring-the-links-between-creative-execution-and-marketing-effectiveness-exclusivepreview" target="_blank" rel="noopener noreferrer">whitepaper</a> written in collaboration between Meta and Ekimetrics. Object Detection (OD) and Optical Character Recognition (OCR) were used to detect specific features in creative images, such as faces, smiles, text, brand logos, etc. Then, in combination with impressions data, marketing mix models were used to investigate what objects, or combinations of objects in creative images in marketing campaigns, drive higher ROIs.
In this last part we share with the reader common pitfalls to avoid when utilising tools for Object Detection, Optical Character Recognition and MMM, as well as things to keep in mind, that will facilitate the process.</p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="common-pitfalls-to-avoid">Common Pitfalls to Avoid<a href="https://ekimetrics.github.io/blog/2023/02/21/creative_execution_and_marketing_effectiveness_part_V#common-pitfalls-to-avoid" class="hash-link" aria-label="Direct link to Common Pitfalls to Avoid" title="Direct link to Common Pitfalls to Avoid">​</a></h2>
<div align="justify"><ul>
<li>
<p><strong>Failing to Define Labels at the Start:</strong> Define the set of labels that are going to be studied at the very beginning. Adding more labels in the middle of the study would involve having to go back to time consuming tasks, such as manual labelling and repeated extraction and processing of labels.</p>
</li>
<li>
<p><strong>Ambiguous Object Definitions:</strong> Describe clearly from the start what each label is (e.g., “Person” can be any body part, not just a whole body with a face). This is helpful when the manual labelling is being done as a team, rather than by one person. Furthermore, if using a pre-trained model, ensure that your definition of the object aligns with what is detected by the model. For example, you may define “Car” as just the outside of the car while the OD model is trained to detect both the interior and exterior of cars.</p>
</li>
<li>
<p><strong>Non-Generalizable Labels:</strong> If you are studying two separate sub-brands within one brand, it is advisable to have two separate studies, rather than one. That is, instead of defining objects “Brand A Logo” and “Brand B Logo”, it may be better to separate the brands into different streams and have the same object labels for both (e.g., logo, brand cue, product and person). This will ensure that you code is reusable for studies of brands that have different numbers of sub-brands.</p>
</li>
<li>
<p><strong>Lazy Manual Labelling:</strong> Make sure to manually label all objects in a creative. For example, if there are three cars, label all of them, not just one. The manually labelled validation set is the ground truth against which a model’s performance is compared. If some objects are missed, you may have strange performance results that indicate that the model may be over-detecting objects.</p>
</li>
<li>
<p><strong>Trying to be Exhaustive:</strong> Avoid testing all open-source resources available, as this can be very time-consuming and not very fruitful. Choose two or three to test and instead spend more time on what you can do to improve their performance on your particular dataset. This could, for example, be done through hyperparameter tuning (e.g., testing different learning rates, batch sizes, confidence thresholds, etc.) or in the processing of the results (e.g., correcting any text labels inside logos or products).</p>
</li>
<li>
<p><strong>Lack of Checkpoints:</strong> Due to the many different data manipulation steps in this project, there are a lot of potential sources for errors. The key is to set up automatic checks at each stage to avoid a trickle down effect of avoidable errors. For example, removing false positives in face detection by only ‘accepting’ a detected face if a person was also detected in the creative will ensure that the feature time series that is used for MMM does not suddenly have more impressions for faces than for people. Similarly, when doing the feature engineering, employing a simple method of checking that there are no negative values, no missing data, and that the impressions and spend data is consistent across each sub-model will ensure that time is not wasted in the MMM stage from modelling with incorrect data.</p>
</li>
</ul><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="key-paths-to-success">Key Paths to Success<a href="https://ekimetrics.github.io/blog/2023/02/21/creative_execution_and_marketing_effectiveness_part_V#key-paths-to-success" class="hash-link" aria-label="Direct link to Key Paths to Success" title="Direct link to Key Paths to Success">​</a></h2><p>There are various components that can contribute to successfully executing this type of study, ranging from technical requirements to general strategies. Some of the key components are outlined below.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="integrated-efficient-tools">Integrated, Efficient Tools<a href="https://ekimetrics.github.io/blog/2023/02/21/creative_execution_and_marketing_effectiveness_part_V#integrated-efficient-tools" class="hash-link" aria-label="Direct link to Integrated, Efficient Tools" title="Direct link to Integrated, Efficient Tools">​</a></h3><p>Choosing the right tools can make or break a project of this complexity. Choose a platform which offers robust functionalities to help streamline and facilitate work. This study used Azure, but GCP is another great alternative. The key features that are required are:</p><ul>
<li><strong>Cloud storage:</strong> Due to the volume of creatives included in this study, cloud storage was crucial. Particularly if extracting frames from videos, it is important to account for the significant volume of additional images that need to be stored. This project used Azure Storage.</li>
<li><strong>Labelling Software:</strong> The labelling process is very manual and time-consuming. Having an intuitive software that can import creatives directly from the cloud storage saves a lot of time and effort and avoids unnecessary duplication of images. Furthermore, the software should be able to export the labels as a JSON file which can then later be converted to the required format (e.g., COCO for Detectron2). For this project, the Azure Machine Learning Studio Data Labelling functionality was used.</li>
<li><strong>External Computation Resources:</strong> As the training, validation and final labelling of images are all computationally expensive processes, the use of external computation resources (clusters) is recommended. The configuration of the clusters can vary depending on the task at hand. For pre-processing and feature engineering, individual CPU-enabled single-node clusters are sufficient. On the other hand, for the training, validation, and labelling processes, it is recommended to use GPU-enabled clusters. While GPUs are a more expensive resources than CPUs, the efficiency gains may make up for the additional cost per hour. For this project, Databricks was used as it can connect to Azure storage, facilitates the use of clusters, supports various programming languages, and allows for collaboration on Notebooks.</li>
</ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="contextual-knowledge">Contextual Knowledge<a href="https://ekimetrics.github.io/blog/2023/02/21/creative_execution_and_marketing_effectiveness_part_V#contextual-knowledge" class="hash-link" aria-label="Direct link to Contextual Knowledge" title="Direct link to Contextual Knowledge">​</a></h3><p>Having a strong understanding of the brand acts as a crucial foundation for every step of this project. It is not only required for making informed decisions regarding which objects should be detected but also vital for defining the features to be measured in the MMM. For example, knowing that products often appear in creatives alongside just a hand raises the question of whether this is the most effective use of a person, or if including the person’s face would be more effective; this in turn leads to the creation of features testing products alongside ‘face-less’ people vs. people with faces. Contextual knowledge can also be gained throughout the project by stopping to analyse the data. For example, checking the distributions of manually labelled objects can give an early indication of performance of custom-trained models (feasibility for successful training and detection) as well as the expected impact for regression models.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="strong-base-models">Strong Base Models<a href="https://ekimetrics.github.io/blog/2023/02/21/creative_execution_and_marketing_effectiveness_part_V#strong-base-models" class="hash-link" aria-label="Direct link to Strong Base Models" title="Direct link to Strong Base Models">​</a></h3><p>Having strong base models is also key for success in this project. Since the target variable of the sub-models is determined by the contribution of the Meta variables in the base model, a poor base model will directly impact the performance of the sub-model. The quality of the base model will largely depend on the dataset used, so ensuring that sufficient, relevant, and good quality data relating to the baseline, market variations, and marketing activity is crucial.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="programmatic-sub-modelling">Programmatic Sub-Modelling<a href="https://ekimetrics.github.io/blog/2023/02/21/creative_execution_and_marketing_effectiveness_part_V#programmatic-sub-modelling" class="hash-link" aria-label="Direct link to Programmatic Sub-Modelling" title="Direct link to Programmatic Sub-Modelling">​</a></h3><p>Depending on the number of feature groups, KPIs and sub-brands included in the study, it may be infeasible to run the sub-models on a one-by-one basis. For context, this project had 156 sub-models (13 base models x 12 feature groups). For that reason, it is recommended to create a methodology that allows for the programmatic creation of sub-models.</p></div>]]></content:encoded>
            <category>Object Detection</category>
            <category>Optical Character Recognition</category>
            <category>Marketing Mix Modelling</category>
            <category>Deep Learning</category>
            <category>Tesseract</category>
        </item>
        <item>
            <title><![CDATA[Exploring the links between creative execution and marketing effectiveness - Part IV: MMM for Creative Marketing Effectiveness]]></title>
            <link>https://ekimetrics.github.io/blog/2023/01/15/creative_execution_and_marketing_effectiveness_part_IV</link>
            <guid>https://ekimetrics.github.io/blog/2023/01/15/creative_execution_and_marketing_effectiveness_part_IV</guid>
            <pubDate>Sun, 15 Jan 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[In this Part IV we explore the MMM methodology applied.]]></description>
            <content:encoded><![CDATA[<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Eki_Meta_part_IV-3e729e3e394461861d5daf47f49da2cc.png" width="5906" height="3938" class="img_ev3q"></p></div>
<div align="justify"><p>This article is <strong>Part IV</strong> of a set of five technical articles that accompany a <a href="https://ekimetrics.com/news-and-events/exploring-the-links-between-creative-execution-and-marketing-effectiveness-exclusivepreview" target="_blank" rel="noopener noreferrer">whitepaper</a> written in collaboration between Meta and Ekimetrics. Object Detection (OD) and Optical Character Recognition (OCR) were used to detect specific features in creative images, such as faces, smiles, text, brand logos, etc. Then, in combination with impressions data, marketing mix models were used to investigate what objects, or combinations of objects in creative images in marketing campaigns, drive higher ROIs.
In this Part IV we explore the MMM methodology applied.</p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="why-you-should-read-this">Why you should read this<a href="https://ekimetrics.github.io/blog/2023/01/15/creative_execution_and_marketing_effectiveness_part_IV#why-you-should-read-this" class="hash-link" aria-label="Direct link to Why you should read this" title="Direct link to Why you should read this">​</a></h2>
<div align="justify"><p>This article is mostly directed to marketing measurers. Here you will learn that to improve accuracy in your measurement you must treat creatives differently. We provide the MMM methodlogy to reproduce this type of analysis for your brands.</p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="mmm-process">MMM Process<a href="https://ekimetrics.github.io/blog/2023/01/15/creative_execution_and_marketing_effectiveness_part_IV#mmm-process" class="hash-link" aria-label="Direct link to MMM Process" title="Direct link to MMM Process">​</a></h2>
<div align="justify"><p>An optimizsed two-staged modelling approach was applied to existing MMM models developed by Ekimetrics for a range of brands, products and KPIs. First, the base models were optimizsed, and second, sub-models were created to explain the variation in Meta contribution using the creative features. Bayesian optimization was used in both steps for variable transformation in relation to lag, adstock and saturation.</p></div>
<p>&nbsp;</p>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/1-422062c25769c3e19657d659ef85db70.png" width="602" height="161" class="img_ev3q"></p><p>Modelling Workflow</p></div>
<br>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="mmm-model">MMM Model<a href="https://ekimetrics.github.io/blog/2023/01/15/creative_execution_and_marketing_effectiveness_part_IV#mmm-model" class="hash-link" aria-label="Direct link to MMM Model" title="Direct link to MMM Model">​</a></h2>
<div align="justify"><p>The MMM models used in this analysis are multilinear regressions accounting for all significant factors driving the KPI (most commonly sales), including digital social. The selection of these models was based on the criteria of model stability (high R2, no autocorrelation, controlled heteroskedasticity) and enough variation in social activity (spend and impressions threshold of 5% per object). Some of the modelling periods were cut to account for sparse data, low correlation between creative impressions and modelled impressions or low variation. Therefore, a trusted relationship from social to the KPI of each MMM was established but could be enhanced by choosing more accurate parameterization of Meta activity through MMM model optimisation. Table 1 details the models included in the study.</p></div>
<p>&nbsp;</p>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/2-de515ac3a725e961be84cc9fdd00e0e6.png" width="820" height="520" class="img_ev3q"></p><p>Table 1: Specifications of MMM models selected Detail of Models selected</p></div>
<br>
<div align="justify"><p>Once the MMM models were optimized, we isolated the impact of Meta on the KPI according to the following relationship:
</p><p>&nbsp;</p><p></p><div align="center"><p><em><strong>Meta Contribution to KPI = Coefficient of Meta Variable ×Transformed Meta Variable</strong></em></p></div><p>The Meta contribution became the dependent variable of the Sub-Model, along with the constant and residuals. The object detection impressions timeseries served as the explanatory variables. The objective of this was to understand the impact of different feature splits into the overall Meta performance, allowing room for movement through the constant and residuals.</p><p>A sub-modelling approach allowed for the object detection features to have indirect effects on the KPI, through the Meta variable. Bayesian Optimisation Methods were employed to find the optimal transformations for each variable.</p><p>Each Sub-model is a linear regression, testing the impact of the appearance of a feature, or feature group, and its opposite (partner feature) against the total Meta performance. Since the features are not mutually exclusive (between 48%-66% of creatives contained at least two objects), they could not all be tested in the same model. Furthermore, the daily impressions and spend per feature were split according to creative type. That is, the impressions associated with a feature or combination of features, e.g., Person &amp; Product, were split out by static, video and carousel. While it was not in the scope of the study, doing this allowed for further analysis into the impact of features by creative type.</p><p>A sequential approach was followed in which each feature group was tested in isolation. Within each sub-model, the transformations of the features (lag, adstock, and diminishing returns) which maximized R2, and minimized p-values were chosen. The transformations that the Meta features follow were bounded to the following ranges: lag of 0-3, adstock of 0-50, saturation (K; S) of 0.1-0.8; 1-9.</p><p>An example of sub-model is shown below.</p><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/9-8150076ab761a3040c0b9e6c5abb159a.png" width="1105" height="28" class="img_ev3q"></p><p>Where:</p><div align="left"><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/10-de15f137482874e0d093b56bdaf5a4d4.png" width="732" height="28" class="img_ev3q"></p></div><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/11-9637738e56b4324a656393a934af1b63.png" width="810" height="27" class="img_ev3q"></p><p>Same for Video (V) and Carousel (C)</p><p>One major challenge with the approach of testing the appearance of a feature or a set of features against its partner feature (e.g., impressions of creatives where a Brand Cue appears vs impressions of creatives where a Brand Cue does not appear), is that the partner feature is indirectly testing the appearance of other features or no features at all.
To manage the risk of results inaccuracy we made sure that:</p></div>
<ol>
<li>Both the main feature and the partner feature in the sub-models had to be statistically significant at a 10% level. For example, if the appearance of a feature is significant but the partner is not, the results of that regression were not used.</li>
<li>The percent of Impressions of the feature being tested had to be &gt; 5%</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="roi-calculations">ROI Calculations<a href="https://ekimetrics.github.io/blog/2023/01/15/creative_execution_and_marketing_effectiveness_part_IV#roi-calculations" class="hash-link" aria-label="Direct link to ROI Calculations" title="Direct link to ROI Calculations">​</a></h2>
<div align="justify"><p>The last step of the analysis centred around calculating ROIs and deriving ROI uplifts for each feature per brand, using the feature contributions from the sub-models and the associated spend. This involved the following steps:</p></div>
<ol>
<li>Calculate the ROI of each feature (and partnering feature/s) using the contribution and the spend associated with that feature. Because the features were modelled by creative type (static, carousel, and video), the results were weighted by the percent of impressions each type represented over the total of the feature.</li>
</ol>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/3-a399c32b962f0021e561541ca80b6276.png" width="602" height="103" class="img_ev3q"></p></div>
<p>&nbsp;</p>
<ol start="2">
<li>Calculate the ROI of the partnering variable (e.g., No Product)</li>
<li>Calculate the Uplift:</li>
</ol>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/4-6b729aff7daaed123d066206e7474d43.png" width="602" height="53" class="img_ev3q"></p></div>
<p>&nbsp;</p>
<ol start="4">
<li>Index and rank ROI uplifts
a.	ROI uplifts are standardized, so they are comparable across brands
b.	They are then ranked so that #1 is the feature with the greatest uplift (maximum difference between the feature appearing and not appearing).</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="results">Results<a href="https://ekimetrics.github.io/blog/2023/01/15/creative_execution_and_marketing_effectiveness_part_IV#results" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results">​</a></h2>
<div align="justify"><p>The results can be found in the   <a href="https://ekimetrics.com/news-and-events/exploring-the-links-between-creative-execution-and-marketing-effectiveness-exclusivepreview" target="_blank" rel="noopener noreferrer">whitepaper.</a></p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="next-article">Next article<a href="https://ekimetrics.github.io/blog/2023/01/15/creative_execution_and_marketing_effectiveness_part_IV#next-article" class="hash-link" aria-label="Direct link to Next article" title="Direct link to Next article">​</a></h2>
<p>In the next article, we outline the key learnings from this project, including key paths to success and common pitfalls to avoid.</p>]]></content:encoded>
            <category>Object Detection</category>
            <category>Optical Character Recognition</category>
            <category>Marketing Mix Modelling</category>
            <category>Deep Learning</category>
            <category>Tesseract</category>
        </item>
        <item>
            <title><![CDATA[Exploring the links between creative execution and marketing effectiveness - Part III: Tesseract Pre-Trained Optical Character Recognition Models]]></title>
            <link>https://ekimetrics.github.io/blog/2022/12/13/creative_execution_and_marketing_effectiveness_part_III</link>
            <guid>https://ekimetrics.github.io/blog/2022/12/13/creative_execution_and_marketing_effectiveness_part_III</guid>
            <pubDate>Tue, 13 Dec 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[In this Part III we explore the methodology for using Tesseract to detect text in creative images.]]></description>
            <content:encoded><![CDATA[<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app " src="https://ekimetrics.github.io/assets/images/Eki_Meta_part_III-7d188e074342397f2ddb6fb21a645ae7.png" width="4095" height="2726" class="img_ev3q"></p></div>
<div align="justify"><p>This article is <strong>Part III</strong> of a set of five technical articles that accompany a <a href="https://ekimetrics.com/news-and-events/exploring-the-links-between-creative-execution-and-marketing-effectiveness-exclusivepreview" target="_blank" rel="noopener noreferrer">whitepaper</a> written in collaboration between Meta and Ekimetrics. Object Detection (OD) and Optical Character Recognition (OCR) were used to detect specific features in creative images, such as faces, smiles, text, brand logos, etc. Then, in combination with impressions data, marketing mix models were used to investigate what objects, or combinations of objects in creative images in marketing campaigns, drive higher ROIs.
In this Part III we explore the methodology for using Tesseract to detect text in creative images.</p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="detecting-text-with-tesseract">Detecting Text with Tesseract<a href="https://ekimetrics.github.io/blog/2022/12/13/creative_execution_and_marketing_effectiveness_part_III#detecting-text-with-tesseract" class="hash-link" aria-label="Direct link to Detecting Text with Tesseract" title="Direct link to Detecting Text with Tesseract">​</a></h2>
<div align="justify"><p>Tesseract is an open-source optical character recognition (OCR) Engine that allows for the recognition of text characters within a digital image. It is an open-source resource, originally developed by Hewlett-Packard, and now managed by Google. This package does not have any parameters to optimise, but, as is exposed here, the basic performance of this resource can be improved with a combination of image processing and detected text correction.</p><p>In this work, Tesseract was used to detect text in all images, in a process outlined in Figure 1. Performance of this detection tool was done using Confusion Matrices, to gain insight not only into the Accuracy, but also other metrics such as the True Positive and True Negative Rates.</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/10-dc93d16eaa104bf23fe228afaf7bd6a2.PNG" width="673" height="265" class="img_ev3q"></p><p>Figure 1 - Three stage process by which the basic performance (accuracy) of Tesseract on the original images was improved by up to 28% points.</p></div>
<br>
<div align="justify"><p>To start with, the functionality was used on the original images to measure a baseline for performance on our sets of images. From this first step, we derived the following learnings:</p><ul>
<li>For general Text (both non-promotional Text and Promotional Text), the Accuracy is 69%, the True Positive Rate is around 65%, the False Positive Rate is about 35%.</li>
<li>Tesseract does not recognise symbols such as % (which may indicate promotional text) accurately.</li>
<li>Tesseract does not perform well on images that have a busy background.</li>
<li>Tesseract does not recognise slanted Text.</li>
</ul><p>Based on the low total volume of Promotional Text in our dataset, as well as the poor performance of Tesseract to detect Promotional Text, the decision was made to remove Promotional Text as one of the desired objects to study.</p><p>Building on those learnings, we implemented a pipeline, outlined in Figure 2, that would help us improve the performance from baseline, by up to 28 percentage points on Accuracy :</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/11-435bc20c4e2284716dcd7b9565a42098.PNG" width="535" height="308" class="img_ev3q"></p><p>Figure 2 - Pipeline for pre-processing images before Optical Character Recognition Models, and correcting detected text.</p></div>
<br>
<div align="justify"><p>For Step 1, pre-processing methods included the following, as well as combinations of those (illustrated in Figure 3): <strong>Binary Threshold, Sharpen, Normalisation, Histogram Equalisation, Histogram Adaptive Equalisation</strong>. As can be imagined, a processing method that enhances the performance of the detection method for one image, might decrease it for different images. Therefore, to ensure we obtained the highest accuracy for all images, we applied all pre-processing methods to all images, running Tesseract on each modified image, and keeping track of the text detected in each iteration.</p></div>
<div align="center"><p><img decoding="async" loading="lazy" alt="screenshot-app" src="https://ekimetrics.github.io/assets/images/12-bdf7d0b4655f39b30eaa56e906ef8e6d.PNG" width="840" height="414" class="img_ev3q"></p><p>Figure 3 - Effect of various image processing methods on the original. Images for illustrative purposes. Original images sourced from <a href="https://unsplash.com/s/photos/coca-cola" target="_top">unsplash.com</a></p></div>
<br>
<div align="justify"><p>In Step 2 we use Tesseract to detect text in the pre-processed images. Step 3 consisted of removing any “incorrect” text, such as text inside the bounding boxes of Logo or Product objects, if the text was shorter than a threshold length (which depended on the brand), if it consisted of keywords, such as the brand name, or if it had ASCII characters.</p><p>With the current methodology (figure 2), the accuracy cannot be improved any further by changing the length of text that is considered to be “true” Text. Furthermore, there is a trade-off between the True Negative Rate and the True Positive Rate, which helped us choose a threshold for the length of string to accept: Since Accuracy does not improve further after length 4, but this value does optimise the other two metrics, 4 characters long is the threshold chosen.</p></div>
<br>
<div align="justify"><p>The performance of Tesseract may be improved further by adding another step to the pipeline in figure 2, where the image is rotated several degrees, and after each rotation, text is detected. Due to time constraint issues, and the fact that we had already achieved a significant improvement above baseline and reached an accuracy of high 90s for some of the brands, we did not implement this step.</p></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="code-snippets">Code snippets<a href="https://ekimetrics.github.io/blog/2022/12/13/creative_execution_and_marketing_effectiveness_part_III#code-snippets" class="hash-link" aria-label="Direct link to Code snippets" title="Direct link to Code snippets">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="install-required-libraries">Install required libraries<a href="https://ekimetrics.github.io/blog/2022/12/13/creative_execution_and_marketing_effectiveness_part_III#install-required-libraries" class="hash-link" aria-label="Direct link to Install required libraries" title="Direct link to Install required libraries">​</a></h3>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain"># To install tesseract</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># !sh apt-get -f -y install tesseract-ocr </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">!sudo apt-get install tesseract-ocr -y</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># To install pytesseract</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">!pip3 install pytesseract</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="import-required-libraries">Import required libraries<a href="https://ekimetrics.github.io/blog/2022/12/13/creative_execution_and_marketing_effectiveness_part_III#import-required-libraries" class="hash-link" aria-label="Direct link to Import required libraries" title="Direct link to Import required libraries">​</a></h3>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">import pytesseract</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from sklearn.metrics import confusion_matrix, accuracy_score</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">import cv2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from PIL import Image</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="processing-images">Processing images<a href="https://ekimetrics.github.io/blog/2022/12/13/creative_execution_and_marketing_effectiveness_part_III#processing-images" class="hash-link" aria-label="Direct link to Processing images" title="Direct link to Processing images">​</a></h3>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def normalise_img(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # normalise original and detect</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    norm_img = np.zeros((image.shape[0], image.shape[1]))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    normalised = cv2.normalize(image, norm_img, 0, 255, cv2.NORM_MINMAX)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    normalised = cv2.threshold(normalised, 100, 255, cv2.THRESH_BINARY)[1]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    normalised = cv2.GaussianBlur(normalised, (1, 1), 0)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return normalised</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def threshold_image(image, threshold=200):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    image = cv2.threshold(image, threshold, 255, cv2.THRESH_BINARY)[1]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return image</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def remove_noise_and_smooth(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # convert to grayscale</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # blur</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    blur = cv2.GaussianBlur(gray, (0,0), sigmaX=33, sigmaY=33)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # divide</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    divide = cv2.divide(gray, blur, scale=255)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # otsu threshold</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    thresh = cv2.threshold(divide, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # apply morphology</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    morph = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return morph</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def im_to_gray(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # convert to grayscale </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return gray</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def create_binary(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # create binary - converting a colored image (RGB) into a black and white image - Adaptive binarization works based on the features of neighboring pixels (i.e) local window.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    binary = cv2.threshold(image ,130,255,cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return binary</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def hist_equaliser(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Image Contrast and Sharpness: histogram equalization</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    equalised = cv2.equalizeHist(gray)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return equalised</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def adaptive_hist_equaliser(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Image Contrast and Sharpness: adaptive histogram equalization</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    equalised = clahe.apply(gray)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return equalised</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def invert_image():</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # invert the image - Color Inversion when different regions have different Foreground and Background colors</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    inverted = cv2.bitwise_not(image)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return inverted</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def im_blur(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Blurring or Smoothing </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    averageBlur = cv2.blur(image, (5, 5))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def im_gaussianBlur(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Blurring or Smoothing (Gaussian Blur)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    gaussian = cv2.GaussianBlur(image, (3, 3), 0)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def im_medianBlur(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Blurring or Smoothing (Median Blur (good at removing salt and pepper noises))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    medianBlur = cv2.medianBlur(image, 9)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def im_bilateralBlur(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Blurring or Smoothing (Bilateral Filtering)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    bilateral = cv2.bilateralFilter(image, 9, 75, 75)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return image</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def unsharp_mask(image, kernel_size=(5, 5), sigma=1.0, amount=1.0, threshold=0):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    """Return a sharpened version of the image, using an unsharp mask."""</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    blurred = cv2.GaussianBlur(image, kernel_size, sigma)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sharpened = float(amount + 1) * image - float(amount) * blurred</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sharpened = np.maximum(sharpened, np.zeros(sharpened.shape))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sharpened = np.minimum(sharpened, 255 * np.ones(sharpened.shape))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sharpened = sharpened.round().astype(np.uint8)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    if threshold &gt; 0:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        low_contrast_mask = np.absolute(image - blurred) &lt; threshold</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        np.copyto(sharpened, image, where=low_contrast_mask)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return sharpened</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def apply_filter_sharpen(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    image = cv2.filter2D(image, -1, kernel)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return image</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def im_erode_dilate(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # apply noise reduction techniques like eroding, dilating</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kernel = np.ones((2,2),np.uint8)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    image = cv2.erode(image, kernel, iterations = 1)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    image = cv2.dilate(image, kernel, iterations = 1)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return image</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def process_image(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    """</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    Example of a pipeline to pre-process an image.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    """</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # convert to grayscale </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     # create binary</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    image = cv2.threshold(image ,130,255,cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # invert the image</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    image = cv2.bitwise_not(image)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # apply noise reduction techniques like eroding, dilating</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kernel = np.ones((2,2),np.uint8)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    image = cv2.erode(image, kernel, iterations = 1)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    image = cv2.dilate(image, kernel, iterations = 1)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return image</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="detect-text">Detect text<a href="https://ekimetrics.github.io/blog/2022/12/13/creative_execution_and_marketing_effectiveness_part_III#detect-text" class="hash-link" aria-label="Direct link to Detect text" title="Direct link to Detect text">​</a></h3>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def detect_text(image, brand_name, t, bboxes_file, verbose=True):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    bboxes = get_bbox_pts(data)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data_out, bboxess, tt = check_text_ok(data, brand_name)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return data, bboxes</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="manipulate-bounding-boxes">Manipulate bounding boxes<a href="https://ekimetrics.github.io/blog/2022/12/13/creative_execution_and_marketing_effectiveness_part_III#manipulate-bounding-boxes" class="hash-link" aria-label="Direct link to Manipulate bounding boxes" title="Direct link to Manipulate bounding boxes">​</a></h3>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def get_bbox_pts(data):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    """</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    From the output of the text detection model, extract the [x1,x2,y1,y2] points.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    """</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data['x1'] = []</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data['x2'] = []</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data['y1'] = []</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data['y2'] = []</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    for i, txt in enumerate(data['text']):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        # extract the width, height, top and left position for that detected word</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        w = data["width"][i]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        h = data["height"][i]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        l = data["left"][i]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        t = data["top"][i]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        # define all the surrounding box points</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        x1 = l</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        x2 = l + w</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        y1 = t</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        y2 = t + h</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        data['x1'].append(x1)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        data['x2'].append(x2)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        data['y1'].append(y1)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        data['y2'].append(y2)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return data</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def transform_bbox(bbox):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    """</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    Transform Azure labels of valid set from [x,y,w,h] (in absolute numbers, prev. transformed) to [x,y,x1,y1].</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    """</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    x = bbox[0]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    y = bbox[1]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    w = bbox[2]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    h = bbox[3]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    x1 = x + w</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    y1 = y + h</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return [x,y,x1,y1]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="useful-links">Useful Links<a href="https://ekimetrics.github.io/blog/2022/12/13/creative_execution_and_marketing_effectiveness_part_III#useful-links" class="hash-link" aria-label="Direct link to Useful Links" title="Direct link to Useful Links">​</a></h2>
<ul>
<li><a href="https://github.com/tesseract-ocr" target="_blank" rel="noopener noreferrer">Tesseract - Github</a></li>
<li><a href="https://tesseract-ocr.github.io/tessdoc/Home.html" target="_blank" rel="noopener noreferrer">Tesseract - User Manual</a></li>
<li><a href="https://pypi.org/project/pytesseract/" target="_blank" rel="noopener noreferrer">Pytesseract - Pypi</a></li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="next-article">Next article<a href="https://ekimetrics.github.io/blog/2022/12/13/creative_execution_and_marketing_effectiveness_part_III#next-article" class="hash-link" aria-label="Direct link to Next article" title="Direct link to Next article">​</a></h2>
<p>In the next article, we outline the MMM process, focusing on how the creative elements are used as variables in a multivariate regression, as well as how the final ROIs were calculated.</p>]]></content:encoded>
            <category>Object Detection</category>
            <category>Optical Character Recognition</category>
            <category>Marketing Mix Modelling</category>
            <category>Deep Learning</category>
            <category>Tesseract</category>
        </item>
    </channel>
</rss>