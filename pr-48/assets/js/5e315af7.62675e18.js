"use strict";(self.webpackChunkeki_lab=self.webpackChunkeki_lab||[]).push([[2342],{20069:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>m,frontMatter:()=>l,metadata:()=>n,toc:()=>c});var n=i(91578),a=i(74848),r=i(28453);const l={title:"Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (2/2)",author:"Milan Bhan",author_title:"Senior Data Science Consultant, PhD student",author_url:"mailto:inno@ekimetrics.com",header_image_url:"./img/blog/interpretability_articles_2.jpg",tags:["NLP","Transformers","BERT","interpretability","explainability","XAI","attention"],draft:!1,description:"Two illustrations of how attention coefficients can be a source of interpretability.",keywords:["Data Science","EkiLab","Ekimetrics","Eki.Lab","Eki","Machine Learning","Artificial Intelligence","Data Science for business","Operational Research","Optimization","Knapsack problem","Deep Reinforcement Learning"]},o=void 0,s={authorsImageUrls:[void 0]},c=[];function p(e){return(0,a.jsx)(a.Fragment,{})}function m(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(p,{...e})}):p()}},28453:(e,t,i)=>{i.d(t,{R:()=>l,x:()=>o});var n=i(96540);const a={},r=n.createContext(a);function l(e){const t=n.useContext(r);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:l(e.components),n.createElement(r.Provider,{value:t},e.children)}},91578:e=>{e.exports=JSON.parse('{"permalink":"/blog/2022/10/26/Interpretability_sentiment_analysis_II","source":"@site/blog/2022-10-26-Interpretability_sentiment_analysis_II.md","title":"Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (2/2)","description":"Two illustrations of how attention coefficients can be a source of interpretability.","date":"2022-10-26T00:00:00.000Z","tags":[{"inline":true,"label":"NLP","permalink":"/blog/tags/nlp"},{"inline":true,"label":"Transformers","permalink":"/blog/tags/transformers"},{"inline":true,"label":"BERT","permalink":"/blog/tags/bert"},{"inline":true,"label":"interpretability","permalink":"/blog/tags/interpretability"},{"inline":true,"label":"explainability","permalink":"/blog/tags/explainability"},{"inline":true,"label":"XAI","permalink":"/blog/tags/xai"},{"inline":true,"label":"attention","permalink":"/blog/tags/attention"}],"readingTime":6.17,"hasTruncateMarker":true,"authors":[{"name":"Milan Bhan","title":"Senior Data Science Consultant, PhD student","url":"mailto:inno@ekimetrics.com","key":null,"page":null}],"frontMatter":{"title":"Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (2/2)","author":"Milan Bhan","author_title":"Senior Data Science Consultant, PhD student","author_url":"mailto:inno@ekimetrics.com","header_image_url":"./img/blog/interpretability_articles_2.jpg","tags":["NLP","Transformers","BERT","interpretability","explainability","XAI","attention"],"draft":false,"description":"Two illustrations of how attention coefficients can be a source of interpretability.","keywords":["Data Science","EkiLab","Ekimetrics","Eki.Lab","Eki","Machine Learning","Artificial Intelligence","Data Science for business","Operational Research","Optimization","Knapsack problem","Deep Reinforcement Learning"]},"unlisted":false,"prevItem":{"title":"Exploring the links between creative execution and marketing effectiveness - Part I: Detectron2 Pre-Trained Object Detection Models","permalink":"/blog/2022/11/10/creative_execution_and_marketing_effectiveness_part_I"},"nextItem":{"title":"Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (1/2)","permalink":"/blog/2022/10/18/Interpretability_sentiment_analysis_I"}}')}}]);