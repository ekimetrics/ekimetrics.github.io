"use strict";(self.webpackChunkeki_lab=self.webpackChunkeki_lab||[]).push([[9161],{28453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>s});var t=i(96540);const a={},r=t.createContext(a);function o(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),t.createElement(r.Provider,{value:n},e.children)}},53485:e=>{e.exports=JSON.parse('{"permalink":"/blog/LLMs_fail","source":"@site/blog/2024-11-25-LLMs_fail.md","title":"Where LLMs still fail","description":"An exploration of current limitations in Large Language Models (LLM) through concrete examples, revealing their surprising struggles with seemingly simple tasks.","date":"2024-11-25T00:00:00.000Z","tags":[{"inline":true,"label":"Generative AI","permalink":"/blog/tags/generative-ai"},{"inline":true,"label":"LLM","permalink":"/blog/tags/llm"},{"inline":true,"label":"NLP","permalink":"/blog/tags/nlp"}],"readingTime":9.075,"hasTruncateMarker":true,"authors":[{"name":"Jean LELONG","title":"Senior Consultant","url":"https://www.linkedin.com/in/jeanlelong/","imageURL":"/img/authors/jean_lelong.jpg","key":"jean.lelong","page":null}],"frontMatter":{"slug":"LLMs_fail","title":"Where LLMs still fail","authors":["jean.lelong"],"header_image_url":"img/blog/LLMs_fail_header.png","image":"img/blog/LLMs_fail_header.png","tags":["Generative AI","LLM","NLP"],"draft":false,"description":"An exploration of current limitations in Large Language Models (LLM) through concrete examples, revealing their surprising struggles with seemingly simple tasks.","keywords":["Generative AI","LLM","NLP","Innovation","Data Science","Large Language Models (LLM)","LLM limitations","ChatGPT","o1","o1-preview","Artificial Intelligence","Misguided attention","Next-token prediction","Natural Language Processing","Language model biases","Generative AI limitations","LLM errors","Machine learning challenges","Deep learning weaknesses","GPT limitations","Natural language processing issues","Chain-of-thought prompting","Ekimetrics AI","OpenAI","LLM tokenization","AI numeric processing","Language model reasoning"]},"unlisted":false,"prevItem":{"title":"How We Created Our First Talk-to-Data (Text-to-SQL) Application in Production","permalink":"/blog/Talk_to_Data_App"},"nextItem":{"title":"Optimizing Chatbot Performance: A Two-Phase Methodology for Enhanced Performance in HR Policy Responses","permalink":"/blog/Wombat_HR"}}')},64793:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>o,metadata:()=>t,toc:()=>g});var t=i(53485),a=i(74848),r=i(28453);const o={slug:"LLMs_fail",title:"Where LLMs still fail",authors:["jean.lelong"],header_image_url:"img/blog/LLMs_fail_header.png",image:"img/blog/LLMs_fail_header.png",tags:["Generative AI","LLM","NLP"],draft:!1,description:"An exploration of current limitations in Large Language Models (LLM) through concrete examples, revealing their surprising struggles with seemingly simple tasks.",keywords:["Generative AI","LLM","NLP","Innovation","Data Science","Large Language Models (LLM)","LLM limitations","ChatGPT","o1","o1-preview","Artificial Intelligence","Misguided attention","Next-token prediction","Natural Language Processing","Language model biases","Generative AI limitations","LLM errors","Machine learning challenges","Deep learning weaknesses","GPT limitations","Natural language processing issues","Chain-of-thought prompting","Ekimetrics AI","OpenAI","LLM tokenization","AI numeric processing","Language model reasoning"]},s=void 0,l={authorsImageUrls:[void 0]},g=[];function L(e){return(0,a.jsx)(a.Fragment,{})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(L,{...e})}):L()}}}]);