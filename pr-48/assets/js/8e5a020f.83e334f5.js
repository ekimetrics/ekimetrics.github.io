"use strict";(self.webpackChunkeki_lab=self.webpackChunkeki_lab||[]).push([[7676],{26003:(e,t,i)=>{i.d(t,{A:()=>n});const n=i.p+"assets/images/Image_4-91c11a737d5e41b98a8657cd3bb17de7.jpg"},28453:(e,t,i)=>{i.d(t,{R:()=>r,x:()=>o});var n=i(96540);const a={},s=n.createContext(a);function r(e){const t=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),n.createElement(s.Provider,{value:t},e.children)}},33779:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>r,metadata:()=>n,toc:()=>c});var n=i(91578),a=i(74848),s=i(28453);const r={title:"Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (2/2)",author:"Milan Bhan",author_title:"Senior Data Science Consultant, PhD student",author_url:"mailto:inno@ekimetrics.com",header_image_url:"./img/blog/interpretability_articles_2.jpg",tags:["NLP","Transformers","BERT","interpretability","explainability","XAI","attention"],draft:!1,description:"Two illustrations of how attention coefficients can be a source of interpretability.",keywords:["Data Science","EkiLab","Ekimetrics","Eki.Lab","Eki","Machine Learning","Artificial Intelligence","Data Science for business","Operational Research","Optimization","Knapsack problem","Deep Reinforcement Learning"]},o=void 0,l={authorsImageUrls:[void 0]},c=[{value:"Summary",id:"summary",level:2},{value:"Work presented in the previous article",id:"work-presented-in-the-previous-article",level:2},{value:"Interpreting through counterfactual generation",id:"interpreting-through-counterfactual-generation",level:2},{value:"Next step",id:"next-step",level:2},{value:"References",id:"references",level:2}];function h(e){const t={a:"a",em:"em",h2:"h2",img:"img",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)("div",{align:"center",children:(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"screenshot-app ",src:i(91981).A+"",width:"5731",height:"3821"})})}),"\n",(0,a.jsx)(t.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)("div",{align:"justify",children:(0,a.jsx)(t.p,{children:"We propose to illustrate how far BERT-type models can be considered as interpretable by design. We show that the attention coefficients specific to BERT architecture constitute a particularly rich piece of information that can be used to perform interpretability. There are mainly two ways to do interpretability: attribution and generation of counterfactual examples. In a first article, we showed how attention coefficients could be the basis of an attribution interpretability method. Here we propose to evaluate how they can also be used to set up counterfactuals."})}),"\n",(0,a.jsxs)("div",{align:"justify",children:[(0,a.jsx)(t.h2,{id:"work-presented-in-the-previous-article",children:"Work presented in the previous article"}),(0,a.jsx)(t.p,{children:"Previously, the BERT [1] and DistilBERT [2] models have been mobilized to tackle the well-known problem of sentiment analysis. In particular, we have shown that the BERT and DistilBERT models contain within their architecture attention coefficients that can be at the heart of an attribution interpretability method. Starting from an initial text, a visualization of the weight assignment method was proposed. The more red the color, the higher the associated attention coefficient."}),(0,a.jsxs)("div",{align:"center",children:[(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"screenshot-app",src:i(94301).A+"",width:"769",height:"91"})}),(0,a.jsx)(t.p,{children:"Figure 1 - Attention-Based token importance"})]}),(0,a.jsx)("p",{children:"\xa0"}),(0,a.jsxs)(t.p,{children:['We saw that the word groups "',(0,a.jsx)(t.em,{children:"favorite movie"}),'", "',(0,a.jsx)(t.em,{children:"it just never gets old"}),'", "',(0,a.jsx)(t.em,{children:"performance brings tears"}),'", or "',(0,a.jsx)(t.em,{children:"it is believable and startling"}),'" stood out. This explained well why the algorithm evaluated the review as positive and what was the semantic field at the root of this prediction. This work was done using the Hugging Face transformers library [3].']}),(0,a.jsx)(t.h2,{id:"interpreting-through-counterfactual-generation",children:"Interpreting through counterfactual generation"}),(0,a.jsx)(t.p,{children:'Another way to do interpretability is to generate counterfactual examples. According to Judea Pearl, counterfactual "involves answering questions which ask what might have been, had circumstances been different\u201d [4]. Thus, the idea is to understand a prediction by generating a counterfactual example, resulting in an opposite prediction. In the context of natural language processing, it is therefore a matter of changing the right words in the review. In order to generate a counterfactual example, we propose the following methodology:'}),(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Compute the attention coefficients of the tokens in a text corpus on each attention layer (6). The text corpus size must be statistically significant"}),"\n",(0,a.jsx)(t.li,{children:"Perform token clustering based on their 6-dimensional representation"}),"\n",(0,a.jsx)(t.li,{children:"Detect clusters associated with positively and negatively charged sentiment words"}),"\n",(0,a.jsx)(t.li,{children:'Replace the tokens with the highest average attention with their "opposite token" in their "opposite cluster"\nThis approach allows us to validate the interpretative strength of the tokens put forward by the attention coefficients, while illustrating what a close review would have been with an opposite sentiment.\nWe apply the methodology on a corpus of 1000 reviews. The clustering method used is the hierarchical ascending classification (HAC) and gives 3 clusters. The obtained clusters and the counterfactual generation procedure can be represented in 2 dimensions as follows:'}),"\n"]}),(0,a.jsxs)("div",{align:"center",children:[(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"screenshot-app",src:i(38720).A+"",width:"416",height:"342"})}),(0,a.jsx)(t.p,{children:"Figure 2 - Token clusters & replacements"})]}),(0,a.jsx)(t.p,{children:"We then generate the counterfactual example of the review tested earlier by changing 2 words:"}),(0,a.jsxs)("div",{align:"center",children:[(0,a.jsx)(t.p,{children:"delight \u27a1 torment"}),(0,a.jsx)(t.p,{children:"favorite \u27a1 worst"})]}),(0,a.jsx)(t.p,{children:"This gives us the following counterfactual example:"}),(0,a.jsxs)(t.p,{children:["\u201c",(0,a.jsx)(t.em,{children:"Probably my all time worst movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring . it just never gets old despite my having seen it some 15 or more times in the last 25 years . paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a torment. the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch . and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling . if i had a dozen thumbs they\u2019d all be up for this movie"}),'".']}),(0,a.jsx)(t.p,{children:'As the text is quite long, 2 tokens are not enough to change the feeling associated with the review. The probability score nevertheless drops significantly by 0.3pts.\nOne way to assess the quality of the generated counterfactual examples is to evaluate the proportion of reviews in a corpus whose associated sentiment has changed. The result can be represented as a "counterfactual confusion matrix" as follows:'}),(0,a.jsx)(t.p,{children:'One way to assess the quality of the generated counterfactual examples is to evaluate the proportion of reviews in a corpus whose associated sentiment has changed. The result can be represented as a "counterfactual confusion matrix" as follows:'}),(0,a.jsxs)("div",{align:"center",children:[(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"screenshot-app",src:i(26003).A+"",width:"756",height:"74"})}),(0,a.jsx)(t.p,{children:"Table 1 - Counterfactual confusion matrix example"})]}),(0,a.jsx)(t.p,{children:"Where :"}),(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["X",(0,a.jsx)("sub",{children:"11"})," represents the share of reviews whose initial associated sentiment and the sentiment of the counterfactual example are positive; sentiment has remained the same"]}),"\n",(0,a.jsxs)(t.li,{children:["X",(0,a.jsx)("sub",{children:"12"})," represents the share of reviews whose sentiment changed from positive to negative; sentiment did change"]}),"\n",(0,a.jsxs)(t.li,{children:["X",(0,a.jsx)("sub",{children:"21"})," represents the share of reviews whose sentiment changed from negative to positive; sentiment changed well"]}),"\n",(0,a.jsxs)(t.li,{children:["X",(0,a.jsx)("sub",{children:"22"})," represents the share of reviews whose initial associated sentiment and the sentiment of the counterfactual example are negative; sentiment has remained the same"]}),"\n"]}),(0,a.jsx)(t.p,{children:'We compute the "counterfactual confusion matrix" on the same text corpus that enabled us to perform clustering, picking 5 tokens for each review. The result is given below:'}),(0,a.jsxs)("div",{align:"center",children:[(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"screenshot-app",src:i(62838).A+"",width:"756",height:"73"})}),(0,a.jsx)(t.p,{children:"Table 2 - Actual counterfactual confusion matrix"})]}),(0,a.jsxs)(t.p,{children:[(0,a.jsx)("p",{children:"\xa0"}),"\nThus, we see that changing the 5 tokens with the highest average attention produces a change in sentiment perception in 44% of cases. In particular, the rate of sentiment change for reviews initially perceived as positive is 31% while the rate of sentiment change for reviews initially perceived as negative is 53%. The change from negative to positive seems to be better achieved with our method."]}),(0,a.jsx)(t.p,{children:'We have shown that attention coefficients can be a source of interpretability. Used in the right way, the attention coefficients allow the detection of tokens with high predictive value. They can also be used to generate counterfactual examples in order to better understand what the sentence should have been in order to be associated with an opposite sentiment. The interest of the attention coefficients is reinforced by the "counterfactual confusion matrix": The high transformation rate of the reviews\' sentiments shows that the tokens selected thanks to the attention are strongly meaningful.'}),(0,a.jsx)(t.h2,{id:"next-step",children:"Next step"}),(0,a.jsx)(t.p,{children:'We plan to test other ways to generate counterfactual examples. One way would be to take advantage of the way DistilBert has been trained: the mask language modeling (MLM). The idea would be to mask the tokens with high average attention, and replace them with the tokens with the highest softmax in the "opposite cluster". This would ensure the grammatical correctness of the generated counterfactual example. Finally, the generation of counterfactual examples can have other applications than interpretability. In particular, it becomes possible to perform data augmentation in order to give more examples to a model. It can mitigate biases by balancing the sentiments of biased discriminated populations. This would improve fairness indicators while not degrading accuracy.'}),(0,a.jsx)(t.h2,{id:"references",children:"References"}),(0,a.jsx)(t.p,{children:"[1] VASWANI, Ashish, SHAZEER, Noam, PARMAR, Niki, et al. Attention is all you need. Advances in neural information processing systems, 2017, vol. 30."}),(0,a.jsx)(t.p,{children:"[2] SANH, Victor, DEBUT, Lysandre, CHAUMOND, Julien, et al. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019."}),(0,a.jsxs)(t.p,{children:["[3] Hugging face library ",(0,a.jsx)(t.a,{href:"https://huggingface.co/",children:"https://huggingface.co/"})]}),(0,a.jsx)(t.p,{children:"[4] PEARL, Judea et MACKENZIE, Dana. The book of why: the new science of cause and effect. Basic books, 2018"})]})]})}function d(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},38720:(e,t,i)=>{i.d(t,{A:()=>n});const n=i.p+"assets/images/Image_3-094ab2c3d6b69a58223f9a733aef845b.jpg"},62838:(e,t,i)=>{i.d(t,{A:()=>n});const n=i.p+"assets/images/Image_5-de802c493bd17b6c40e5c5714774720e.jpg"},91578:e=>{e.exports=JSON.parse('{"permalink":"/blog/2022/10/26/Interpretability_sentiment_analysis_II","source":"@site/blog/2022-10-26-Interpretability_sentiment_analysis_II.md","title":"Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (2/2)","description":"Two illustrations of how attention coefficients can be a source of interpretability.","date":"2022-10-26T00:00:00.000Z","tags":[{"inline":true,"label":"NLP","permalink":"/blog/tags/nlp"},{"inline":true,"label":"Transformers","permalink":"/blog/tags/transformers"},{"inline":true,"label":"BERT","permalink":"/blog/tags/bert"},{"inline":true,"label":"interpretability","permalink":"/blog/tags/interpretability"},{"inline":true,"label":"explainability","permalink":"/blog/tags/explainability"},{"inline":true,"label":"XAI","permalink":"/blog/tags/xai"},{"inline":true,"label":"attention","permalink":"/blog/tags/attention"}],"readingTime":6.17,"hasTruncateMarker":true,"authors":[{"name":"Milan Bhan","title":"Senior Data Science Consultant, PhD student","url":"mailto:inno@ekimetrics.com","key":null,"page":null}],"frontMatter":{"title":"Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (2/2)","author":"Milan Bhan","author_title":"Senior Data Science Consultant, PhD student","author_url":"mailto:inno@ekimetrics.com","header_image_url":"./img/blog/interpretability_articles_2.jpg","tags":["NLP","Transformers","BERT","interpretability","explainability","XAI","attention"],"draft":false,"description":"Two illustrations of how attention coefficients can be a source of interpretability.","keywords":["Data Science","EkiLab","Ekimetrics","Eki.Lab","Eki","Machine Learning","Artificial Intelligence","Data Science for business","Operational Research","Optimization","Knapsack problem","Deep Reinforcement Learning"]},"unlisted":false,"prevItem":{"title":"Exploring the links between creative execution and marketing effectiveness - Part I: Detectron2 Pre-Trained Object Detection Models","permalink":"/blog/2022/11/10/creative_execution_and_marketing_effectiveness_part_I"},"nextItem":{"title":"Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (1/2)","permalink":"/blog/2022/10/18/Interpretability_sentiment_analysis_I"}}')},91981:(e,t,i)=>{i.d(t,{A:()=>n});const n=i.p+"assets/images/interpretability_articles_2-cc917aa65ee22681f92d30c33c40d0d0.jpg"},94301:(e,t,i)=>{i.d(t,{A:()=>n});const n=i.p+"assets/images/Image_2-fc95973e15821ef99dbc2fd6e4a5b6c8.jpg"}}]);