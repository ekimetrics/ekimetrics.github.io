<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Eki.Lab Blog</title>
        <link>https://ekimetrics.github.io/blog</link>
        <description>Eki.Lab Blog</description>
        <lastBuildDate>Fri, 23 Feb 2024 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Uncertainty in time series forecasting]]></title>
            <link>https://ekimetrics.github.io/blog/Uncertainty_TS</link>
            <guid>https://ekimetrics.github.io/blog/Uncertainty_TS</guid>
            <pubDate>Fri, 23 Feb 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Discover uncertainty quantification for time series forecasting.]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/Uncertainty_TS-f4b33f9c07724504284127dee3524d01.jpg" width="5434" height="3623" class="img_ev3q"></p></div><br><div align="justify"><p>Uncertainty in time series forecasting refers to the unpredictability and variability associated with predicting future values in a sequence of data points over time. Time series forecasting involves analyzing historical data to make informed predictions about future trends, patterns, or values. However, due to various factors such as randomness, external influences, and incomplete information, forecasting models often encounter uncertainty. Managing and quantifying uncertainty in time series forecasting is crucial for decision-making processes across diverse domains such as finance, economics, weather forecasting, and supply chain management.</p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="why-prefer-uncertainty">Why prefer uncertainty?<a href="#why-prefer-uncertainty" class="hash-link" aria-label="Direct link to Why prefer uncertainty?" title="Direct link to Why prefer uncertainty?">​</a></h2><div align="justify"><p>Accuracy methods, otherwise known as point forecasting, are commonly used to address issues confronted in times series. Attempting to estimate the closest value to reality without considering all risk-related factors refers to accuracy. This approach leans more toward a retrospective examination of past errors. </p><p>Considering the stochastic nature of the universe, we aim to approach interval within which the true values will fall, considering potential future risks and potential future errors. We're no longer referring to point forecasting but rather interval forecasting.</p><p>The need to quantify uncertainty can arise in various ways, for example:</p><ul><li><p>Uncertainty forecasting helps energy companies make decisions about buying and selling energy in real-time markets, considering fluctuations in demand and supply</p></li><li><p>Forecasting uncertainty in demand and supply can help retail businesses optimize their production, distribution processes and stock management </p></li><li><p>Hospitals can use uncertainty forecasts to anticipate patient admissions, enabling better resource allocation, staffing decisions and pharmaceutical orders</p></li></ul></div><div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/EarthFromSpace-239895b591f3b6601a8a93c29fde5683.jpg" width="1386" height="473" class="img_ev3q"></p></div><div align="justify"><p><strong>The most important thing to keep in mind is that even a good estimator can make important errors due to the volatility that a time series can present.  To prevent this problem, the use of prediction intervals is recommended.</strong></p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="popular-methods-for-uncertainty-quantification">Popular methods for uncertainty quantification<a href="#popular-methods-for-uncertainty-quantification" class="hash-link" aria-label="Direct link to Popular methods for uncertainty quantification" title="Direct link to Popular methods for uncertainty quantification">​</a></h2><div align="justify"><p>There are a huge number of methods capable of creating prediction intervals that quantify the uncertainty surrounding an observation. However, it is possible to group most of them into five methods. </p></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="bootstrap">Bootstrap<a href="#bootstrap" class="hash-link" aria-label="Direct link to Bootstrap" title="Direct link to Bootstrap">​</a></h3><div align="justify"><p>Bootstrap is a statistical technique used to estimate the variability of a statistic . It involves generating multiple bootstrap samples through resampling, with replacement, from  the original dataset. Subsequently, these samples undergo analysis employing the same statistical model.  The results provide a range of values that represent the uncertainty associated with your forecast or analysis, offering a more nuanced understanding of potential fluctuations in outcomes. This approach provides a flexible and data-driven method to assess the precision of statistical estimates.</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Bootstrap-20851c749e5bd9b16eaa51a8ea37312e.png" width="506" height="189" class="img_ev3q"></p></div><br><p>Advantages of this approach include its utilization of non-parametric methods, requiring a minimal amount of data, and ease of implementation. However, the method has disadvantages such as demanding intensive computing resources, being highly dependent on input data, and posing challenges in terms of interpretation.</p></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="asymmetric-regression">Asymmetric regression<a href="#asymmetric-regression" class="hash-link" aria-label="Direct link to Asymmetric regression" title="Direct link to Asymmetric regression">​</a></h3><div align="justify"><p>In machine learning, altering the loss function can reshape a model's predictions by influencing how it handles errors. During the training phase, a machine learning model learns to adjust its parameters to minimize the overall loss  , but in the case of asymmetric loss functions, penalties for overestimations and underestimations are different. The optimization process involves balancing the penalties associated with overestimating and underestimating based on parameter values. Fine-tuning these parameters allows to adapt the model's behavior. This introduces a nuanced response to errors. Among other things, this is the principle behind quantile regression.</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/AssymetricRegression-dac8b6542211b054dbc87ff3355f7f39.png" width="749" height="189" class="img_ev3q"></p></div><p>Advantages of the approach include its compatibility with non-parametric methods . However, it comes with disadvantages such as the absence of mathematical guarantees, uncalibrated forecasts, and the complexity of interpretation associated with the method.</p></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="probabilistic-models">Probabilistic models<a href="#probabilistic-models" class="hash-link" aria-label="Direct link to Probabilistic models" title="Direct link to Probabilistic models">​</a></h3><div align="justify"><p>Probabilistic models are favored for their capacity to output distributions instead of single points estimates.  Numerous probabilistic models exist for time series forecasting, and among them, ARIMA models are notably popular.</p><p>The process begins with the model fitting the time series data, capturing the autoregressive (AR) and moving average (MA) components, and ensuring stationarity through differencing (integrated component). After generating point forecasts, the model assesses the residuals, which represent the differences between the observed and predicted values. ARIMA utilizes the standard deviation of the normally distributed residuals to construct prediction intervals around the point forecasts. Essentially, the wider the prediction interval, the greater the uncertainty associated with the forecast. This technical methodology not only refines the accuracy of point forecasts but also provides a statistically sound measure of the range within which future observations are likely to fall.</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Probabilistic-ee0c8ad2bf822f5fa79f424fe51ee035.png" width="520" height="189" class="img_ev3q"></p></div><p>Advantages of this method include its simplicity and interpretability, along with the absence of a requirement for exogenous variables (even if it could be also a disadvantage). However, the method comes with disadvantages such as reliance on assumptions about the data and the need for parameter selection like most probabilistic methods.</p></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="bayesian-approach">Bayesian approach<a href="#bayesian-approach" class="hash-link" aria-label="Direct link to Bayesian approach" title="Direct link to Bayesian approach">​</a></h3><div align="justify"><p>Bayesian time series forecasting employs a technically advanced approach, involving probabilistic modeling, Bayesian inference, and iterative parameter estimation. During modeling, prior distributions are specified, which are updated through Bayesian inference with observed data to obtain posterior distributions. Techniques such as Markov Chain Monte Carlo or variational inference optimize these distributions for parameter estimation. It allows you to quantify uncertainty in your forecasts thanks to posterior distributions.</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Bayesian-92853115c2370535ab40c2646c2308d9.png" width="745" height="189" class="img_ev3q"></p></div><p>Advantages of this approach include the ability to incorporate business knowledge and the possibility of extracting insights despite weakly informative datasets. However, a notable disadvantage is that incorrect priors may lead to inaccurate results.</p></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conformal-prediction">Conformal prediction<a href="#conformal-prediction" class="hash-link" aria-label="Direct link to Conformal prediction" title="Direct link to Conformal prediction">​</a></h3><div align="justify"><p>Conformal prediction offers finite-sample conformal intervals applicable to any model and dataset without additional costs. It is particularly valuable for black box models, requiring no modification to their analysis or training. In addition to the usual testing and training datasets, the methodology also relies on a calibration set, which aids in generating conformalized intervals by rectifying errors introduced by the regressor. A crucial aspect in the design of conformal prediction is adaptivity. The objective is to ensure that the procedure yields larger intervals for more challenging inputs and smaller intervals for simpler inputs, tailoring the predictive uncertainty to the inherent difficulty of each input.</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/ConformalPrediction-d03b13f13bab4b37e5eeff7c6a4dd274.png" width="940" height="189" class="img_ev3q"></p></div>Advantages of this method encompass mathematical coverage guarantees, consistent and robust performance, and applicability to any model or dataset. However, there are drawbacks, such as certain methods requiring intensive computing, which may pose challenges in a business application context, and a dependency on a good regressor.</div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="evaluation-key-points">Evaluation key points<a href="#evaluation-key-points" class="hash-link" aria-label="Direct link to Evaluation key points" title="Direct link to Evaluation key points">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="coverage">Coverage<a href="#coverage" class="hash-link" aria-label="Direct link to Coverage" title="Direct link to Coverage">​</a></h3><div align="justify"><p>Coverage is a critical aspect in uncertainty prediction, particularly when considering an error level α (0 &lt; α &lt; 1). The anticipated outcome is that, for a given confidence level of (1 - α), we expect that (1 - α)% of the observations fall within the predicted uncertainty intervals. In other words, this statement underscores the importance of the model accurately capturing and quantifying uncertainty, ensuring that the specified percentage of observations is encompassed by the predicted uncertainty range, aligning with the desired level of confidence.</p><p>This concept becomes especially crucial in scenarios where robust and reliable uncertainty estimates are required.</p></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="interval">Interval<a href="#interval" class="hash-link" aria-label="Direct link to Interval" title="Direct link to Interval">​</a></h3><div align="justify"><p>In uncertainty quantification, a brief interval is commonly seen as advantageous, suggesting improved precision in predictions with constrained uncertainty. While a narrow uncertainty interval signifies heightened confidence in the model's predictive capabilities, it is imperative to remain watchful of another critical factor: variance. Although low variance can indicate prediction stability, an excessively low variance may raise concerns. This could suggest that the model oversimplifies the complexity of the problem, potentially overlooking crucial nuances in the data.</p><p>Thus, achieving a balance between pursuing tight uncertainty intervals, indicative of precision, and considering variance is vital to ensure the resilience of the uncertainty quantification model, aligning it with the characteristics of the data.</p></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="adaptiveness">Adaptiveness<a href="#adaptiveness" class="hash-link" aria-label="Direct link to Adaptiveness" title="Direct link to Adaptiveness">​</a></h3><div align="justify"><p>Adaptability is a crucial aspect in evaluating uncertainty quantification, focusing on temporal and sectional adaptiveness. Temporal adaptiveness ensures that the model can dynamically adjust its uncertainty estimation over time, maintaining relevance amid evolving data trends. Similarly, sectional adaptiveness underscores the need for adaptability within different segments or subsets of a dataset. An adaptive model should effectively address variations in uncertainty within these sections, acknowledging potential diversity in conditions or characteristics.</p><p>In summary, within the realm of uncertainty quantification evaluation, adaptability, exemplified through temporal and sectional adaptiveness, represents a comprehensive and nuanced approach that return larger sets for harder inputs and smaller sets for easier inputs.</p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><div align="justify"><p>In conclusion, the exploration of time series forecasting underscores the shift from traditional accuracy methods to uncertainty quantification. Various methods, such as Bootstrap, Asymmetric Regression, Probabilistic Models, Bayesian Approach, and Conformal Prediction, offer diverse approaches, each with their respective advantages and disadvantages. Conformal prediction, a set of recent methods, still under research and development,  holds the potential to outperform other approaches in the field. That's why, at Eki.Lab, we're pushing ahead with many studies linked to Conformal Prediction.</p><p>The main points of evaluation  we need to look at when evaluating these methods are coverage and adaptivity. We want our predictions to be able to evolve over time and across different dimensions  of the dataset to ensure a good coverage rate whatever the particularities of the predicted observation. It's important to find a balance between keeping our uncertainty ranges narrow enough to be precise but also considering how much our predictions might vary. This is all the key to building a strong model that can handle the complexities of real-world data.</p></div>]]></content:encoded>
            <category>Uncertainty Quantification</category>
            <category>Interval Forecasting</category>
            <category>Time Series Modeling</category>
            <category>Demand Sensing</category>
        </item>
        <item>
            <title><![CDATA[Maths Men: The Economics of Online Advertising]]></title>
            <link>https://ekimetrics.github.io/blog/Economics_online_ad</link>
            <guid>https://ekimetrics.github.io/blog/Economics_online_ad</guid>
            <pubDate>Fri, 01 Sep 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Willing to learn about the online ad market, cross-media effects and contextual advertising? This article wraps up the content of a PhD thesis on the economics of online advertising, defended by Rémi Devaux, Senior Consultant at Ekimetrics.]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/economics_online_ad-6ebd119792714bda9f9c002c0bb7f669.jpg" width="4096" height="2730" class="img_ev3q"></p></div><br><div align="justify"><p>This article wraps up the content of a PhD thesis in economics defended on April 4<sup>th</sup> 2023, by Rémi Devaux, now senior consultant in data science at Ekimetrics. This PhD thesis was part of a 3-year academic partnership between Ekimetrics and Mines Paris PSL. </p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-is-new-with-online-advertising">What is New with Online Advertising<a href="#what-is-new-with-online-advertising" class="hash-link" aria-label="Direct link to What is New with Online Advertising" title="Direct link to What is New with Online Advertising">​</a></h2><div align="justify"><p>Accounting for 67% of the 930 billion media spend worldwide<sup id="fnref-1-f3baf9"><a href="#fn-1-f3baf9" class="footnote-ref">1</a></sup>, online advertising is an obsessive presence in our online life. Search engines, video-sharing platforms, social media and now streaming services: ads are present in many formats on the Internet. In fact, most online platforms are partially or totally financed by advertising. Ad spending by brands subsidies online services, allowing platforms to charge no price to users. This is not new: classified ads financed free newspapers, just as brand sponsoring subsidized radio broadcast. So, what’s new with online advertising? </p><p>Economists pointed out that online advertising decreased the cost of targeting consumers<sup id="fnref-2-f3baf9"><a href="#fn-2-f3baf9" class="footnote-ref">2</a></sup>. This is a major change in the marketing industry. While offline, advertisers can only target their ads toward a general context (newspaper title, TV show, billboard location), online ads can be addressed on a consumer-level basis using personal data (socio-demographics, web history, online behaviors). This decrease in targeting cost acted as a game-changer on many parts of the advertising industry:</p><ul><li><p><strong>Pricing and auctions</strong>: finer targeting increases the quantity of ad impressions available to advertisers. As a result, such a massive volume of impressions cannot be sold at a flat price, just as in the offline advertising world. On the Internet, ad impressions are thus often allocated by auctions. Advertisers compete for a given ad impression based on the characteristics of the impression (user data, website…). The brand associated with the higher bid then wins the impression and displays its ad. This allows ad spaces to be priced at their right price, making advertisers and publishers better off.</p></li><li><p><strong>Intermediaries</strong>: this complex system of auctions induces that brands and publishers should be able to manage their ad opportunities and bidding strategy on an impression-per-impression basis. Naturally, given the high volume of ad impressions traded every minute, this is not feasible. That is why, online, many ad intermediaries manage the trading of ad spaces on the supply-side (publishers) as well as on the demand-side (advertisers).</p></li><li><p><strong>Data and Measurement</strong>: the technical nature of online advertising emphasizes the power of data. Views, clicks, conversions: low-targeting costs allow advertisers to observe individual responses to their ads, producing a huge amount of data. Machine learning and data analysis are used at every stage of the ad-serving to optimize advertisers’ bid, click-through-rate, publisher revenues, post-campaign attribution…
That is why observers often made the claim that with the Internet, the advertising industry went from <em>Mad Men to Maths Men</em>.</p></li></ul><p>This PhD thesis focuses on two questions: (i) how online advertising is distinct from traditional advertisement and (ii) how context plays a role in online ads effectiveness. These questions are answered using econometric techniques applied to advertising data.</p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="old-world-new-world">Old World, New World?<a href="#old-world-new-world" class="hash-link" aria-label="Direct link to Old World, New World?" title="Direct link to Old World, New World?">​</a></h2><div align="justify"><p>The first part of the thesis studies whether online advertising is a complement or a substitute to traditional advertisements (the one defined by offline mediums such as TV, radio, billboards, newspapers, or cinema). As we saw in the introduction, online advertising works in a very different way compared to offline media where ad buying involves human negotiation, discounts and where ad effectiveness cannot be observed directly. However, many economists claimed that offline and online advertising were close substitutes<sup id="fnref-3-f3baf9"><a href="#fn-3-f3baf9" class="footnote-ref">3</a></sup>. </p><p>One can investigate this question according to two viewpoints: advertising supply and demand.</p><p>On the supply-side, we can determine that two types of advertising media are substitutes if brands tend to use both interchangeably. Conversely, two advertising media types are complements when advertisers tend to use them together, with the goal of generating synergies. Substitutability between two media types can be investigated using the concept of <em>cross-price elasticity</em>: when the price of a media type increases, how does it impact the demand for another media type? </p><p>Using data from 10 big national advertisers in three different industries, I investigate how firm’s spending on offline, internet display<sup id="fnref-4-f3baf9"><a href="#fn-4-f3baf9" class="footnote-ref">4</a></sup>  and search ads react to each media price. Using an econometric model derived from microeconomic theory, I find that offline and online advertising media types are not close substitutes.<br>
<!-- -->Intuitively, offline and search do not substitute for each other. Display and offline ad media types seem to be substitutes, but mostly for non-audiovisual media such as newspapers, magazines, and billboards. Indeed, internet display formats substitutes more easily to print advertisement than to TV or radio ads.<br>
<!-- -->Finally, the model shows that display and search ads are substitutable for advertisers. This can be natural as it seems certain types of display ads (such as retargeting) can be used to generate conversions, just as search ads.</p><p>We can also determine whether offline and online ads are complements or substitutes by looking at the demand side: i.e., how consumers respond to ads. Marketing researchers showed that advertising campaigns on different media may generate <em>synergies</em><sup id="fnref-5-f3baf9"><a href="#fn-5-f3baf9" class="footnote-ref">5</a></sup>. That is, the return of investing on two media may be greater than the sum of the two returns taken individually. This kind of synergy typically happens when branding and conversion media are played together. For example, a national TV campaign may raise awareness and desirability for a product, increasing online search volume and thus the effectiveness of sponsored links.</p><p>In another study of the thesis, I try to draw a line between offline ad spending and the effectiveness of search ads. Using data from a panel of brands in the hotel sector, I build an econometric model where clicks on sponsored search ads depend on the brand’s own offline ad spending, other ad spending, seasonality, search ad exposure and market-specific factors. Because the effect of offline ad campaigns may be inter-temporal, the investments of period t-1 enters period t after being discounted by a factor <em>λ</em>. I find that, on average, increasing the discounted stock of offline ad investments by 10% increases the volume of clicks recorded on search ads by more than 9%. Thus, the effectiveness of online ads also depends on the investment offline, making both types of media complementary.</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Picture1-41598bbb139ac23ca96028e34acc0cbf.png" width="1279" height="895" class="img_ev3q"></p><div align="center"> Figure 1: Relationship between Clicks on Online Ads and Offline Advertising</div></div><br><p>Overall, my results suggest that offline and online advertisements are not close substitutes. Looking at how both types of media work, the way advertisers use them and how consumers react to them, offline and internet advertising are different, and sometimes complementary, markets.</p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="context-matters">Context Matters!<a href="#context-matters" class="hash-link" aria-label="Direct link to Context Matters!" title="Direct link to Context Matters!">​</a></h2><div align="justify"><p>The ability to finely-target advertisements has allowed brands to better match their message to consumers. Targeted advertising thus generates gains by reducing ad ‘waste’<sup id="fnref-6-f3baf9"><a href="#fn-6-f3baf9" class="footnote-ref">6</a></sup> (which occurs whenever the ad is distributed to a consumer who has no preference for the product advertised).</p><p>However, focusing solely on individual targeting obliterates an important factor: context. In many cases, internet advertising as been subject to placement issues: an ad may be placed on an unviewable part of the page or worse, inside a bad context. The latter case includes ads displayed on controversial websites (e.g., conspiracy-related blogs, pornographic websites), or inside a content that is irrelevant to the ad: in 2012, an ad for cruise vacations run on a YouTube showing the sinking of the Costa Concordia<sup id="fnref-7-f3baf9"><a href="#fn-7-f3baf9" class="footnote-ref">7</a></sup>. There are many other examples.</p><p>This kind of issue arises from an information asymmetry between the advertiser and the publisher. Online advertising features so many possible ad placements and the process of matching ads to placements is so complex that, eventually, advertisers rarely know where their ad appears. Without carefully considering contextual effects, advertisers may waste their ad budget, and even harm their brand image by appearing in bad contexts. Two works of my studies try to shed light on such contextual effects.</p><p>In a first study, I use data from a viewability provider to investigate the determinant of ad viewability. The data showed that more than 20% of video ads for a healthcare advertiser were never seen. I try to investigate the determinant of ad viewability. Using an econometric model, I show that when the ad network did not have incentives to make ads viewable, it reduced the overall viewability rate of the campaign. Buying ads on a cost-per-view instead of cost-per-impressions basis induces the ad network to make the ad viewable, increasing viewability by almost 20%. Programmatic advertising also deters ad viewability by introducing opacity between the advertiser and the publisher.</p><p>In a second study, I investigate how controversial content on socials impact advertising effectiveness. I use the 2020 Facebook Ad Boycott as a natural experiment. In July 2020, more than 1,000 advertisers pulled out their ad spending from Facebook and Instagram as both platforms were hosting controversial content. Using data from an advertiser in the skincare industry that continued to advertise, I estimate how Facebook ad effectiveness changed compared to the brand’s ads on other platforms. Controlling for platform-specific effectiveness and seasonality, I find that during the boycott, Facebook Ads experienced a decrease of up to 10,000 clicks compared to the expected effectiveness of the ads.</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Picture2-e913d29bdfa63e5fff1150162fdd325f.png" width="1358" height="988" class="img_ev3q"></p><div align="center"> Figure 2: Effect of the boycott (week 0) on clicks recorded on Facebook Ads. Estimates are reported with 95% confidence interval.</div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><div align="justify"><p>Online advertising is evolving at an unprecedent pace and so is regulation. Since 2020, political initiatives foster consumer protection, privacy, and abuse of dominance in the digital economy. This has at least two implications for online advertising.</p><p>First, for antitrust purposes market regulators need to define the relevant market for advertising. Does regulation on the online ad market affect offline advertising? Does Google Ads compete with national TV networks? Such questions are crucial to understand the market power of online advertising players. This thesis brings some insight from this perspective.</p><p>Second, the strengthening of personal data regulation, along the end of third-party cookies will emphasize the importance of ad context. As targeting individual-level characteristics will be more and more difficult, brands will gain in leveraging context to target their ads. For example, targeting hotel ads on travel blog pages is a way to reach relevant consumers. </p><p>Finally, cookies limitation will impact advertiser’s ability to finally measure the return of their advertisement. More than never, holistic econometric measurement with Marketing Mix Modeling is a key tool to measure advertising effectiveness.</p><br></div><div class="footnotes"><hr><ol><li id="fn-1-f3baf9">eMarketer (2023). Worldwide Digital Ad Spending 2023. <em>Insider Intelligence</em>. <a href="https://www.insiderintelligence.com/content/worldwide-digital-ad-spending-2023" target="_blank" rel="noopener noreferrer">https://www.insiderintelligence.com/content/worldwide-digital-ad-spending-2023</a><a href="#fnref-1-f3baf9" class="footnote-backref">↩</a></li><li id="fn-2-f3baf9">Goldfarb, A. (2014). What is different about online advertising? <em>Review of Industrial Organization, 44</em>, 115-129.<a href="#fnref-2-f3baf9" class="footnote-backref">↩</a></li><li id="fn-3-f3baf9">Goldfarb, A., &amp; Tucker, C. (2011). Substitution between offline and online advertising markets. <em>Journal of Competition Law and Economics, 7</em>(1), 37-44.<a href="#fnref-3-f3baf9" class="footnote-backref">↩</a></li><li id="fn-4-f3baf9">Here, Internet display is defined as banners, videos, and social media ads.<a href="#fnref-4-f3baf9" class="footnote-backref">↩</a></li><li id="fn-5-f3baf9">Naik, P. A., &amp; Raman, K. (2003). Understanding the impact of synergy in multimedia communications. <em>Journal of marketing research, 40</em>(4), 375-388.<a href="#fnref-5-f3baf9" class="footnote-backref">↩</a></li><li id="fn-6-f3baf9">Iyer, G., Soberman, D., &amp; Villas-Boas, J. M. (2005). The targeting of advertising. <em>Marketing Science, 24</em>(3), 461-476<a href="#fnref-6-f3baf9" class="footnote-backref">↩</a></li><li id="fn-7-f3baf9">Heilpern, W. (2016, 22 mai). 26 of the most hilarious, unfortunate online ad placements. Business Insider. <a href="https://www.businessinsider.com/here-are-the-most-hilarious-unfortunate-online-ad-placements-ever-2016-5?r=US&amp;IR=T" target="_blank" rel="noopener noreferrer">https://www.businessinsider.com/here-are-the-most-hilarious-unfortunate-online-ad-placements-ever-2016-5?r=US&amp;IR=T</a><a href="#fnref-7-f3baf9" class="footnote-backref">↩</a></li></ol></div>]]></content:encoded>
            <category>Online Advertising</category>
            <category>Economics</category>
            <category>Econometrics</category>
            <category>Cross-media</category>
            <category>Contextual advertising</category>
            <category>Academic Research</category>
        </item>
        <item>
            <title><![CDATA[ClimateQ&A]]></title>
            <link>https://ekimetrics.github.io/blog/ClimateQ&amp;A</link>
            <guid>https://ekimetrics.github.io/blog/ClimateQ&amp;A</guid>
            <pubDate>Thu, 13 Jul 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[AI to make climate and biodiversity science more accessible.]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/ClimateQA-dc6ccd7ee02461ffd516b5d71680d886.jpg" width="1280" height="720" class="img_ev3q"></p></div><div align="justify"><p>ClimateQ&amp;A is an AI-powered tool that has been specifically developed to help address the challenges associated with understanding and accessing climate change and biodiversity-related literature. The tool aims to democratize access to the scientific literature of climate change and biodiversity, making it easier for researchers, policymakers, and the public to understand and use this critically important information. This article aims to provide users with information regarding the tool, encompassing its contextual framework, technical operation, as well as its inherent limitations.</p><p>Disclaimer : for simplicity, we use “climate” as an umbrella term to designate the phenomena of climate change &amp; biodiversity loss. We are currently thinking about more inclusive names.</p><p>For a shorter version of this article, please refer to the <a href="https://www.youtube.com/watch?v=DwGm0-53iTQ" target="_blank" rel="noopener noreferrer">conference ClimateQ&amp;A</a> by Datacraft and Ekimetrics (only in French). </p><p>Click <a href="https://www.climateqa.com/" target="_blank" rel="noopener noreferrer">here</a> to access the latest version of the tool, now featuring a kids version.</p><p>Click <a href="https://fr.linkedin.com/posts/th%C3%A9o-alves-da-costa-09397a82_bilan-2-mois-apr%C3%A8s-le-lancement-de-la-activity-7067146668965519361-AUgW" target="_blank" rel="noopener noreferrer">here</a> to access a preliminary analysis of the questions asked to the tool.</p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="i-introduction-why-climateqa">I. Introduction: Why ClimateQ&amp;A<a href="#i-introduction-why-climateqa" class="hash-link" aria-label="Direct link to I. Introduction: Why ClimateQ&amp;A" title="Direct link to I. Introduction: Why ClimateQ&amp;A">​</a></h2><div align="justify"><p>Against the backdrop of rising global temperatures and the alarming decline of biodiversity, human societies are not catching up on action. Across geographies, widespread climate literacy remains low, which limits and hinders actions to mitigate and adapt to these pressing challenges. This lack of actionable knowledge is due to several factors, including the increasing demand for climate-related information from all groups of society (including the public sector, students, corporates, policymakers, investors, and local communities, among many others), and rising fake news &amp; climate skepticism. Another important issue is that scientific reports of IPCC and IPBES present some inherent limitations regarding accessibility, language, and understanding. Indeed, at a Side Event at COP21 in Paris, in the lead-up to the Oslo Expert Meeting on Communication in 2016, IPCC Chair Hoseung Lee asked :</p><blockquote><p>“<em>What use are the IPCC reports if many of the intended users cannot understand them, do not know where to find them, or cannot use them in their work?”</em></p></blockquote><p>In this part we explore these issues, which are vital to inform the development of our tool ClimateQ&amp;A.</p></div><br><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="aclimate-knowledge-and-literacy-are-still-lagging">A.	Climate knowledge and literacy are still lagging<a href="#aclimate-knowledge-and-literacy-are-still-lagging" class="hash-link" aria-label="Direct link to A.	Climate knowledge and literacy are still lagging" title="Direct link to A.	Climate knowledge and literacy are still lagging">​</a></h3><div align="justify">In order to act against climate change, citizens need to be climate-literate:<blockquote><p><em>"Climate literacy is a subset of the broader science literacy that refers to the knowledge, skills, and attitudes that individuals, communities, and societies need to understand and address climate change effectively. It draws on climate science, the quantitative and geospatial technologies by which it is understood, and the interconnectedness of human beings with their environment . Literacy on climate change is vital for informed decision-making, emissions reduction, and community resilience."</em> <sup id="fnref-1-8d0ebe"><a href="#fn-1-8d0ebe" class="footnote-ref">1</a></sup></p></blockquote><p>Despite the importance of climate literacy, studies find that in advanced economies, ~70% of people are aware of climate change and its consequences, but only 20% are climate-literate<sup id="fnref-2-8d0ebe"><a href="#fn-2-8d0ebe" class="footnote-ref">2</a></sup> <sup id="fnref-3-8d0ebe"><a href="#fn-3-8d0ebe" class="footnote-ref">3</a></sup>.
Additionally, climate awareness is unequally distributed: in 2015, 40% of adults in the world had never heard of climate change – roughly 2Bn people. In some developing countries like Egypt, Bangladesh, or India, this represents more than 65% of the adult population<sup id="fnref-4-8d0ebe"><a href="#fn-4-8d0ebe" class="footnote-ref">4</a></sup>.</p></div><br><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="bthe-demand-for-climate-related-information-is-increasing">B.	The demand for climate-related information is increasing<a href="#bthe-demand-for-climate-related-information-is-increasing" class="hash-link" aria-label="Direct link to B.	The demand for climate-related information is increasing" title="Direct link to B.	The demand for climate-related information is increasing">​</a></h3><div align="justify"><p>Driven by the global and far-reaching implications of climate change - across geographies, populations, sectors of activity, organizations- the demand for accurate, timely, and relevant climate information is increasing. Businesses and investors seek to understand their exposure to climate and biodiversity-related risks. Educators need to train a new generation of students, who will live with the consequences of climate change. Policymakers require reliable information for effective strategies. The media plays a vital role in disseminating factual information to the general public. As climate change shapes our world, this demand will continue to grow, requiring informed decision-making across sectors. </p><p>Another significant catalyst for information demand stems from the targeting of the IPCC by misinformation campaigns. The surge in media coverage surrounding climate change has witnessed a simultaneous surge in influential campaigns disseminating misinformation. These campaigns employ tactics such as selective media exposure, contrived controversies, alternative facts, and distorted media balance. Adding to this, social media algorithms amplify existing social circles and reinforce pre-existing opinions, while individuals increasingly rely on these platforms as their primary sources of information. Consequently, people find themselves trapped in echo chambers, unaware of the consensus or under the false impression that substantial uncertainty exists<sup id="fnref-5-8d0ebe"><a href="#fn-5-8d0ebe" class="footnote-ref">5</a></sup>. Frequently, socio-economic narratives and a sense of loss resulting from climate change policies direct individuals toward disbelief and denial. These narratives create a clash between scientific communications on climate change and one's familiar ideas and way of life, often intertwined with automobiles, industry, consumption, and a carbon-centric economy. As climate skepticism is based on narratives that do not even revolve around climate itself<sup id="fnref-6-8d0ebe"><a href="#fn-6-8d0ebe" class="footnote-ref">6</a></sup>, climate science communications have often achieved limited success.</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Picture1-b412ea7d57e7a93ea09abf2ec6efa1fb.png" width="449" height="304" class="img_ev3q"></p></div><p>In this context, it becomes paramount for climate scientists to multiply their communication endeavors and generate tailored content that caters to diverse audiences. By doing so, they would ensure that each segment of society has access to information that is relevant to their specific needs and concerns. This would enable users to not only access the desired information but also empower them to fact-check the information they come across. While the IPCC has made significant efforts to improve its communication strategy (e.g.; the organization held The Expert Meeting on Communication of 2016 which led to a number of recommendations to enhance IPCC communications activities, strategy and capacity), there are a number of limitations to this strategy stemming from how these organizations operate.</p></div><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Screenshot_Twitter_ClimateQA-312a1bc148ecfb2c621262e7c0760339.png" width="598" height="679" class="img_ev3q"></p></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="cthe-role-of-the-ipcc-and-ipbes--practical-limitations-and-consequences">C.	The role of the IPCC and IPBES : practical limitations and consequences<a href="#cthe-role-of-the-ipcc-and-ipbes--practical-limitations-and-consequences" class="hash-link" aria-label="Direct link to C.	The role of the IPCC and IPBES : practical limitations and consequences" title="Direct link to C.	The role of the IPCC and IPBES : practical limitations and consequences">​</a></h3><div align="justify"><p>IPCC and IPBES reports are the Gold Standard of the science of climate and nature, as they present a scientific consensus and synthetize in great detail, coverage, and historical depth the key considerations regarding living and countering with climate change, and the trends of biodiversity and nature’s contribution to people. Despite the unquestionable thoroughness and quality of these assessments, challenges to their true adoption by policymakers and civil society persist. Some practical challenges include:</p><ul><li><p><strong>Language.</strong> One significant limitation is the language barrier. Due to their affiliation with the United Nations, the IPCC and IPBES reports are only available in the UN's official languages, namely Arabic, Chinese, French, Russian, Spanish, and English. While these languages are spoken by approximately four billion people as either a first or second language, which accounts for 50% of the global population, the remaining 50% are unable to access climate science information from official sources due to this language restriction.</p></li><li><p><strong>Audience.</strong> Another challenge lies in the intended audience of these reports. Originally designed to provide a comprehensive review and recommendations on the state of climate science, the main recipients of these reports are technical audiences, such as scientists and policymakers. The IPCC and IPBES produce various types of documents, including Summary for Policymakers (SPMs), Full Reports, and Technical Summaries. These reports are often lengthy, exceeding 15,000 pages for the main IPCC and IPBES reports, and are filled with specialized terminology, such as confidence levels and uncertainty. Additionally, these reports are accessible only in PDF format through the respective organizations' websites.</p></li></ul><p>The level of complexity of this publications is such that at the 5th Assessment Report, senior policymakers called for a “Summary for Citizens” to complement the Summary for Policymakers<sup id="fnref-7-8d0ebe"><a href="#fn-7-8d0ebe" class="footnote-ref">7</a></sup>.</p><p>Moreover, as the organizations' mandates emphasize neutrality and a "policy-relevant" but not "policy-prescriptive" approach, their ability to communicate and engage with stakeholders is limited. They are constrained from publishing any information that has not undergone rigorous multilateral validation or that lacks contextualization. With the general purpose of bridging climate and nature science to policymaking, both organizations are built on three emergent principles: holding the line between policy relevance and prescription, enlisting geographically diverse participants, and evolving a thicket of procedures to guard scientific credibility<sup id="fnref-8-8d0ebe"><a href="#fn-8-8d0ebe" class="footnote-ref">8</a></sup>. The first principle aims to provide policy-relevant information without prescribing specific policies<sup id="fnref-9-8d0ebe"><a href="#fn-9-8d0ebe" class="footnote-ref">9</a></sup>, thus upholding neutrality and respecting the mandates of multilateral environmental agreements<sup id="fnref-10-8d0ebe"><a href="#fn-10-8d0ebe" class="footnote-ref">10</a></sup>. In an official statement, the IPCC declares :</p><p><em>“By endorsing the IPCC reports, governments acknowledge the authority of their specific content. The work of the organization is therefore policy-relevant and yet policy-neutral, never policy-prescriptive<sup id="fnref-11-8d0ebe"><a href="#fn-11-8d0ebe" class="footnote-ref">11</a></sup>.”</em></p><p>What this means, is that in order to publish approved, adopted, and accepted reports, these organizations avoid technocratically mandating a particular policy solution when a range of options are feasible (even to different degrees)<sup id="fnref-12-8d0ebe"><a href="#fn-12-8d0ebe" class="footnote-ref">12</a></sup>. While this neutrality reinforces the credibility that is derived from their strict processes, it gives rise to two major improvement areas:</p><ol><li><p><strong>The need to consider reports collectively</strong><sup id="fnref-13-8d0ebe"><a href="#fn-13-8d0ebe" class="footnote-ref">13</a></sup>.  To avoid missing important information, it is essential to consider the entire body of reports collectively. The IPCC reports undergo a formal review process involving multiple drafts. Comments and feedback are provided by scientific experts, as well as representatives from governments and appointed consultants. These comments are taken into account during the approval of the Summary for Policymakers (SPM) and acceptance of the full report. Given that SPMs are subject to a high degree of compression, each sentence requires approval from the parties involved (scientists and government representatives), which often leads to rigorous discussions<sup id="fnref-14-8d0ebe"><a href="#fn-14-8d0ebe" class="footnote-ref">14</a></sup>. While this dialogue-based approach strengthens the IPCC assessment, it is important to make other relevant documents accessible to interested audiences.</p></li><li><p><strong>That scientific knowledge must be accessible to non-technical audiences</strong>. Building upon the previous point, the strict rules governing report texts restrict their modification. The IPCC faces challenges in presenting reports to non-specialists in a language that is more accessible, avoiding jargon such as confidence levels and likelihood language<sup id="fnref-15-8d0ebe"><a href="#fn-15-8d0ebe" class="footnote-ref">15</a></sup>, as well as reducing their length. While the IPCC may face criticism for delivering messages in a clearer, shorter, and potentially less rigorous manner, collaboration with other stakeholders can help bridge this gap.</p></li></ol></div><br><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="ii-implications-for-the-tool--desired-features--outcomes">II. Implications for the tool : desired features &amp; outcomes<a href="#ii-implications-for-the-tool--desired-features--outcomes" class="hash-link" aria-label="Direct link to II. Implications for the tool : desired features &amp; outcomes" title="Direct link to II. Implications for the tool : desired features &amp; outcomes">​</a></h2><div align="justify"><p>Stemming from 1. The need for increased climate-related information in a relevant and fast manner and 2. The practical limitations stemming from IPCC and IPBES operating principles, we derive some features to inform the product development of ClimateQ&amp;A. The tool must be, and provide answers that are: </p><ul><li><strong>Truthful</strong>: Our tool aims to present climate change and biodiversity information as accurately as it is documented in scientific reports. It serves as a reliable resource for fact-checking fake news, and its content can be easily fact-checked with the original sources. It should also follow the structure of IPCC reports, from SPMs to Full Reports.</li><li><strong>Clear</strong>: With a focus on catering to diverse audiences, our tool avoids technical jargon and provides concise answers that are easily understandable by both technical and non-technical users. It ensures that complex concepts, such as uncertainty and confidence levels, are explained in a user-friendly manner.</li><li><strong>Comprehensive</strong>: The tool compiles information from a wide range of available reports, supplementing individual report findings with additional sources and relevant discoveries. This comprehensive approach ensures that users gain a holistic understanding of the subject matter.</li><li><strong>Accessible</strong>: Our tool is designed to be freely accessible to all users, transcending language barriers by providing multilingual support. It is optimized for compatibility across various devices, allowing users to access the tool seamlessly.</li><li><strong>Fast &amp; Intuitive</strong>: By utilizing advanced search functionalities, our tool enables users to navigate through reports swiftly, eliminating the need for cumbersome manual searches (e.g., Ctrl + F). It automatically identifies and retrieves relevant words and topics related to the user's query, streamlining the information retrieval process.</li><li><strong>Collaborative</strong>: The development of our tool is a collaborative effort that incorporates feedback and input from stakeholders across different domains, including non-technical users, technical experts, and scientists. This collaborative approach ensures that the tool meets the needs and expectations of a diverse user base.</li></ul><p>We have designed the technical core of the tool to fit the aforementioned specifications.</p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="iii-technical-core">III. Technical core<a href="#iii-technical-core" class="hash-link" aria-label="Direct link to III. Technical core" title="Direct link to III. Technical core">​</a></h2><div align="justify"><p>ClimateQ&amp;A is built of several technical algorithmic modules, ChatGPT being the last one (the generation of an answer). The core of Climate Q&amp;A consists of three steps, summarized in Figure 1: creation of a structured dataset, question enrichment and sourcing, and generation &amp; display in an interface.</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Picture2-b1aa91178ecc7f67c433bbc28fb1a8df.svg" width="565" height="318" class="img_ev3q"></p></div><div align="center"> Figure 1: Climate Q&amp;A global architecture. First, a structure dataset is created from a set of heterogeneous documents in an offline mode (1.Structured dataset creation). Then, user questions  are enriched and sources (step 2). Finally, the answers with its sources are displayed in a web application.</div><br><p>All the documents used for Climate Q&amp;A are listed in the application page (section Sources) : <a href="https://huggingface.co/spaces/Ekimetrics/climate-question-answering" target="_blank" rel="noopener noreferrer">https://huggingface.co/spaces/Ekimetrics/climate-question-answering</a> </p><ins> Step 1: Creation of a structured dataset from a set of heterogeneous documents</ins><br><p>The first step parses a heterogeneous set of documents (PDFs, Words etc) and extracts information, which is stored in a structured database.</p><p>The algorithm uses OCR techniques for all the documents stored in any given folder. The document structure is preserved, meaning that paragraphs are linked to section titles, figures are linked to the passages that cite them, etc. A structured database is thus created, containing all the information extracted from the documents and the relationships between the different entries in the database. These entries are then represented as a vector of finite dimension. In ClimateGPT, we use SentenceBert, but any type of representation is possible. This will serve as the basis for a quick search in the database for step 3.</p><ins>Step 2: Enrichment of the user's LLM query</ins><br><p>In the second step, the user's query is reformulated and enriched by searching for similar passages in the structured database. The relevant question and passages are encapsulated in a prompt, which also serves to limit the scope of acceptable answers for ClimateQ&amp;A.</p><p>Within the step of enriching the user's query, the goal is to create a query on an LLM from the question, whose answer will be formulated from the documents of interest (the structured database from step 1). First, the user's question is reformulated in a more intelligible way for a LLM by asking the model to do it. Then, the new question is compared to all entries in the database using the Faiss algorithm developed by Facebook, known for its execution speed on data corpora up to billions of entries. The most relevant entries, meaning those with the most significant similarity to the question, are selected, filtered by a thematic classification model (in this case, does the entry discuss climate?) and encapsulated in a prompt, necessary for querying an LLM (in our case, GPT-3.5 Turbo). The prompt contains other information, such as not going beyond the scope of its knowledge and formulating its answer based on the selected entries. The prompt created is then used to query an LLM.</p><ins>Step 3: Displaying the results : generative part</ins><p>The query result (i.e.; the question) is displayed on an interface. The answer is generated using the 10 most relevant references found in the corpus of documents. The sources are displayed alongside the answer, so that the user can verify the information and extract it for a report if necessary.
The display is based on two parts: the LLM's answer with the notes serving as a reference, as well as the sources used to formulate the answer. For ClimateQ&amp;A, we use the OpenAI Azure environment to send the request and get the response. Just like the ClimateQ&amp;A interface, the user can continue their search by asking several questions in a row like a real ChatBot.</p><ins>Comments and future work</ins><p>We use GPT-3.5 Turbo for Climate Q&amp;A, but our methodology can be applied to any type of LLM. We have chosen this tool for the generative part of our model because it has been exceptionally optimized for dialogue using Reinforcement Learning with Human Feedback (RLHF) – a method that uses human demonstrations and preference comparisons to guide the model towards desired behaviors. In the future we will test other models like Bard and Llama.</p><p>The hierarchy of the documents are not taken into account in the first version. Summary reports can be used in a first time to a concise answer. If the user wants more details, global and longer reports can then served as references to add additional facts, numbers to the answer. Looking for passages or references in the most recent documents could also make our approach more relevant. This feature will be added in the next version of Climate Q&amp;A. </p><p>Qdrant is an excellent alternative to FAISS algorithm for search similarity, as it allows natively metadata featuring ( e.g. : search for a specific set of documents, or creating filters on passages), whereas FAISS can not. Moreover, Qdrant is faster for a small amount of data and is easier to use and maintain in production. We will add Qdrant as an option of similarity search for our next version.</p><p>The interface of the first version of Climate Q&amp;A displays textual information only. In the future, we will integrate images and graphics to enrich the answer. Building a specific graph for the answer would also be a notable improvement. For example, a graph which shows the evolution of the carbon footprint for a country per year is of a great interest to the question : “Did the carbon footprint of France increase the last five years?” </p><p>Textual data is now the main source of references for Climate Q&amp;A, but a lot of information is contained in tabs or graphics (like checked numbers etc). One of the main improvements for Climate Q&amp;A is to better parse and retrieve information from graphics and tabs. It will include novels features in OCR techniques and information retrieval techniques. </p></div><br><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="ivalgorithm-performance">IV.	Algorithm performance<a href="#ivalgorithm-performance" class="hash-link" aria-label="Direct link to IV.	Algorithm performance" title="Direct link to IV.	Algorithm performance">​</a></h2><div align="justify"><p>In order to assess the performance of our model, we evaluate the individual performances of each of the elements that form the generation of an answer.</p><ins>Retrieval</ins>: evaluating the retrieval process specifically on our data is a difficult task without a custom labialized dataset. Our approach to quantitatively evaluate it was to sample a thousand paragraphs from our knowledge base, and for each ask a LLM to generate a question that could be answered with each paragraph. Then, we did the inverse process and tried retrieving the documents using the questions. For the model we currently use, the paragraph was in the top 10 documents retrieved in 3 cases out of 4. There are lots of biases to this method and we are still working to refine it, but the results allow us to test and compare performance for different retrieval models.<br><br><ins>Generation</ins>: answers provided by ClimateQ&amp;A are subject to a human evaluation whose evaluation criteria are outlined below.<br><br><p>We analyse the performance of our model by comparing the answers of ChatGPT and ClimateQ&amp;A to those provided by the IPCC, assumed to be a “ground of truth”. We evaluate the answers based on the below criteria, i.e. relevance, accuracy and completeness, structure, quoting, and consistency. These serve as a qualitative measure of performance for the algorithm.</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Picture3-6296bf595cc1ef3cb0a95109b789a3cd.png" width="718" height="814" class="img_ev3q"></p></div></div><br><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="vlimitations">V.	Limitations<a href="#vlimitations" class="hash-link" aria-label="Direct link to V.	Limitations" title="Direct link to V.	Limitations">​</a></h2><div align="justify"><p>ClimateQ&amp;A has been built to answer to climate and biodiversity-related questions by finding 10 relevant paragraphs in the original IPCC and IPBES documents and generating an answer from those paragraphs. There are inherent limitations to this configuration and the underlying assets and tools that we use to parse the documents, retrieve relevant sources, and generate answers. In this part, we elaborate on these limitations, which we are planning to address in the following versions of the tool. These limitations are related to the content of the answers, biases inherent to the configuration of the model and the generative module, and the environmental footprint of AI.</p><p><strong>Limitations regarding the answers</strong> : ClimateQ&amp;A performs well in questions that are somewhat specific and whose answers can be found in the reports.</p><ul><li>ClimateQ&amp;A does not work well when it comes to summarizing entire documents. For example, a question such as “What are the conclusions of the last IPCC report on climate mitigation?” will not yield a high quality answer. The questions need to be more precise and concise in order for the retriever to find relevant passages and produce a better answer.</li><li>ClimateQ&amp;A cannot directly answer questions whose answers cannot be found in the reports, but that are relevant to understand and fight against global warming in specific contexts, for example “What is the IPCC’s position on the use of megadams as a solution to climate change?”, “Which French political party has the strongest measures against climate change?”, etc. These questions might not be answered precisely and could even be blocked by our moderation feature.</li><li>For other limitations of GPT-3 models regarding text synthesis and structural &amp; algorithmic limitations, please refer to (Brown, et al. 2020). </li></ul><p><strong>Bias</strong>: there are some bias associated to how the model is built and the underlying assets.</p><ins>Bias from the generation of answers linked to ChatGPT</ins>: GPT-3 was trained on a massive dataset of text from the open internet, including the entire public internet toughly every month since its inception (60% of weight in training mix, also called CommonCrawl), a crowd-sourced curated selection of the internet most popular page referrals from Reddit and Social Media (with a weight of 22%, also called the WebText2 dataset), portions of books that are available online (16%), and Wikipedia in English (3%).<p> Though sources are diverse, the biases of the data that the model has been trained on are retained. Biases favor language &amp; culture (predominantly English and Western-centric) and men. Conversely, negative sentiments are recorded for black people and Islam in GPT-3<sup id="fnref-16-8d0ebe"><a href="#fn-16-8d0ebe" class="footnote-ref">16</a></sup>. These biases represent an important aspect to assess the broader societal impacts of tools like ClimateQ&amp;A, especially as those biases translate in prejudices and views about climate change, which are, as seen in Part 1, very context-specific. In general, we find that the Anglo-Saxon single-materiality approach, i.e. to consider climate change risks that might have a financial impact on businesses (and not also the impacts of businesses on climate change), might produce some biases in our model. ChatGPT has also been finetuned via human feedback and user feedback. These might represent additional biases to the model.</p><br><ins>Bias in structure and selection</ins> :  The developers of ClimateQ&amp;A are European data scientists, engineers and climate / biodiversity consultants. There may be some biases related to the background of these persons, as well as the fact that none are climate or biodiversity scientists. While we have received feedback from expert scientists, most users that have given feedback are non-technical regarding climate change. These might influence how the model is built and the answers that are generated.<br><br><ins>Positive bias in the questions that are asked to the algorithm</ins>: we have introduced a positive bias in the questions asked to the model by providing a reformulation feature that softens the contents of the answers. For example, questions that read “Should we eat the rich?” were reformulated as “What are the most effective ways to reduce the carbon footprint of the wealthiest people in society?”.<br><br><p><strong>Limitations regarding the environmental impact of AI</strong>: training and using large AI models requires substantial amounts of resources. The progress driving large models has been achieved through increasingly large and computationally-intensive deep learning models and in general, improving accuracy of models is at the expense of economic, environmental, and social costs<sup id="fnref-17-8d0ebe"><a href="#fn-17-8d0ebe" class="footnote-ref">17</a></sup>. In order to evaluate the environmental impact of our model, we need indicators that measure the environmental impact of each module (given that we don’t train the model ourselves), and that can be aggregated for total. As the environmental impacts of AI depend on the local energy infrastructure, hardware used, running time, and the number of parameters, there are still few measures available to perform this task and inform users.</p><ins>Carbon footprint</ins>:<p>to estimate the carbon footprint of our model, we use CodeCarbon, a Python package that estimates the power consumption of hardware (GPU, CPU and RAM) and applies the carbon intensity of the region where the computing is done<sup id="fnref-18-8d0ebe"><a href="#fn-18-8d0ebe" class="footnote-ref">18</a></sup>. We estimate the following emissions for each phase of the project at Ekimetrics.</p><br><br><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Picture4-bccede718c7194ba56700afbae8b3210.png" width="742" height="219" class="img_ev3q"></p></div><p>*Link for <a href="https://medium.com/@chrispointon/the-carbon-footprint-of-chatgpt-e1bc14e4cc2a" target="_blank" rel="noopener noreferrer">The carbon footprint of ChatGPT</a></p><br><p>Carbon Emissions are <strong>relatively low but not negligible</strong> compared to other usages: one question asked to ClimateQ&amp;A is around 0.482gCO2e - equivalent to 2.2m by car and 1ml of water<sup id="fnref-19-8d0ebe"><a href="#fn-19-8d0ebe" class="footnote-ref">19</a></sup>.
In this calculation we do not include the carbon footprint of training ChatGPT (done by OpenAI) that has been estimated to 1,287MWh, corresponding to 552 tons of CO2 equivalent, which is similar to 1.2M liters of water and 2.5M km by car, 63x the Earth’s circumference. These calculations are estimates and should be considered carefully<sup id="fnref-20-8d0ebe"><a href="#fn-20-8d0ebe" class="footnote-ref">20</a></sup>.</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Picture5-9ff3cf64645e3d04670bd7a174b457e4.png" width="605" height="338" class="img_ev3q"></p></div></div><div class="footnotes"><hr><ol><li id="fn-1-8d0ebe">Lesley-Ann L. et Cole 2018<a href="#fnref-1-8d0ebe" class="footnote-backref">↩</a></li><li id="fn-2-8d0ebe">Oliver and Adkins 2020<a href="#fnref-2-8d0ebe" class="footnote-backref">↩</a></li><li id="fn-3-8d0ebe">Bell, et al. 2021<a href="#fnref-3-8d0ebe" class="footnote-backref">↩</a></li><li id="fn-4-8d0ebe">Lee, et al. 2015<a href="#fnref-4-8d0ebe" class="footnote-backref">↩</a></li><li id="fn-5-8d0ebe">Fake news threatens a climate literate world 2017)<a href="#fnref-5-8d0ebe" class="footnote-backref">↩</a></li><li id="fn-6-8d0ebe">Lejano 2019<a href="#fnref-6-8d0ebe" class="footnote-backref">↩</a></li><li id="fn-7-8d0ebe">Lynn, Araya, et al. 2016)<a href="#fnref-7-8d0ebe" class="footnote-backref">↩</a></li><li id="fn-8-8d0ebe">De Pryck and Hulme 2022<a href="#fnref-8-8d0ebe" class="footnote-backref">↩</a></li><li id="fn-9-8d0ebe">IPCC, Appendix A to the Principles Governing IPCC Work 2013<a href="#fnref-9-8d0ebe" class="footnote-backref">↩</a></li><li id="fn-10-8d0ebe">IPBES, Functions, operating principles and institutional arrangements of the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services 2012<a href="#fnref-10-8d0ebe" class="footnote-backref">↩</a></li><li id="fn-11-8d0ebe">IPCC, Organization n.d.<a href="#fnref-11-8d0ebe" class="footnote-backref">↩</a></li><li id="fn-12-8d0ebe">Havstad and Brown 2017<a href="#fnref-12-8d0ebe" class="footnote-backref">↩</a></li><li id="fn-13-8d0ebe">Havstad and Brown 2017<a href="#fnref-13-8d0ebe" class="footnote-backref">↩</a></li><li id="fn-14-8d0ebe">Broome 2014<a href="#fnref-14-8d0ebe" class="footnote-backref">↩</a></li><li id="fn-15-8d0ebe">Lynn and Peeva, Communications in the IPCC’s Sixth Assessment Report cycle 2021<a href="#fnref-15-8d0ebe" class="footnote-backref">↩</a></li><li id="fn-16-8d0ebe">Brown, et al. 2020<a href="#fnref-16-8d0ebe" class="footnote-backref">↩</a></li><li id="fn-17-8d0ebe">Schwartz, et al. 2019<a href="#fnref-17-8d0ebe" class="footnote-backref">↩</a></li><li id="fn-18-8d0ebe">BCG Gamma 2020<a href="#fnref-18-8d0ebe" class="footnote-backref">↩</a></li><li id="fn-19-8d0ebe">ADEME n.d.<a href="#fnref-19-8d0ebe" class="footnote-backref">↩</a></li><li id="fn-20-8d0ebe">Patterson, et al. 2022<a href="#fnref-20-8d0ebe" class="footnote-backref">↩</a></li></ol></div>]]></content:encoded>
            <category>Climate</category>
            <category>biodiversity</category>
            <category>generative AI</category>
            <category>genAI</category>
            <category>IPCC</category>
            <category>IPBES</category>
            <category>OCR</category>
        </item>
        <item>
            <title><![CDATA[Does your company level with your car in terms of Analytics?]]></title>
            <link>https://ekimetrics.github.io/blog/2023/03/07/level_car_analytics</link>
            <guid>https://ekimetrics.github.io/blog/2023/03/07/level_car_analytics</guid>
            <pubDate>Tue, 07 Mar 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[See what ABS and ESP do in a car and what it corresponds to in a company. This article advocates real-time analytics for ML insights. ]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/level_car_analytics-6c2b9d1a859a0af66c1a2fe9d626982a.jpg" width="2992" height="2000" class="img_ev3q"></p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h2><div align="justify"><p>Data Engineers at Ekimetrics use Spark to execute data ingestion and data preparation tasks, and Spark can process incoming data in batches as well as data in real-time streams. We also work on GCP and Cloud Dataflow, where Apache Beam provides the same flexibility to process streams as well as batches.</p><p>But IT teams ans Business teams are so used to batch analytics that it is hard to convey the value of real-time or up-to-date analytics, even though we now have platforms to handle the sources that can stream data. And with the emergence of micro-services, we also have more and more of these sources.</p><p>Based on current market evolutions and on our previsions:</p><ul><li><strong>Companies should create real-time views</strong> of customers, products, suppliers and production plans on an easy-to-grow platform with the adequate governance,</li><li><strong>Companies should assemble a real-time analytical view of their whole business</strong> to be able to steer it appropriately,</li></ul><p>Because this article is about real-time analytics, it starts with the definition of these terms. We use the example of driving a car as parallel for running a company. We all drive cars, we all see cars change, for example from “manual gearbox” to “automatic gearbox” to “no more gearbox”, and we all have a lot of knowledge about cars. Car technology will be used in this article to illustrate the following questions:</p><ul><li>What is the minimal data available to drive a car and what is the benefit of additional data ?</li><li>How to transpose for example the ABS of a car into a platform to help drive a company ?</li></ul><br><p>Below are the steps in this article, as they will be presented and explained: </p><div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/Global_Table_1-9067e1c766fae75a98d1cc3069117889.png" width="605" height="725" class="img_ev3q"></p></div><br></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="definitions">Definitions<a href="#definitions" class="hash-link" aria-label="Direct link to Definitions" title="Direct link to Definitions">​</a></h2><div align="justify"><p>Let’s first introduce definitions of the two topics of this article (independent of the car illustration):</p><p><strong>Analytics</strong>, as separated from <strong>Operational</strong>, is related to <strong>data</strong>:</p><ul><li><p><strong>Operational data</strong> is internal data from the business operations,</p></li><li><p><strong>Analytical data</strong> is operational data that is aligned, consolidated, reprocessed and possibly augmented with external data.</p><br></li></ul><p><strong>Real-time</strong>, as separated from <strong>Batch</strong>, is related to <strong>processing</strong>:</p><ul><li><p><strong>Batch processing</strong> is about processing data on a schedule, between every few hours and every month. Users gets an image that is always a bit old, but for many businesses and operations this is seen as acceptable,</p></li><li><p><strong>Real-time processing</strong> is about reflecting the current status of the operations. The latency depends of the source rather than the processing, it is between a few seconds and a few hours depending on the capability of the source system.</p></li></ul><p>When analyzing a business process, it is always a very interesting exercise to diagnose if a certain metric is operational or analytic. And if the data is analytic, the next question is to wonder if the business could benefit from having that metric in real-time.</p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="car-wise-lets-start-in-the-1950s">Car-wise, let’s start in the 1950s<a href="#car-wise-lets-start-in-the-1950s" class="hash-link" aria-label="Direct link to Car-wise, let’s start in the 1950s" title="Direct link to Car-wise, let’s start in the 1950s">​</a></h2><div align="justify"><p>I work in Paris, and tourists love to tour the city in Citroën 2CV, a French car that was produced between the 50s and the 80s, like the picture below from one of the numerous companies renting these cars.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Block_figures_1-b6db335c04027e9ef0a8beb8703d3b46.png" width="2492" height="649" class="img_ev3q"></p><p>No electronics in there, this is pure mechanics and a few electrical wires, an ideal starting point for our journey !</p><p>What data do you have to drive a Citroën 2CV? Speed, battery level, gas levels, total mileage, that’s it.
And the engine is so noisy that there is no need for an engine RPM meter !</p><p>According to the definitions above, the data is the direct readings of the mechanical operations of the car, for example from the speed of the wheels, so this is operational data.</p><p>No further processing here, so no analytical data at all.</p><br><p>Finally, let’s introduce this diagram of the car with no Analytics at all:</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Diagram_1-60f3d7b918b8194ac8b89e4cc869f4d0.png" width="945" height="158" class="img_ev3q"></p><br><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-does-driving-a-2cv-transpose-to-driving-a-company">How does driving a 2CV transpose to driving a company?<a href="#how-does-driving-a-2cv-transpose-to-driving-a-company" class="hash-link" aria-label="Direct link to How does driving a 2CV transpose to driving a company?" title="Direct link to How does driving a 2CV transpose to driving a company?">​</a></h3><p>When we transpose to driving a company, operational data is the data that resides in the operational systems, such a CRMs and ERPs. It contains all the referential and transactional information necessary to run the company, from HR to Sales to Finance. All reports and dashboards that are directly embedded in these applications or close to them are also considered operational data.</p><p>Since there is enough information to run the business, we can jump to our first conclusion:</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Diagram_2-2b6413adb9435c83218636d55314faac.png" width="1322" height="223" class="img_ev3q"></p><br></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="born-in-the-80s-and-mandatory-in-eu-in-2003-the-abs">Born in the 80s and mandatory in EU in 2003, the ABS<a href="#born-in-the-80s-and-mandatory-in-eu-in-2003-the-abs" class="hash-link" aria-label="Direct link to Born in the 80s and mandatory in EU in 2003, the ABS" title="Direct link to Born in the 80s and mandatory in EU in 2003, the ABS">​</a></h2><div align="justify"><p>The ABS, or Anti-lock Braking System, measures the speed of each wheel and reduces the pressure on the brakes to avoid the wheel locking up when losing grip.</p><p>To brake efficiently, it applies “threshold braking” and “cadence braking” that are techniques of the most skillful drivers, taken here at a much faster rate than what these drivers can perform.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Diagram_3-540eed6d65cce9d3f553a6ea992c35b7.png" width="1323" height="266" class="img_ev3q"></p><br><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-is-real-time-analytical-processing-of-operational-data-in-a-business">What is real-time analytical processing of operational data in a business?<a href="#what-is-real-time-analytical-processing-of-operational-data-in-a-business" class="hash-link" aria-label="Direct link to What is real-time analytical processing of operational data in a business?" title="Direct link to What is real-time analytical processing of operational data in a business?">​</a></h3><p>If we change the wheels of the cars by the BUs of a company, we get to a functional diagrams like the one below, where each BU includes efficient controls applied in real-time on the data from the operations:</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Diagram_4-1fbf05bf4351f9e1586c4c3fb5f7f47a.png" width="1323" height="238" class="img_ev3q"></p><br><p>Data Science is applied locally on the operational data, for purposes like segmentation, scoring, pricing and other applications of Machine Learning. The purpose here is to get value from investments in each BU, <strong>but there are no insights to make the whole bigger than the sum of its parts.</strong> It is like in the example of the ABS, we want to replicate the techniques of the best players, locally in each domain.</p><p>It is important to note that local AI initiatives do not require a dedicated platform for analytical data and that improving the operations locally can be achieved in the operational applications. For example, SAP and Salesforce include more and more predictive features. And since these platform work on up-to-date information, <strong>embedding Machine Learning in operational systems is the closest to what ABS is in a car.</strong></p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="in-2014-the-esp-becomes-mandatory-in-all-european-cars">In 2014, the ESP becomes mandatory in all European cars<a href="#in-2014-the-esp-becomes-mandatory-in-all-european-cars" class="hash-link" aria-label="Direct link to In 2014, the ESP becomes mandatory in all European cars" title="Direct link to In 2014, the ESP becomes mandatory in all European cars">​</a></h2><div align="justify"><p>The ESP (Electronic Stability Program) is an anti-skid system. It leverages four wheel-speed sensors, the steering angle sensor and an inertial measurement unit to act on the break pressure of each wheel and engine acceleration command. Depending on the situation, it can reduce the speed of the engine and brake all wheels or activate the brake on only one wheel only to alter the heading of the car.</p><p>The diagram changes, since all insights are now consolidated before they are processed:</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Diagram_5-c8a535f563ec37ed3f13c1fff4522610.png" width="1323" height="278" class="img_ev3q"></p><br><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-does-an-esp-looks-like-in-a-company">What does an ESP looks like in a company?<a href="#what-does-an-esp-looks-like-in-a-company" class="hash-link" aria-label="Direct link to What does an ESP looks like in a company?" title="Direct link to What does an ESP looks like in a company?">​</a></h3><p>In your company, we are leaving the territory of ERPs, because even SAP does not cover all aspects that must be monitored and given sense together.<br>
<!-- -->In addition, ERPs have many limitations regarding Advanced Analytics:</p><ul><li>ERPs are complex to query and it may impact production,</li><li>ERPs don’t need and don’t keep history,</li><li>ERPs don’t care about weak signals and don’t make sense of unstructured data,</li><li>ERPs are not designed to consolidate data with other ERPs or other operational platforms,</li><li>And lastly, when a change is interesting, it takes ages to impact ERPs.</li></ul><br><p>Because consolidation cannot occur in the operational systems, <strong>we introduce an additional data storage and processing layer called the analytical data plane</strong> . We assemble data platforms that can host data assets of different Business Units, with some local autonomy and some global governance, so insights can be derived from many elements. This is where the Lakehouse and the Federated Governance patterns apply. </p><p>This is also where we believe we must now work in real-time, like in the ESP in the car analogy. The driving force is that <strong>the analytical plane should be up-to-date with the data that is shared from the operational systems</strong>, it should not add an hourly or a daily delay between the times it processes data.</p><p>Most use-cases that we currently in batch will benefit from being executed with up-to-date data from the operations. Let’s take a two examples:</p><ul><li><p>The purpose of a Customer 360° use-case is to consolidate Transaction history, Social media presence, Customer support interactions, Segmentation history, Sensitivity to the promotions and prices, etc. into a single view of the customers. </p><p>Having that information always up-to-date could let you, for example:</p><ul><li>Target, or restrain from targeting, in certain situations,</li><li>Send messages at the right moment,</li><li>Propose Next Best Action when the customer calls,</li><li>etc.</li></ul></li><li><p>Expanding on that up-to-date Customer 360 and adding up-to-date supply-chain information and up-to-date suppliers information will enable new use-cases to better serve the customers needs.</p></li></ul><p>In addition to use cases, and if we consider a company as a complex system, <strong>it is important to monitor all its moving parts together to better understand its reactions.</strong> And when there are days or weeks between an action and the monitoring of its effect, it is more difficult to steer.</p><p>For these reasons, we want to propose real-time as the new standard, today: </p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Diagram_6-211461493b954a49a017803de5ebd7e2.png" width="1324" height="237" class="img_ev3q"></p><br></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="mandatory-in-2022-aeb-is-the-current-state-of-the-art">Mandatory in 2022, AEB is the current state-of-the-art<a href="#mandatory-in-2022-aeb-is-the-current-state-of-the-art" class="hash-link" aria-label="Direct link to Mandatory in 2022, AEB is the current state-of-the-art" title="Direct link to Mandatory in 2022, AEB is the current state-of-the-art">​</a></h2><div align="justify"><p>In addition to sensors that focus on the inside of the car, AEB (Automatic Emergency Braking) uses sensors that look outside of the car, like Front radar sensor, multi-purpose dashboard camera(s) and corner radar sensors. All these components embark object detection and tracking with deep neural networks and share their insights to a central unit that can act on the car brakes. </p><p>This is the most complex system that we study here and we can anticipate that the next step will be cars exchanging information together about conditions of the road. In the projects we currently participate in on connected cars, no data is currently shared between cars.</p><p>So our last diagram is the most complete:</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Diagram_7-75a42aaebbe8167b49183711dd738429.png" width="1325" height="321" class="img_ev3q"></p><br><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="do-you-have-aeb-like-system-in-your-company">Do you have AEB-like system in your company?<a href="#do-you-have-aeb-like-system-in-your-company" class="hash-link" aria-label="Direct link to Do you have AEB-like system in your company?" title="Direct link to Do you have AEB-like system in your company?">​</a></h3><p>AEB is about adding data sources to better anticipate market conditions, to better understand your customers, to better interact with your suppliers and your distributors, to gather data about your competitors, etc.</p><p>And by now you have understood that in the context of the AEB, the purpose is to consolidate these insights in real-time and leverage these to take the best decisions.</p><p>While buying or trading data is good, sharing data on platforms like Snowflake, BigQuery or Databricks Delta Sharing is better, because it is maintained in real-time by the other parties and you don’t have to replicate it in batches.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Diagram_8-4c937e00cd3427775844bd983709534d.png" width="1324" height="280" class="img_ev3q"></p><br></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><div align="justify"><p>While the focus of this article has been on the value of up-to-date analytics, we’ll close this article with a preview of a next topic : the costs of the streaming datasets compared to traditional batch approaches.</p><p>Ekimetrics is investing a lot in making sure we manage the environmental aspects of the solutions we build, so this last point is critical! We currently work with Spark streams that have the .trigger(availableNow=True) option so they only process the newly arrived rows when the pipelines are executed every hour for example.<br>
<!-- -->So that it remains cost- and emission- efficient.</p><p>And as a closing remark, as I was looking for the dashboards of the Renault 4L, Peugeot 205 and other 2CV, I found that in the latest small car from Citroën, called the AMI, it is nearly as frugal as its predecessors ! </p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Block_figures_2-36f628f6186899118cc13dfe1dd18e5b.png" width="2451" height="650" class="img_ev3q"></p><p>And it weighs only 500kg, which is frugal too.</p></div>]]></content:encoded>
            <category>Thought Leadership</category>
            <category>Data Processing</category>
            <category>Real-Time</category>
            <category>Analytics</category>
        </item>
        <item>
            <title><![CDATA[Building a datalake - Part 2 - Smart storage & computing strategies for better usability and usefulness]]></title>
            <link>https://ekimetrics.github.io/blog/2023/02/28/building_datalake_part_2</link>
            <guid>https://ekimetrics.github.io/blog/2023/02/28/building_datalake_part_2</guid>
            <pubDate>Tue, 28 Feb 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[For this second part of datalake building, we’ll go deeper into the journey of data, more specifically expand on storage and compute strategies.]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/overview_datalake_part_2_v2-2239a71ec2a5454c6e79ec8ed9c3ad48.png" width="5464" height="3640" class="img_ev3q"></p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="article-scope">Article Scope<a href="#article-scope" class="hash-link" aria-label="Direct link to Article Scope" title="Direct link to Article Scope">​</a></h2><div align="justify"><p>In a previous article - <a href="https://ekimetrics.github.io/blog/2022/02/07/building_datalake_part_1/" target="_blank" rel="noopener noreferrer">Building a datalake - Part 1 - Usable, Useful, Used, or how to avoid dataswamp and empty shell traps | Eki.Lab</a> - we took a look at the foundation architecture used at Ekimetrics when building a datalake. Its focus was to present design elements to ensure your datalake is useful and usable at its core, as well as best practices to avoid falling into the so-called data swamp and empty shell traps.</p><p>For this second part, we’ll go deeper into the journey of data, more specifically expand on storage and compute strategies, and see how the organisation of data and the way it is transformed impact a datalake’s usability &amp; usefuleness.</p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h2><div align="justify"><p>The overview of data’s journey through a datalake or data platform can be broken down into five steps, represented below from left to right.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/img1_data_journey-b88b0c7ad94bd8652615a4bdb7ae128a.png" width="1803" height="693" class="img_ev3q"></p><div align="center"> Data’s journey, from source to usage</div><br><p>A data platform is highly versatile in providing technical options the five steps above. Getting your data from one step to the next means applying a strategy for how the data input is stored, how it is processed and how the output is exposed for the next step.</p><p>These strategies will vary depending on the use case and platform, taking business as well as technical constraints in consideration. Designing your datalake’s strategies to be versatile and homogeneous is essential. It allows your data platform to grow fast, in terms of data content as well as use case possibilities. It also ensures that the datalake is under control with a common way of treating data, where its only varying specifities are the entry point (data sources) and output (serving layer). </p><p>At Ekimetrics, we’ve developed versatile strategies that are applicable to most common use cases, easily reproducible. These strategies help build new capabilities and provide a better understanding of your data platform.</p></div><br><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="where-designing-storage--compute-strategies-really-matters">Where designing storage &amp; compute strategies really matters<a href="#where-designing-storage--compute-strategies-really-matters" class="hash-link" aria-label="Direct link to Where designing storage &amp; compute strategies really matters" title="Direct link to Where designing storage &amp; compute strategies really matters">​</a></h2><div align="justify"><p>In most datalake architectures, the data journey’s step where the most impactful design decisions can be made is <em>Data storage and processing</em>. The other steps are more straightforward:</p><ul><li>Data sources are usually out of the architect or data engineer’s control, as they often sit outside of the data platform (e.g. on a third-party server).</li><li>Ingestion is a step for which design questions around data validation &amp; organization processes may be worth considering. We’ve talked about these processes in the first part of our “Building a datalake” articles. As for streaming vs batch, it is only dependent on the source: if the source system is streaming data, a resource to ingest that is necessary ; otherwise, recurrent batch ingestion is the go-to.</li><li>Serving will depend on the target use case, so even if there are important design decisions to make, they will only be impactful in the scope of their use case, not for the whole datalake.</li><li>Usage will most of the time be outside the datalake and depend on the use case. The few design decisions that may be necessary here won’t be as impactful to the datalake’s usability and usefulness either.</li></ul><p>Of course, this is not to say that designing relevant strategies and architecture for these steps doesn’t matter: they must be tailored to the business case and technical constraints.</p><p>On the flipside, the <em>Data storage and processing</em> step is the central piece in the datalake puzzle. It is where storage and compute strategies will be the most impactful. </p><p>What could it look like, then ? Inside this central step, data transits through four zones, from its raw form to fully processed for a particular use case, ready to serve to your businesses. These four zones are detailed below, in between ingestion and serving.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/img2_data_storage-905c99d1d5a0c6e5421ba02084e2063e.png" width="1803" height="500" class="img_ev3q"></p><div align="center"> Data storage and processing - storage zones</div><br><p>These zones can be found under various names: Landing - Bronze - Silver - Gold, or Temp - Raw - Cleaned - Conformed, etc. The intent is the same, where data becomes more and more usable and business use case oriented with each zone.</p><p>In between each storage zone, organization and transformation processes are applied to organize and extract insights out of data. This is where our storage and computing strategies come in.</p></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="going-from-source-to-raw-storage-strategy">Going from source to raw: storage strategy<a href="#going-from-source-to-raw-storage-strategy" class="hash-link" aria-label="Direct link to Going from source to raw: storage strategy" title="Direct link to Going from source to raw: storage strategy">​</a></h3><div align="justify"><p>An ingested source will usually be exposed in one of two possible ways: </p><ul><li>Incremental changes, where only what is new or updated is exposed.</li><li>Full datasets, where all of the up-to-date data (or a new timeframe of data) is exposed.</li></ul><p>In these two cases, the ingestion strategy we recommend results in the same outcome. Here the strategy is to historize all received data, adding metadata about its reception date (or validity date) through organization in the storage architecture or in the dataset itself. The RAW storage zone becomes a source of historical knowledge about all data points and their changes through time. Doing this, we’re also able to add a “slowly changing dimension” / “change data capture” aspect to our data, where we can easily find a data point’s values at any moment in time.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/img3_computing_1-cef3f81042eb0c848b50f0d4c4451404.png" width="1803" height="1297" class="img_ev3q"></p><div align="center"> Fig. 1: Computing &amp; storage strategies in between Landing and Raw zones</div><br><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_S0QG"><p>There are limitations to historizing all received data, notably storage costs: this is where the data’s lifecycle must be discussed. Keeping all older versions of a single dataset may start to become expensive as the data piles up, outweighing the pros if only the latest values are used.</p><p>In this case, a solution can be to keep only the latest values of the source’s data points, or just one version of the source every N periods on top of the latest ones. We can then archive older and unused versions in cold, less costly storage resources. The archived data enters a different lifecycle, where it could be removed at some point in the future if it doesn’t serve any business purpose.</p></div></div><p>The “historize everything” strategy ensures RAW storage is the most useful it can be: use case agnostic, your businesses can use and explore the data at its full potential. It also helps unify the way RAW storage is meant to be read by your later processes, improving usability while allowing for costs optimization without sacrificing the underlying principle.</p></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="going-from-raw-input-to-refined-output-computing-strategy">Going from raw input to refined output: computing strategy<a href="#going-from-raw-input-to-refined-output-computing-strategy" class="hash-link" aria-label="Direct link to Going from raw input to refined output: computing strategy" title="Direct link to Going from raw input to refined output: computing strategy">​</a></h3><div align="justify"><p>By applying this highly inclusive storage strategy for our RAW storage, we’re then able to fetch the necessary data for our business cases, whether it’s using the full history of a source, its latest version or the latest changes only.</p><p>The most common computing strategy will be to generate an up-to-date view of the data: at the time of computing, what are the insights’ state ? We’re also able to use older data to track changes and generate insights from these.</p><p>Refined data can then be exposed in two ways, acting as the source for another system: </p><ul><li>Exposing the full dataset, where all data is up-to-date</li><li>Exposing incremental changes, where only what is new or updated is sent to the serving layer</li></ul><p><img loading="lazy" alt="screenshot-app" src="/assets/images/img4_computing_2-dcc7a76dc859f9bae73bb079498fd3eb.png" width="1803" height="1561" class="img_ev3q"></p><div align="center"> Fig. 2: computing and storage strategies in between Raw, Trusted and Refined zones</div><br><p>For some use cases, you can generate “frozen in time” views of the data, only updating the current timeframe’s view. For instance, we could update the current month’s exposed insights each day, then stop updating it at the end of the month, writing a new one for the next month’s computed insights, and so on.</p><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_S0QG"><p>Historizing our transformations and results is useful for debugging and business exploration, but iterations may start to pile up, so defining a lifecycle for Trusted (TRD) and Refined (RFD) storage is important as well: do our technical processes or business use cases need all the iterations history stored in trusted TRD ? Can our business use cases work with a simplification of RFD storage, similar to the Delta historization for RAW zone in figure 1 ? </p></div></div><p>You may have noticed, our exposed Refined storage looks strangely similar to what we had in the data source: we’re exposing either the full up-to-date version of our insights, or just updates and new ones, just like our sources. The datalake is now a source for your business’ use cases, so it makes sense that it would be able to expose data in a similar way.</p><p>The “up-to-date view” strategy is highly useful for most use cases, and through smart use of historization, still allows for your businesses to get insights on the data’s evolution through time. It also ensures versatility in making your datalake a usable source for other systems when exposing data.</p><p>From there, the serving layer can leverage this source in a wide range of solutions, be it a database, reporting, CRM, AI models, etc. </p></div><br><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><div align="justify">In essence, storage &amp; compute strategies can be sumarized in three questions:<ul><li>Why are these strategies necessary: to ensure versatility for businesses and technical processes, all the while improving usability.</li><li>How do they do that: by capturing and organizing data’s history efficiently, unifying the way we look at data while allowing for versatility in its usage.</li><li>What they mean: a unified, highly useful and usable storage, and computing that help the datalake become the source for a wide range of systems and use cases.</li></ul><p>As data engineers and architects, we’re always looking for ways to improve our data products. In the context of building a datalake, this means finding ways to make data easy to find, explain and extract insights from. The strategies we’ve talked are key answers to these challenges ; as we use them to grow data platforms and apply them to new use cases, these storage &amp; compute strategies have proved themselves to be highly useful for other data challenges, providing new capabilities and solutions for our clients.</p></div>]]></content:encoded>
            <category>Datalake</category>
            <category>Data Engineering</category>
            <category>Architecture</category>
            <category>Data Governance</category>
            <category>Data Mesh</category>
        </item>
        <item>
            <title><![CDATA[Exploring the links between creative execution and marketing effectiveness - Part V: Key Paths to Success and Common Pitfalls to Avoid]]></title>
            <link>https://ekimetrics.github.io/blog/2023/02/21/creative_execution_and_marketing_effectiveness_part_V</link>
            <guid>https://ekimetrics.github.io/blog/2023/02/21/creative_execution_and_marketing_effectiveness_part_V</guid>
            <pubDate>Tue, 21 Feb 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[ In this last part, we outline the key learnings from this project, including key paths to success and common pitfalls to avoid.]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/Eki_Meta_part_V-cb6cceedb9c505b6b88b6f86ad0ca88e.png" width="3041" height="4055" class="img_ev3q"></p></div><div align="justify"><p>This article is the last part of a set of five technical articles that accompany a <a href="https://ekimetrics.com/news-and-events/exploring-the-links-between-creative-execution-and-marketing-effectiveness-exclusivepreview" target="_blank" rel="noopener noreferrer">whitepaper</a> written in collaboration between Meta and Ekimetrics. Object Detection (OD) and Optical Character Recognition (OCR) were used to detect specific features in creative images, such as faces, smiles, text, brand logos, etc. Then, in combination with impressions data, marketing mix models were used to investigate what objects, or combinations of objects in creative images in marketing campaigns, drive higher ROIs.
In this last part we share with the reader common pitfalls to avoid when utilising tools for Object Detection, Optical Character Recognition and MMM, as well as things to keep in mind, that will facilitate the process.</p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="common-pitfalls-to-avoid">Common Pitfalls to Avoid<a href="#common-pitfalls-to-avoid" class="hash-link" aria-label="Direct link to Common Pitfalls to Avoid" title="Direct link to Common Pitfalls to Avoid">​</a></h2><div align="justify"><ul><li><p><strong>Failing to Define Labels at the Start:</strong> Define the set of labels that are going to be studied at the very beginning. Adding more labels in the middle of the study would involve having to go back to time consuming tasks, such as manual labelling and repeated extraction and processing of labels.</p></li><li><p><strong>Ambiguous Object Definitions:</strong> Describe clearly from the start what each label is (e.g., “Person” can be any body part, not just a whole body with a face). This is helpful when the manual labelling is being done as a team, rather than by one person. Furthermore, if using a pre-trained model, ensure that your definition of the object aligns with what is detected by the model. For example, you may define “Car” as just the outside of the car while the OD model is trained to detect both the interior and exterior of cars.</p></li><li><p><strong>Non-Generalizable Labels:</strong> If you are studying two separate sub-brands within one brand, it is advisable to have two separate studies, rather than one. That is, instead of defining objects “Brand A Logo” and “Brand B Logo”, it may be better to separate the brands into different streams and have the same object labels for both (e.g., logo, brand cue, product and person). This will ensure that you code is reusable for studies of brands that have different numbers of sub-brands.</p></li><li><p><strong>Lazy Manual Labelling:</strong> Make sure to manually label all objects in a creative. For example, if there are three cars, label all of them, not just one. The manually labelled validation set is the ground truth against which a model’s performance is compared. If some objects are missed, you may have strange performance results that indicate that the model may be over-detecting objects.</p></li><li><p><strong>Trying to be Exhaustive:</strong> Avoid testing all open-source resources available, as this can be very time-consuming and not very fruitful. Choose two or three to test and instead spend more time on what you can do to improve their performance on your particular dataset. This could, for example, be done through hyperparameter tuning (e.g., testing different learning rates, batch sizes, confidence thresholds, etc.) or in the processing of the results (e.g., correcting any text labels inside logos or products).</p></li><li><p><strong>Lack of Checkpoints:</strong> Due to the many different data manipulation steps in this project, there are a lot of potential sources for errors. The key is to set up automatic checks at each stage to avoid a trickle down effect of avoidable errors. For example, removing false positives in face detection by only ‘accepting’ a detected face if a person was also detected in the creative will ensure that the feature time series that is used for MMM does not suddenly have more impressions for faces than for people. Similarly, when doing the feature engineering, employing a simple method of checking that there are no negative values, no missing data, and that the impressions and spend data is consistent across each sub-model will ensure that time is not wasted in the MMM stage from modelling with incorrect data. </p></li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="key-paths-to-success">Key Paths to Success<a href="#key-paths-to-success" class="hash-link" aria-label="Direct link to Key Paths to Success" title="Direct link to Key Paths to Success">​</a></h2><p>There are various components that can contribute to successfully executing this type of study, ranging from technical requirements to general strategies. Some of the key components are outlined below. </p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="integrated-efficient-tools">Integrated, Efficient Tools<a href="#integrated-efficient-tools" class="hash-link" aria-label="Direct link to Integrated, Efficient Tools" title="Direct link to Integrated, Efficient Tools">​</a></h3><p>Choosing the right tools can make or break a project of this complexity. Choose a platform which offers robust functionalities to help streamline and facilitate work. This study used Azure, but GCP is another great alternative. The key features that are required are:</p><ul><li><strong>Cloud storage:</strong> Due to the volume of creatives included in this study, cloud storage was crucial. Particularly if extracting frames from videos, it is important to account for the significant volume of additional images that need to be stored. This project used Azure Storage. </li><li><strong>Labelling Software:</strong> The labelling process is very manual and time-consuming. Having an intuitive software that can import creatives directly from the cloud storage saves a lot of time and effort and avoids unnecessary duplication of images. Furthermore, the software should be able to export the labels as a JSON file which can then later be converted to the required format (e.g., COCO for Detectron2). For this project, the Azure Machine Learning Studio Data Labelling functionality was used. </li><li><strong>External Computation Resources:</strong> As the training, validation and final labelling of images are all computationally expensive processes, the use of external computation resources (clusters) is recommended. The configuration of the clusters can vary depending on the task at hand. For pre-processing and feature engineering, individual CPU-enabled single-node clusters are sufficient. On the other hand, for the training, validation, and labelling processes, it is recommended to use GPU-enabled clusters. While GPUs are a more expensive resources than CPUs, the efficiency gains may make up for the additional cost per hour. For this project, Databricks was used as it can connect to Azure storage, facilitates the use of clusters, supports various programming languages, and allows for collaboration on Notebooks.</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="contextual-knowledge">Contextual Knowledge<a href="#contextual-knowledge" class="hash-link" aria-label="Direct link to Contextual Knowledge" title="Direct link to Contextual Knowledge">​</a></h3><p>Having a strong understanding of the brand acts as a crucial foundation for every step of this project. It is not only required for making informed decisions regarding which objects should be detected but also vital for defining the features to be measured in the MMM. For example, knowing that products often appear in creatives alongside just a hand raises the question of whether this is the most effective use of a person, or if including the person’s face would be more effective; this in turn leads to the creation of features testing products alongside ‘face-less’ people vs. people with faces. Contextual knowledge can also be gained throughout the project by stopping to analyse the data. For example, checking the distributions of manually labelled objects can give an early indication of performance of custom-trained models (feasibility for successful training and detection) as well as the expected impact for regression models.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="strong-base-models">Strong Base Models<a href="#strong-base-models" class="hash-link" aria-label="Direct link to Strong Base Models" title="Direct link to Strong Base Models">​</a></h3><p>Having strong base models is also key for success in this project. Since the target variable of the sub-models is determined by the contribution of the Meta variables in the base model, a poor base model will directly impact the performance of the sub-model. The quality of the base model will largely depend on the dataset used, so ensuring that sufficient, relevant, and good quality data relating to the baseline, market variations, and marketing activity is crucial. </p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="programmatic-sub-modelling">Programmatic Sub-Modelling<a href="#programmatic-sub-modelling" class="hash-link" aria-label="Direct link to Programmatic Sub-Modelling" title="Direct link to Programmatic Sub-Modelling">​</a></h3><p>Depending on the number of feature groups, KPIs and sub-brands included in the study, it may be infeasible to run the sub-models on a one-by-one basis. For context, this project had 156 sub-models (13 base models x 12 feature groups). For that reason, it is recommended to create a methodology that allows for the programmatic creation of sub-models. </p></div>]]></content:encoded>
            <category>Object Detection</category>
            <category>Optical Character Recognition</category>
            <category>Marketing Mix Modelling</category>
            <category>Deep Learning</category>
            <category>Tesseract</category>
        </item>
        <item>
            <title><![CDATA[Exploring the links between creative execution and marketing effectiveness - Part IV: MMM for Creative Marketing Effectiveness]]></title>
            <link>https://ekimetrics.github.io/blog/2023/01/15/creative_execution_and_marketing_effectiveness_part_IV</link>
            <guid>https://ekimetrics.github.io/blog/2023/01/15/creative_execution_and_marketing_effectiveness_part_IV</guid>
            <pubDate>Sun, 15 Jan 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[In this Part IV we explore the MMM methodology applied.]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/Eki_Meta_part_IV-3e729e3e394461861d5daf47f49da2cc.png" width="5906" height="3938" class="img_ev3q"></p></div><div align="justify"><p>This article is <strong>Part IV</strong> of a set of five technical articles that accompany a <a href="https://ekimetrics.com/news-and-events/exploring-the-links-between-creative-execution-and-marketing-effectiveness-exclusivepreview" target="_blank" rel="noopener noreferrer">whitepaper</a> written in collaboration between Meta and Ekimetrics. Object Detection (OD) and Optical Character Recognition (OCR) were used to detect specific features in creative images, such as faces, smiles, text, brand logos, etc. Then, in combination with impressions data, marketing mix models were used to investigate what objects, or combinations of objects in creative images in marketing campaigns, drive higher ROIs.
In this Part IV we explore the MMM methodology applied.</p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="why-you-should-read-this">Why you should read this<a href="#why-you-should-read-this" class="hash-link" aria-label="Direct link to Why you should read this" title="Direct link to Why you should read this">​</a></h2><div align="justify">This article is mostly directed to marketing measurers. Here you will learn that to improve accuracy in your measurement you must treat creatives differently. We provide the MMM methodlogy to reproduce this type of analysis for your brands.</div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="mmm-process">MMM Process<a href="#mmm-process" class="hash-link" aria-label="Direct link to MMM Process" title="Direct link to MMM Process">​</a></h2><div align="justify">An optimizsed two-staged modelling approach was applied to existing MMM models developed by Ekimetrics for a range of brands, products and KPIs. First, the base models were optimizsed, and second, sub-models were created to explain the variation in Meta contribution using the creative features. Bayesian optimization was used in both steps for variable transformation in relation to lag, adstock and saturation.</div><p>&nbsp;</p><div align="center"><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/1-422062c25769c3e19657d659ef85db70.png" width="602" height="161" class="img_ev3q"></p><p>Modelling Workflow</p></div><br><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="mmm-model">MMM Model<a href="#mmm-model" class="hash-link" aria-label="Direct link to MMM Model" title="Direct link to MMM Model">​</a></h2><div align="justify">The MMM models used in this analysis are multilinear regressions accounting for all significant factors driving the KPI (most commonly sales), including digital social. The selection of these models was based on the criteria of model stability (high R2, no autocorrelation, controlled heteroskedasticity) and enough variation in social activity (spend and impressions threshold of 5% per object). Some of the modelling periods were cut to account for sparse data, low correlation between creative impressions and modelled impressions or low variation. Therefore, a trusted relationship from social to the KPI of each MMM was established but could be enhanced by choosing more accurate parameterization of Meta activity through MMM model optimisation. Table 1 details the models included in the study.</div><p>&nbsp;</p><div align="center"><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/2-de515ac3a725e961be84cc9fdd00e0e6.png" width="820" height="520" class="img_ev3q"></p><p>Table 1: Specifications of MMM models selected Detail of Models selected</p></div><br><div align="justify">Once the MMM models were optimized, we isolated the impact of Meta on the KPI according to the following relationship:<p>&nbsp;</p><div align="center"><p><strong><em>Meta Contribution to KPI = Coefficient of Meta Variable ×Transformed Meta Variable</em></strong></p></div><p>The Meta contribution became the dependent variable of the Sub-Model, along with the constant and residuals. The object detection impressions timeseries served as the explanatory variables. The objective of this was to understand the impact of different feature splits into the overall Meta performance, allowing room for movement through the constant and residuals. </p><p>A sub-modelling approach allowed for the object detection features to have indirect effects on the KPI, through the Meta variable. Bayesian Optimisation Methods were employed to find the optimal transformations for each variable. </p><p>Each Sub-model is a linear regression, testing the impact of the appearance of a feature, or feature group, and its opposite (partner feature) against the total Meta performance. Since the features are not mutually exclusive (between 48%-66% of creatives contained at least two objects), they could not all be tested in the same model. Furthermore, the daily impressions and spend per feature were split according to creative type. That is, the impressions associated with a feature or combination of features, e.g., Person &amp; Product, were split out by static, video and carousel. While it was not in the scope of the study, doing this allowed for further analysis into the impact of features by creative type. </p><p>A sequential approach was followed in which each feature group was tested in isolation. Within each sub-model, the transformations of the features (lag, adstock, and diminishing returns) which maximized R2, and minimized p-values were chosen. The transformations that the Meta features follow were bounded to the following ranges: lag of 0-3, adstock of 0-50, saturation (K; S) of 0.1-0.8; 1-9. </p><p>An example of sub-model is shown below.</p><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/9-8150076ab761a3040c0b9e6c5abb159a.png" width="1105" height="28" class="img_ev3q"></p><p>Where:</p><div align="left"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/10-de15f137482874e0d093b56bdaf5a4d4.png" width="732" height="28" class="img_ev3q"></p></div><p>  <img loading="lazy" alt="screenshot-app" src="/assets/images/11-9637738e56b4324a656393a934af1b63.png" width="810" height="27" class="img_ev3q"></p><p>Same for Video (V) and Carousel (C)</p><p>One major challenge with the approach of testing the appearance of a feature or a set of features against its partner feature (e.g., impressions of creatives where a Brand Cue appears vs impressions of creatives where a Brand Cue does not appear), is that the partner feature is indirectly testing the appearance of other features or no features at all.
To manage the risk of results inaccuracy we made sure that:</p></div><ol><li>Both the main feature and the partner feature in the sub-models had to be statistically significant at a 10% level. For example, if the appearance of a feature is significant but the partner is not, the results of that regression were not used.</li><li>The percent of Impressions of the feature being tested had to be &gt; 5%</li></ol><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="roi-calculations">ROI Calculations<a href="#roi-calculations" class="hash-link" aria-label="Direct link to ROI Calculations" title="Direct link to ROI Calculations">​</a></h2><div align="justify">The last step of the analysis centred around calculating ROIs and deriving ROI uplifts for each feature per brand, using the feature contributions from the sub-models and the associated spend. This involved the following steps:</div><ol><li>Calculate the ROI of each feature (and partnering feature/s) using the contribution and the spend associated with that feature. Because the features were modelled by creative type (static, carousel, and video), the results were weighted by the percent of impressions each type represented over the total of the feature.</li></ol><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/3-a399c32b962f0021e561541ca80b6276.png" width="602" height="103" class="img_ev3q"></p></div><p>&nbsp;</p><ol start="2"><li>Calculate the ROI of the partnering variable (e.g., No Product) </li><li>Calculate the Uplift:</li></ol><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/4-6b729aff7daaed123d066206e7474d43.png" width="602" height="53" class="img_ev3q"></p></div><p>&nbsp;</p><ol start="4"><li>Index and rank ROI uplifts
a.	ROI uplifts are standardized, so they are comparable across brands
b.	They are then ranked so that #1 is the feature with the greatest uplift (maximum difference between the feature appearing and not appearing).</li></ol><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="results">Results<a href="#results" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results">​</a></h2><div align="justify"><p>The results can be found in the   <a href="https://ekimetrics.com/news-and-events/exploring-the-links-between-creative-execution-and-marketing-effectiveness-exclusivepreview" target="_blank" rel="noopener noreferrer">whitepaper.</a></p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="next-article">Next article<a href="#next-article" class="hash-link" aria-label="Direct link to Next article" title="Direct link to Next article">​</a></h2><p> In the next article, we outline the key learnings from this project, including key paths to success and common pitfalls to avoid.</p>]]></content:encoded>
            <category>Object Detection</category>
            <category>Optical Character Recognition</category>
            <category>Marketing Mix Modelling</category>
            <category>Deep Learning</category>
            <category>Tesseract</category>
        </item>
        <item>
            <title><![CDATA[Exploring the links between creative execution and marketing effectiveness - Part III: Tesseract Pre-Trained Optical Character Recognition Models]]></title>
            <link>https://ekimetrics.github.io/blog/2022/12/13/creative_execution_and_marketing_effectiveness_part_III</link>
            <guid>https://ekimetrics.github.io/blog/2022/12/13/creative_execution_and_marketing_effectiveness_part_III</guid>
            <pubDate>Tue, 13 Dec 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[In this Part III we explore the methodology for using Tesseract to detect text in creative images.]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/Eki_Meta_part_III-7d188e074342397f2ddb6fb21a645ae7.png" width="4095" height="2726" class="img_ev3q"></p></div><div align="justify"><p>This article is <strong>Part III</strong> of a set of five technical articles that accompany a <a href="https://ekimetrics.com/news-and-events/exploring-the-links-between-creative-execution-and-marketing-effectiveness-exclusivepreview" target="_blank" rel="noopener noreferrer">whitepaper</a> written in collaboration between Meta and Ekimetrics. Object Detection (OD) and Optical Character Recognition (OCR) were used to detect specific features in creative images, such as faces, smiles, text, brand logos, etc. Then, in combination with impressions data, marketing mix models were used to investigate what objects, or combinations of objects in creative images in marketing campaigns, drive higher ROIs.
In this Part III we explore the methodology for using Tesseract to detect text in creative images.</p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="detecting-text-with-tesseract">Detecting Text with Tesseract<a href="#detecting-text-with-tesseract" class="hash-link" aria-label="Direct link to Detecting Text with Tesseract" title="Direct link to Detecting Text with Tesseract">​</a></h2><div align="justify">Tesseract is an open-source optical character recognition (OCR) Engine that allows for the recognition of text characters within a digital image. It is an open-source resource, originally developed by Hewlett-Packard, and now managed by Google. This package does not have any parameters to optimise, but, as is exposed here, the basic performance of this resource can be improved with a combination of image processing and detected text correction.<p>In this work, Tesseract was used to detect text in all images, in a process outlined in Figure 1. Performance of this detection tool was done using Confusion Matrices, to gain insight not only into the Accuracy, but also other metrics such as the True Positive and True Negative Rates.  </p></div><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/10-dc93d16eaa104bf23fe228afaf7bd6a2.PNG" width="673" height="265" class="img_ev3q"></p><p>Figure 1 - Three stage process by which the basic performance (accuracy) of Tesseract on the original images was improved by up to 28% points.</p></div><br><div align="justify">To start with, the functionality was used on the original images to measure a baseline for performance on our sets of images. From this first step, we derived the following learnings:<ul><li>For general Text (both non-promotional Text and Promotional Text), the Accuracy is 69%, the True Positive Rate is around 65%, the False Positive Rate is about 35%.</li><li>Tesseract does not recognise symbols such as % (which may indicate promotional text) accurately.</li><li>Tesseract does not perform well on images that have a busy background.</li><li>Tesseract does not recognise slanted Text.</li></ul><p>Based on the low total volume of Promotional Text in our dataset, as well as the poor performance of Tesseract to detect Promotional Text, the decision was made to remove Promotional Text as one of the desired objects to study.</p><p>Building on those learnings, we implemented a pipeline, outlined in Figure 2, that would help us improve the performance from baseline, by up to 28 percentage points on Accuracy : </p></div><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/11-435bc20c4e2284716dcd7b9565a42098.PNG" width="535" height="308" class="img_ev3q"></p><p>Figure 2 - Pipeline for pre-processing images before Optical Character Recognition Models, and correcting detected text.</p></div><br><div align="justify"><p>For Step 1, pre-processing methods included the following, as well as combinations of those (illustrated in Figure 3): <strong>Binary Threshold, Sharpen, Normalisation, Histogram Equalisation, Histogram Adaptive Equalisation</strong>. As can be imagined, a processing method that enhances the performance of the detection method for one image, might decrease it for different images. Therefore, to ensure we obtained the highest accuracy for all images, we applied all pre-processing methods to all images, running Tesseract on each modified image, and keeping track of the text detected in each iteration. </p></div><div align="center"><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/12-bdf7d0b4655f39b30eaa56e906ef8e6d.PNG" width="840" height="414" class="img_ev3q"></p><p>Figure 3 - Effect of various image processing methods on the original. Images for illustrative purposes. Original images sourced from <a href="https://unsplash.com/s/photos/coca-cola" target="_top" rel="noopener noreferrer">unsplash.com</a></p></div><br><div align="justify">In Step 2 we use Tesseract to detect text in the pre-processed images. Step 3 consisted of removing any “incorrect” text, such as text inside the bounding boxes of Logo or Product objects, if the text was shorter than a threshold length (which depended on the brand), if it consisted of keywords, such as the brand name, or if it had ASCII characters.<p>With the current methodology (figure 2), the accuracy cannot be improved any further by changing the length of text that is considered to be “true” Text. Furthermore, there is a trade-off between the True Negative Rate and the True Positive Rate, which helped us choose a threshold for the length of string to accept: Since Accuracy does not improve further after length 4, but this value does optimise the other two metrics, 4 characters long is the threshold chosen. </p></div><br><div align="justify">The performance of Tesseract may be improved further by adding another step to the pipeline in figure 2, where the image is rotated several degrees, and after each rotation, text is detected. Due to time constraint issues, and the fact that we had already achieved a significant improvement above baseline and reached an accuracy of high 90s for some of the brands, we did not implement this step.</div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="code-snippets">Code snippets<a href="#code-snippets" class="hash-link" aria-label="Direct link to Code snippets" title="Direct link to Code snippets">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="install-required-libraries">Install required libraries<a href="#install-required-libraries" class="hash-link" aria-label="Direct link to Install required libraries" title="Direct link to Install required libraries">​</a></h3><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain"># To install tesseract</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># !sh apt-get -f -y install tesseract-ocr </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">!sudo apt-get install tesseract-ocr -y</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># To install pytesseract</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">!pip3 install pytesseract</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="import-required-libraries">Import required libraries<a href="#import-required-libraries" class="hash-link" aria-label="Direct link to Import required libraries" title="Direct link to Import required libraries">​</a></h3><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">import pytesseract</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from sklearn.metrics import confusion_matrix, accuracy_score</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">import cv2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from PIL import Image</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="processing-images">Processing images<a href="#processing-images" class="hash-link" aria-label="Direct link to Processing images" title="Direct link to Processing images">​</a></h3><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def normalise_img(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # normalise original and detect</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    norm_img = np.zeros((image.shape[0], image.shape[1]))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    normalised = cv2.normalize(image, norm_img, 0, 255, cv2.NORM_MINMAX)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    normalised = cv2.threshold(normalised, 100, 255, cv2.THRESH_BINARY)[1]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    normalised = cv2.GaussianBlur(normalised, (1, 1), 0)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return normalised</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def threshold_image(image, threshold=200):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    image = cv2.threshold(image, threshold, 255, cv2.THRESH_BINARY)[1]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return image</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def remove_noise_and_smooth(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # convert to grayscale</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # blur</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    blur = cv2.GaussianBlur(gray, (0,0), sigmaX=33, sigmaY=33)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # divide</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    divide = cv2.divide(gray, blur, scale=255)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # otsu threshold</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    thresh = cv2.threshold(divide, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # apply morphology</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    morph = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return morph</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def im_to_gray(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # convert to grayscale </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return gray</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def create_binary(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # create binary - converting a colored image (RGB) into a black and white image - Adaptive binarization works based on the features of neighboring pixels (i.e) local window.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    binary = cv2.threshold(image ,130,255,cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return binary</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def hist_equaliser(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Image Contrast and Sharpness: histogram equalization</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    equalised = cv2.equalizeHist(gray)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return equalised</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def adaptive_hist_equaliser(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Image Contrast and Sharpness: adaptive histogram equalization</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    equalised = clahe.apply(gray)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return equalised</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def invert_image():</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # invert the image - Color Inversion when different regions have different Foreground and Background colors</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    inverted = cv2.bitwise_not(image)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return inverted</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def im_blur(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Blurring or Smoothing </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    averageBlur = cv2.blur(image, (5, 5))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def im_gaussianBlur(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Blurring or Smoothing (Gaussian Blur)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    gaussian = cv2.GaussianBlur(image, (3, 3), 0)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def im_medianBlur(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Blurring or Smoothing (Median Blur (good at removing salt and pepper noises))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    medianBlur = cv2.medianBlur(image, 9)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def im_bilateralBlur(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Blurring or Smoothing (Bilateral Filtering)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    bilateral = cv2.bilateralFilter(image, 9, 75, 75)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return image</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def unsharp_mask(image, kernel_size=(5, 5), sigma=1.0, amount=1.0, threshold=0):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    """Return a sharpened version of the image, using an unsharp mask."""</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    blurred = cv2.GaussianBlur(image, kernel_size, sigma)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sharpened = float(amount + 1) * image - float(amount) * blurred</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sharpened = np.maximum(sharpened, np.zeros(sharpened.shape))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sharpened = np.minimum(sharpened, 255 * np.ones(sharpened.shape))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sharpened = sharpened.round().astype(np.uint8)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    if threshold &gt; 0:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        low_contrast_mask = np.absolute(image - blurred) &lt; threshold</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        np.copyto(sharpened, image, where=low_contrast_mask)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return sharpened</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def apply_filter_sharpen(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    image = cv2.filter2D(image, -1, kernel)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return image</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def im_erode_dilate(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # apply noise reduction techniques like eroding, dilating</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kernel = np.ones((2,2),np.uint8)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    image = cv2.erode(image, kernel, iterations = 1)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    image = cv2.dilate(image, kernel, iterations = 1)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return image</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def process_image(image):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    """</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    Example of a pipeline to pre-process an image.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    """</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # convert to grayscale </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     # create binary</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    image = cv2.threshold(image ,130,255,cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # invert the image</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    image = cv2.bitwise_not(image)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # apply noise reduction techniques like eroding, dilating</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kernel = np.ones((2,2),np.uint8)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    image = cv2.erode(image, kernel, iterations = 1)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    image = cv2.dilate(image, kernel, iterations = 1)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return image</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="detect-text">Detect text<a href="#detect-text" class="hash-link" aria-label="Direct link to Detect text" title="Direct link to Detect text">​</a></h3><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def detect_text(image, brand_name, t, bboxes_file, verbose=True):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    bboxes = get_bbox_pts(data)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data_out, bboxess, tt = check_text_ok(data, brand_name)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return data, bboxes</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="manipulate-bounding-boxes">Manipulate bounding boxes<a href="#manipulate-bounding-boxes" class="hash-link" aria-label="Direct link to Manipulate bounding boxes" title="Direct link to Manipulate bounding boxes">​</a></h3><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def get_bbox_pts(data):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    """</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    From the output of the text detection model, extract the [x1,x2,y1,y2] points.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    """</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data['x1'] = []</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data['x2'] = []</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data['y1'] = []</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data['y2'] = []</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    for i, txt in enumerate(data['text']):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        # extract the width, height, top and left position for that detected word</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        w = data["width"][i]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        h = data["height"][i]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        l = data["left"][i]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        t = data["top"][i]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        # define all the surrounding box points</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        x1 = l</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        x2 = l + w</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        y1 = t</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        y2 = t + h</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        data['x1'].append(x1)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        data['x2'].append(x2)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        data['y1'].append(y1)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        data['y2'].append(y2)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return data</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def transform_bbox(bbox):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    """</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    Transform Azure labels of valid set from [x,y,w,h] (in absolute numbers, prev. transformed) to [x,y,x1,y1].</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    """</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    x = bbox[0]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    y = bbox[1]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    w = bbox[2]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    h = bbox[3]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    x1 = x + w</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    y1 = y + h</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return [x,y,x1,y1]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="useful-links">Useful Links<a href="#useful-links" class="hash-link" aria-label="Direct link to Useful Links" title="Direct link to Useful Links">​</a></h2><ul><li><a href="https://github.com/tesseract-ocr" target="_blank" rel="noopener noreferrer">Tesseract - Github</a></li><li><a href="https://tesseract-ocr.github.io/tessdoc/Home.html" target="_blank" rel="noopener noreferrer">Tesseract - User Manual</a></li><li><a href="https://pypi.org/project/pytesseract/" target="_blank" rel="noopener noreferrer">Pytesseract - Pypi</a></li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="next-article">Next article<a href="#next-article" class="hash-link" aria-label="Direct link to Next article" title="Direct link to Next article">​</a></h2><p>In the next article, we outline the MMM process, focusing on how the creative elements are used as variables in a multivariate regression, as well as how the final ROIs were calculated.</p>]]></content:encoded>
            <category>Object Detection</category>
            <category>Optical Character Recognition</category>
            <category>Marketing Mix Modelling</category>
            <category>Deep Learning</category>
            <category>Tesseract</category>
        </item>
        <item>
            <title><![CDATA[Exploring the links between creative execution and marketing effectiveness - Part II Custom trained Detectron2 for OD]]></title>
            <link>https://ekimetrics.github.io/blog/2022/11/30/creative_execution_and_marketing_effectiveness_part_II</link>
            <guid>https://ekimetrics.github.io/blog/2022/11/30/creative_execution_and_marketing_effectiveness_part_II</guid>
            <pubDate>Wed, 30 Nov 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[In this Part II we explore the methodology for training Detectron2 models to detect brand-specific object in creative images.]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/Eki_Meta_part_II-34e9070bebe22829eed217b330dd3164.png" width="4928" height="3264" class="img_ev3q"></p></div><div align="justify"><p>This article is <strong>Part II</strong> of a set of five technical articles that accompany a <a href="https://ekimetrics.com/news-and-events/exploring-the-links-between-creative-execution-and-marketing-effectiveness-exclusivepreview" target="_blank" rel="noopener noreferrer">whitepaper</a> written in collaboration between Meta and Ekimetrics. Object Detection (OD) and Optical Character Recognition (OCR) were used to detect specific features in creative images, such as faces, smiles, text, brand logos, etc. Then, in combination with impressions data, marketing mix models were used to investigate what objects, or combinations of objects in creative images in marketing campaigns, drive higher ROIs.
In this Part II we explore the methodology for training Detectron2 models to detect brand-specific object in creative images.</p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="why-you-should-read-this">Why you should read this<a href="#why-you-should-read-this" class="hash-link" aria-label="Direct link to Why you should read this" title="Direct link to Why you should read this">​</a></h2><div align="justify">This article is mostly directed to machine learning practitioners. Here you will find a practical application of object detection algorithms; we present different open-source resources, comparisons and trade-offs in model selection for specific objects, methodology to improve performance for custom datasets, and how the object detection is then used to make inferences on the impact of creatives in marketing strategies.</div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="dataset">Dataset<a href="#dataset" class="hash-link" aria-label="Direct link to Dataset" title="Direct link to Dataset">​</a></h2><div align="justify">Our dataset contained &gt;50k image and video creatives across four different brands. The main goal of the custom object detection (OD) was to detect three types of brand-specific objects - logo, product and brand cue - as well as faces and smiles. Initially, faces and smiles were detected by pre-trained algorithms such as Haar cascades and Dlib, but due to the poor performance, it was decided to use a custom algorithm.<p>Before beginning the OD process, videos were converted to images by extracting every tenth frame. Training and Validation sets were then created using the Microsoft Azure Machine Learning Studio labelling tool. The labels were then converted to COCO format, and registered in Detectron as custom COCO libraries. Read more about this in <a href="https://ekimetrics.github.io/blog/2022/11/10/creative_execution_and_marketing_effectiveness_part_I" target="_blank" rel="noopener noreferrer">Part I</a>. </p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">## Registering COCO format datasets</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from detectron2.data.datasets import register_coco_instances</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">register_coco_instances(train, {'thing_classes': train_metadata.thing_classes, 'thing_dataset_id_to_contiguous_id': train_metadata.thing_dataset_id_to_contiguous_id}, train_json_path, TRAINING_IMAGES_PATH)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">register_coco_instances(valid, {'thing_classes': valid_metadata.thing_classes, 'thing_dataset_id_to_contiguous_id': valid_metadata.thing_dataset_id_to_contiguous_id}, valid_json_path, VALID_IMAGES_PATH)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="algorithm">Algorithm<a href="#algorithm" class="hash-link" aria-label="Direct link to Algorithm" title="Direct link to Algorithm">​</a></h2><div align="justify">The Faster Region-based Convolutional Neural Network (R-CNN) model, Faster R-CNN X 101 32x8d FPN 3x, was used for the custom OD model.<p>One model was trained per object per brand using the manually labelled training sets. For detecting faces and smiles, the training set consisted of creatives from all four brands, but each model was developed separately for face and smile per brand. In total there were, thus, 19 custom models. The validation set was used to tune hyperparameters of each model, with accuracy as the main metric. The final models were then used to detect objects in the unlabelled images for all four brands. The training, validation and final detections were all done using a single node GPU (CUDA) on Databricks.</p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="hyperparameter-tuning">Hyperparameter Tuning<a href="#hyperparameter-tuning" class="hash-link" aria-label="Direct link to Hyperparameter Tuning" title="Direct link to Hyperparameter Tuning">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="process">Process<a href="#process" class="hash-link" aria-label="Direct link to Process" title="Direct link to Process">​</a></h3><div align="justify">While Detectron2 allows for the customization of many configurations, including learning rate, backbone, image size, and number of images per batch, we limited the scope to just three parameters due to time constraints. These are summarized in Table 1 along with the parameter values tested. The choice of parameters was based on those deemed to be the most influential in performance. In addition to these parameters, the learning rate – the rate at which the algorithm converges to a solution   – was customized.  Rather than using the default value of 0.001, the learning rate was determined by the linear learning rate scaling rule. This was done in order to facilitate training on larger batch sizes.</div><p>&nbsp;</p><div align="center"><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/1-63bf2e1d86f975c672b49017135f6ec8.png" width="1015" height="268" class="img_ev3q"></p><p>Table 1: Parameters Tested in Custom Models</p></div><br><p>Example Code</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">## Parameters values to test</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">parameters = {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    'SOLVER.MAX_ITER': [300, 500, 1000],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    'ROI_HEADS.BATCH_SIZE_PER_IMAGE': [265, 512, 1024]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">## Configuring the algorithmn</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">def config_detectron(train_dataset, max_iter, batchsize):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    classes = MetadataCatalog.get(train_dataset).thing_classes</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    num_classes = len(classes)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    print(f"Number of classes in dataset: {num_classes}")</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    cfg = get_cfg()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    cfg.DATASETS.TRAIN = (train_dataset, )</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    cfg.DATASETS.TEST = ()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    cfg.DATALOADER.NUM_WORKERS = 0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml") # Let training initialize from model zoo</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    cfg.SOLVER.IMS_PER_BATCH = 2 ## # How many images per batch? The original models were trained on 8 GPUs with 16 images per batch, since we have 1 GPUs: 16/8 = 2 (we actually have 2 GPUs but we cannot use both as we do not have enough CUDA memory)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    cfg.SOLVER.BASE_LR = 0.00125 # We do the same calculation with the learning rate as the GPUs, the original model used 0.01, so we'll divide by 8: 0.01/8 = 0.00125. </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    cfg.SOLVER.MAX_ITER = max_iter   # How many iterations are we going for? </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = batchsize</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    print(dbutils.fs.ls(cfg.OUTPUT_DIR))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return cfg</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">### Training</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">def train_detectron(config):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    trainer = DefaultTrainer(config)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    trainer.resume_or_load(resume=False)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    trainer.train()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return trainer</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div align="justify">Exhaustive Grid Search was used to determine the combination of Max Iterations and Batch Size that yielded the highest accuracy on the validation set for each model. The results were visualized on a heat map, with confidence thresholds of 20%, 40%, 60%, and 80%. An example is shown in Figure 1. In this example, the model with Max Iteration of 1000 and a Batch Size of 265 performed the best, and the results did not improve beyond a confidence threshold of 40%. Confusion Matrices (see example in Figure 2) were also used to gain insight into the False Positive vs. False Negative rate.</div><p>&nbsp;</p><div align="center"><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/3-59df6df54647bdb3ecb7a9625ce33b5c.png" width="417" height="347" class="img_ev3q"></p><p>Figure 1 : Example Results of Grid Search</p></div><br><div align="center"><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/4_2-cfad45d3ddbbd7a950805c077bd28cb1.png" width="1082" height="373" class="img_ev3q"></p><p>Figure 2 : Example Confusion Matrices for Best Performing Model with 80% Confidence Threshold</p></div><br><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">## Prediction</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">def prediction(config, test_dataset, date_string, threshold):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    cfg = config</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    cfg.MODEL.WEIGHTS = f'/dbfs/mnt/trd/{brand}/objectdetection/custom_output/GPU/{date_string}/model_final.pth'</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = threshold # set the testing threshold for this model</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    cfg.DATASETS.TEST = (test_dataset, )</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    predictor = DefaultPredictor(cfg)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return predictor</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">def evaluation(config, test_dataset, trainer):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Setup an evaluator, we use COCO because it's one of the standards for object detection: https://detectron2.readthedocs.io/modules/evaluation.html#detectron2.evaluation.COCOEvaluator</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    evaluator = COCOEvaluator(dataset_name=test_dataset, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                              cfg=config, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                              distributed=False, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                              output_dir="./output/")</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Create a dataloader to load in the test data (cmaker-fireplace-valid)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    val_loader = build_detection_test_loader(config, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                                             dataset_name=test_dataset)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Make inference on the validation dataset: https://detectron2.readthedocs.io/tutorials/evaluation.html</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    inference = inference_on_dataset(model=trainer, # get the model from the trainer</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                         data_loader=val_loader, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                         evaluator=evaluator)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return inference</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">## Make Predictions</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">def get_predictions(predictor, imagePath):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Get predictions</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    image = cv2.imread(imagePath)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    predictions = predictor(image)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    instances = predictions["instances"]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    class_indexes = instances.pred_classes</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    prediction_boxes = instances.pred_boxes</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    class_catalog = valid_metadata.thing_classes</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    class_labels = [class_catalog[i] for i in class_indexes] </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    class_scores = instances.scores</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return class_indexes, prediction_boxes, class_labels, class_scores</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="results">Results<a href="#results" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results">​</a></h3><div align="justify">The best performing model was then used to determine the optimal threshold for each object type, at 1% intervals ranging from 20% to 99%. The threshold yielding the highest accuracy for each object class per model was then selected. The results are summarized in Table 2. Following these results, the best custom-trained model per object per brand was used to detect object in the unlabelled dataset in batches of 200 creatives. This was done in order to limit the CUDA memory and ensure that work was saved along the way should something happen. The total detection time for each brand ranged between 72-168 hours.</div><p>&nbsp;</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/2-2d2d4dac8274023a1627443f50d359e9.png" width="723" height="460" class="img_ev3q"></p><p>Table 2: Final Custom Models</p></div><br><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="useful-links">Useful links<a href="#useful-links" class="hash-link" aria-label="Direct link to Useful links" title="Direct link to Useful links">​</a></h3><ul><li><a href="https://github.com/facebookresearch/detectron2" target="_blank" rel="noopener noreferrer">Detectron2 - Github</a></li><li><a href="https://detectron2.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener noreferrer">Detectron2 - Documentation</a></li><li><a href="https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md" target="_blank" rel="noopener noreferrer">Detectron2 - ModelZoo</a></li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="other-useful-code">Other useful code<a href="#other-useful-code" class="hash-link" aria-label="Direct link to Other useful code" title="Direct link to Other useful code">​</a></h3><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="imports">Imports<a href="#imports" class="hash-link" aria-label="Direct link to Imports" title="Direct link to Imports">​</a></h4><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Install Detectron2 dependencies: https://detectron2.readthedocs.io/tutorials/install.html (use cu100 because colab is on CUDA 10.0)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">!pip install -U torch==1.4+cu100 torchvision==0.5+cu100 -f https://download.pytorch.org/whl/torch_stable.html </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">!pip install cython pyyaml==5.1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">!pip install awscli # you'll need this if you want to download images from Open Images (we'll see this later)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Make sure we can import PyTorch (what Detectron2 is built with)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">import torch, torchvision</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">torch.__version__</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">!gcc --version</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu100/index.html</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Setup detectron2 logger</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">import detectron2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from detectron2.utils.logger import setup_logger</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">setup_logger() # this logs Detectron2 information such as what the model is doing when it's training# import some common detectron2 utilities</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Other detectron2 imports</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from detectron2.engine import DefaultTrainer</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from detectron2.evaluation import COCOEvaluator, inference_on_dataset</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from detectron2.data import build_detection_test_loader</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from detectron2 import model_zoo </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from detectron2.engine import DefaultPredictor </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from detectron2.config import get_cfg</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from detectron2.utils.visualizer import Visualizer </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from detectron2.data import MetadataCatalog </span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="next-article">Next article<a href="#next-article" class="hash-link" aria-label="Direct link to Next article" title="Direct link to Next article">​</a></h2><p>In the next article, we will showcase Tesseract, a open-source optical character recognition (OCR) Engine, and the image-processing methods we developed to raise the baseline performance of this library, from 68% accuracy, by up to 28 percentage points.</p>]]></content:encoded>
            <category>Object Detection</category>
            <category>Optical Character Recognition</category>
            <category>Marketing Mix Modelling</category>
            <category>Deep Learning</category>
            <category>Tesseract</category>
        </item>
        <item>
            <title><![CDATA[Exploring the links between creative execution and marketing effectiveness - Part I: Detectron2 Pre-Trained Object Detection Models]]></title>
            <link>https://ekimetrics.github.io/blog/2022/11/10/creative_execution_and_marketing_effectiveness_part_I</link>
            <guid>https://ekimetrics.github.io/blog/2022/11/10/creative_execution_and_marketing_effectiveness_part_I</guid>
            <pubDate>Thu, 10 Nov 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[In this Part I we explore the methodology for using pre-trained Detectron2 models to detect brand-specific object in creative images.]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/Eki_meta_part_I-9053859fdf1b882a95f043936d6fe517.png" width="5184" height="3456" class="img_ev3q"></p></div><div align="justify"><p>This article is <strong>Part I</strong> of a set of five technical articles that accompany a <a href="https://ekimetrics.com/news-and-events/exploring-the-links-between-creative-execution-and-marketing-effectiveness-exclusivepreview" target="_blank" rel="noopener noreferrer">whitepaper</a> written in collaboration between Meta and Ekimetrics. Object Detection (OD) and Optical Character Recognition (OCR) were used to detect specific features in creative images, such as faces, smiles, text, brand logos, etc. Then, in combination with impressions data, marketing mix models were used to investigate what objects, or combinations of objects in creative images in marketing campaigns, drive higher ROIs.
In this Part I we explore the methodology for using pre-trained Detectron2 models to detect brand-specific object in creative images.</p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="why-you-should-read-this">Why you should read this<a href="#why-you-should-read-this" class="hash-link" aria-label="Direct link to Why you should read this" title="Direct link to Why you should read this">​</a></h2><div align="justify">This article is primarily directed to machine learning practitioners. Here you will find a practical application of object detection algorithms. We present different open-source resources, comparisons and trade-offs in model selection for specific objects, and methodology to improve performance for custom data sets.</div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="manual-labeling-of-the-training-and-validation-data-sets">Manual labeling of the Training and Validation data sets<a href="#manual-labeling-of-the-training-and-validation-data-sets" class="hash-link" aria-label="Direct link to Manual labeling of the Training and Validation data sets" title="Direct link to Manual labeling of the Training and Validation data sets">​</a></h2><div align="justify">In order to train custom train models (Part II of these series), as well as to evaluate the performance of all models, a proportion of the images were manually labeled in the Microsoft Azure Machine Learning Studio labeling tool. The tool facilitates the interactive selection of the region of interest, and the labeling of said region with a pre-defined object label, as shown in Figure 1.</div><p>&nbsp;</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/1-5d699faf2cd0bbf73f07d494f22a1aab.jpg" width="309" height="244" class="img_ev3q"></p><p>Figure 1 - Microsoft Azure Machine Learning Studio Image labeling Tool. Source: Microsoft.</p></div><br><div align="justify">Before undertaking the task of manually labeling images, and specially if it is done by a team of people and not a single person, it is advisable to have a pre-defined set of rules on how to approach the task. For example: when labeling “Person”, will you label only whole people? Or would you also label body parts as "Person"? Or at least those that show a face? - It is also important to ensure that all relevant objects in an image are labeled, specially in the validation data set, since that will constitute the ground truth when it comes to evaluating the performance of Object Detection models. When this is not done carefully, it becomes a problem, as some OD models might actually detect more objects than those which have been manually labeled.</div><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="distribution-of-objects-in-manually-labeled-data-set-and-implications-for-the-labeling-of-the-unlabeled-data-set">Distribution of Objects in Manually labeled Data set and Implications for the labeling of the Unlabeled Data set<a href="#distribution-of-objects-in-manually-labeled-data-set-and-implications-for-the-labeling-of-the-unlabeled-data-set" class="hash-link" aria-label="Direct link to Distribution of Objects in Manually labeled Data set and Implications for the labeling of the Unlabeled Data set" title="Direct link to Distribution of Objects in Manually labeled Data set and Implications for the labeling of the Unlabeled Data set">​</a></h4><div align="justify">The train, validation and unlabeled data sets were created by taking random samples of all the creatives. This strategy generated a training and validation set that have very close matching distributions of the labels of interest, as shown in Figure 2. Therefore, we assessed that it is likely that the performance of the models (pre-trained or custom) on the validation data sets will be transferable to the unlabeled data sets.</div><p>&nbsp;</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/2_v3-ff0909404b10024cc2c7572bfce3b5f0.png" width="387" height="320" class="img_ev3q"></p><p>Figure 2 - The distribution of labeled objects in the Train (outer) and Validation (inner) data sets.</p></div><br><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="models">Models<a href="#models" class="hash-link" aria-label="Direct link to Models" title="Direct link to Models">​</a></h2><div align="justify">The object detection process relied on a combination of pre-trained and custom-trained models from two Python libraries: Tesseract and Detectron2. Tesseract, an open-source Optical Character Recognition (OCR) library, was used for detecting text, while Detectron2, Facebook AI Research's next-generation platform for object detection and segmentation, was used for the remaining objects. We will explore the details of Tesseract in Part III of this series. From Detectron2, pre-trained object detection models were used for detecting common objects such as people, cars, and technology, while custom-trained models were used for detecting brand-specific objects such as logos (see Part II). Figure 3 outlines the objects detected by each model type.</div><p>&nbsp;</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/3-82ce22ae2a107dd08161357c7717b76a.png" width="602" height="170" class="img_ev3q"></p><p>Figure 3 - Object Detection using different pre-trained and custom-trained models.</p></div><br><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="pre-trained-models">Pre-Trained Models<a href="#pre-trained-models" class="hash-link" aria-label="Direct link to Pre-Trained Models" title="Direct link to Pre-Trained Models">​</a></h3><div align="justify">Pre-trained Detectron2 models are trained on the Microsoft COCO (Common Objects in Context) data set [1]; a large-scale object detection, segmentation, and captioning data set popularly used for computer vision projects. It contains over 200k labeled images, across 80 object categories (person, car, etc.) and 90 "stuff" (sky, grass, etc.) categories. However, for this study, we focused only on the detection of objects relating to the features of interest, detailed in figure 4.</div><p>&nbsp;</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/4-8f63f097c9b9804229b60c70042fb91b.png" width="923" height="233" class="img_ev3q"></p><p>Figure 4 - COCO Objects Detected in Pre-Trained Models, Present in Creatives of Brands Studied</p></div><br><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="choosing-a-detectron2-pre-trained-model">Choosing a Detectron2 Pre-Trained Model<a href="#choosing-a-detectron2-pre-trained-model" class="hash-link" aria-label="Direct link to Choosing a Detectron2 Pre-Trained Model" title="Direct link to Choosing a Detectron2 Pre-Trained Model">​</a></h3><p>The performance on Accuracy and Confidence Score (see figure 5 - Definition Box 1) were compared for three models : </p><ul><li><p>Model 1: Faster R-CNN R50 FPN 1x </p></li><li><p>Model 2: Faster R-CNN R101 FPN 3x</p></li><li><p>Model 3: Faster R-CNN X 101 32x8d FPN 3x </p></li></ul><p>Overall, Model 3 has the highest average Accuracy while Model 2 has the highest average Confidence Score. The number of creatives with correctly identified objects is highest in Model 3. Figure 6 shows the results. </p><p>&nbsp;</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/5-1a97c753bf14c3dc771d16c7709b91ce.png" width="337" height="299" class="img_ev3q"></p><p>Figure 5 - Definition Box 1</p></div><br><p>More in detail, for each object type: </p><ul><li>People: All three models have varying performance in detecting people across different brands. Overall, Model 3 has the highest accuracy.</li><li>Technology: Model 2 is most confident in detecting tech, but misses tech in many creatives entirely. Overall, Model 3 has the highest accuracy.</li><li>Cars: Model 3 has the highest average confidence and accuracy in detecting cars.</li><li>Hotel Interiors and Food: Model 2 and Model 3 have a similar performance in detecting food and interior, both higher than Model 1.</li></ul><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/6-712978dded9489ef7192af85f9a6df73.png" width="874" height="747" class="img_ev3q"></p><p> Figure 6 - Comparison in performance on the validation data set of three pre-trained Detectron2 models.</p></div><br><div align="justify">Furthermore, Model 3 overcomes challenges faced by the other two models, such as identifying people in close-up shots, identifying cars from interior shots, and identifying technology when in a person’s hand or lying on a table.<p>Figures 7 and 8 show the performance of Model 3 on the specific Objects of interest. In particular, the Confidence Scores (figure 8) that lead to the high Accuracies in figure 7 range from between 80% to 98%. As we developed our methodology to be applicable regardless of the industry, we needed to choose a Confidence Score Threshold of Acceptance that is high enough to ensure accuracy, but not too high that it would miss objects of interest entirely. Therefore, a threshold of 85% confidence was selected.</p></div><div align="center"><p><img loading="lazy" alt="screenshot-app" src="data:image/png;base64,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" width="318" height="248" class="img_ev3q"></p><p>Figure 7 - Accuracy of Faster R-CNN X 101 32x8d FPN 3x on Specific Object Types.</p></div><br><div align="center"><p><img loading="lazy" alt="screenshot-app" src="data:image/png;base64,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" width="313" height="251" class="img_ev3q"></p><p>Figure 8 - Average Confidence of Faster R-CNN X 101 32x8d FPN 3x on Specific Object Types.</p></div><br><div align="justify">Finally, since our creatives comprise of both still images, as well as frames extracted from video files, we wanted to make sure Object Detection was accurate in both types of images. Figure 9 shows the performance of Faster R-CNN X 101 32x8d FPN 3x (Model 3). The confidence scores do not differ significantly between frames and still images (at most 1.4% points). This means that the model can recognise object of interest with equal confidence in both types of images. The differences in performance may be attributed to people in frames more often being close-up; cartoons appear in frames while they are absent in stills; and frames of cars over-index on close-up shots and interior shots. The total detection time for all brands ranged 210 hours.</div><p>&nbsp;</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/9-7ab664c2c59cb1343be333a4f43dda73.PNG" width="873" height="649" class="img_ev3q"></p><p>Figure 9 - Performance of Faster R-CNN X 101 32x8d FPN 3x on Still vs Frame images.</p></div><br><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="useful-links">Useful links<a href="#useful-links" class="hash-link" aria-label="Direct link to Useful links" title="Direct link to Useful links">​</a></h3><ul><li><a href="https://github.com/facebookresearch/detectron2" target="_blank" rel="noopener noreferrer">Detectron2 - Github</a></li><li><a href="https://detectron2.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener noreferrer">Detectron2 - Documentation</a></li><li><a href="https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md" target="_blank" rel="noopener noreferrer">Detectron2 - ModelZoo</a></li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="snippets-of-code">Snippets of code<a href="#snippets-of-code" class="hash-link" aria-label="Direct link to Snippets of code" title="Direct link to Snippets of code">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="installations">Installations<a href="#installations" class="hash-link" aria-label="Direct link to Installations" title="Direct link to Installations">​</a></h3><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="import-libraries">Import libraries<a href="#import-libraries" class="hash-link" aria-label="Direct link to Import libraries" title="Direct link to Import libraries">​</a></h3><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">import detectron2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from detectron2.engine import DefaultPredictor</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from detectron2.config import get_cfg</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from detectron2.data import MetadataCatalog, data setCatalog</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from detectron2.utils.visualizer import ColorMode, Visualizer</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from detectron2 import model_zoo</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from detectron2.evaluation import COCOEvaluator, inference_on_data set</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from detectron2.data import build_detection_test_loader</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from PIL import Image</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">import cv2</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="define-detector-class">Define detector class<a href="#define-detector-class" class="hash-link" aria-label="Direct link to Define detector class" title="Direct link to Define detector class">​</a></h3><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">class Detector:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    def __init__(self):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self.cfg = get_cfg()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        # Load model config and pretrained model</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        model_name = "COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml"        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#         model_name = "COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#         model_name = "COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self.cfg.merge_from_file(model_zoo.get_config_file(model_name))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self.cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_name)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.85</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self.cfg.MODEL.DEVICE = "cpu" # or cuda</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self.predictor = DefaultPredictor(self.cfg)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Define a function to visualise the result of the prediction on an image and print the label IDs</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    def onImages(self, imagePath, if_visualise=False, if_print=False):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        # Get predictions</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        image = cv2.imread(imagePath)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        predictions = self.predictor(image)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        instances = predictions["instances"]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        class_indexes = instances.pred_classes</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        prediction_boxes = instances.pred_boxes</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        class_catalog = MetadataCatalog.get(self.cfg.data setS.TRAIN[0]).thing_classes</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        class_labels = [class_catalog[i] for i in class_indexes] </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        class_scores = instances.scores</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        # Visualise preditions</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        if if_visualise:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            viz = Visualizer(image[:,:,::-1], </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                               metadata = MetadataCatalog.get(self.cfg.data setS.TRAIN[0]), </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                               #scale=1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                               instance_mode = ColorMode.IMAGE_BW #IMAGE #SEGMENTATION #IMAGE_BW </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                              )</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            output = viz.draw_instance_predictions(predictions["instances"].to("cpu")) </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            plt.figure(figsize = (25,17))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            plt.imshow(output.get_image()[:, :, ::-1])</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        # Print labels</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        if if_print:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            print(f"Class Labels: {class_labels}")</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            print(f"Class Confidence Scores: {class_scores}")</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            print(f"Class Indexes: {class_indexes}") </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            print(f"Prediction Boxes: {prediction_boxes}")</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        return class_indexes, prediction_boxes, class_labels, class_scores</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # valuate its performance using AP metric implemented in COCO API.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # https://detectron2.readthedocs.io/modules/evaluation.html#detectron2.evaluation.COCOEvaluator</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    def evaluate(self, data set_name):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        evaluator = COCOEvaluator(data set_name, self.cfg.OUTPUT_DIR)#self.cfg, False, self.cfg.OUTPUT_DIR)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        val_loader = build_detection_test_loader(self.cfg, data set_name)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        result = inference_on_data set(self.predictor.model, val_loader, evaluator)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        print(result)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="run-detector">Run detector<a href="#run-detector" class="hash-link" aria-label="Direct link to Run detector" title="Direct link to Run detector">​</a></h3><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain"># label new creatives</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">job_name = 'new_creatives_set_50_scores_faster_rcnn_X_101_32x8d_FPN_3x'</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">datapath = "your-data-path"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">save_path = "your-save-path"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># get list of all image files to process</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">im_files_ext = ['jpg','png','jpeg','gif','webp','tiff','psd','raw','bmp','heif','indd','jpeg 2000','svg','ai','eps','pdf']</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">ims_list = []</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">for ext in im_files_ext:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    temp = glob.glob(f'{datapath}*.{ext}')</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    for file in temp:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        ims_list.append(file)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">num_files = len(ims_list)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">print(num_files)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">print(ims_list)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># get list of all image files to process</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">im_files_ext = ['jpg','png','jpeg','gif','webp','tiff','psd','raw','bmp','heif','indd','jpeg 2000','svg','ai','eps','pdf']</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">ims_list = []</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">for ext in im_files_ext:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    temp = glob.glob(f'{datapath}*.{ext}')</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    for file in temp:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        ims_list.append(file)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">num_files = len(ims_list)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">print(num_files)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">print(ims_list)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Initialise detector </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">detector = Detector()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Run detector on chunks</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">j = 1 # for keeping track of chunk</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">for chunk in chunks:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Create export file</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    df = pd.DataFrame(columns=['file_name', ' class_indexes', 'prediction_boxes', 'class_labels', 'class_scores'])</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    i = 1 # for keeping track of file number</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    for im_file in chunk: </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        start = time.time()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        class_indexes, prediction_boxes, class_labels, class_scores = detector.onImages(f'{im_file}', if_visualise=False, if_print=False)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        new_row = {'file_name': im_file,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                   'class_indexes': class_indexes.tolist(),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                   'prediction_boxes': prediction_boxes,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                   'class_labels': class_labels,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                   'class_scores': class_scores.tolist()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                  }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        df = df.append(new_row, ignore_index=True)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        end = time.time()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        print(f'\tChunk {j}/{len(chunks)} - file {i}/{len(chunk)}: {im_file} took {end-start} seconds.')</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        i = i + 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sp = f'{save_path}{job_name}_chunk_{j}.csv'</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    df.to_csv(sp, sep=',', header=True, index=False) </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    print(f"\t\tJSON saved in: {sp}")</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    j = j + 1</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="other-helpful-code">Other helpful code<a href="#other-helpful-code" class="hash-link" aria-label="Direct link to Other helpful code" title="Direct link to Other helpful code">​</a></h3><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def display_im_path(dbfs_img_path, figsize=(14,10)):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    image = Image.open(dbfs_img_path)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    plt.figure(figsize=(14,10))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    plt.imshow(image)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    plt.axis('off')</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    plt.show()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def display_im_array(array, figsize=(14,10)):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    plt.figure(figsize=figsize)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    plt.imshow(array)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    plt.axis('off')</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    plt.show()</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def get_files(basepath, num=None):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    all_files = os.listdir(basepath)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    im_files_ext = ['jpg','png','jpeg','gif','webp','tiff','psd','raw','bmp','heif','indd','jpeg 2000','svg','ai','eps','pdf']</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    img_list_all = [f"{basepath}{file}" for file in all_files if file.split('.')[-1] in im_files_ext]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#     random.shuffle(img_list_all)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    if num:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        return img_list_all[:num]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    else:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        return img_list_all</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="next-article">Next article<a href="#next-article" class="hash-link" aria-label="Direct link to Next article" title="Direct link to Next article">​</a></h2><p>After having explored Detector2 pre-trained models in this first part, in the next article, we will present custom-trained Detectron2 models, how they were used to detect logos, brand cues, faces and smiles, and code snipped on how to optimise parameter values. Stay tuned! </p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="appendix">Appendix<a href="#appendix" class="hash-link" aria-label="Direct link to Appendix" title="Direct link to Appendix">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="resources">Resources<a href="#resources" class="hash-link" aria-label="Direct link to Resources" title="Direct link to Resources">​</a></h3><div align="justify">An Azure Resource Group was set up in order to manage the Storage, Code Development and Machine Learning Studio functionalities. All the data was stored in Azure Storage. All code to process images, and implement Object Detection was written in Python scripts, in Azure Databricks notebooks. labeling of images was done in Azure Machine Learning Studio. Custom-trained models were trained and validated, and the unlabeled data set was labeled using the chosen models (both pre-trained and custom-trained), in a GPU cluster (Databricks Runtime Version 9.1 LTS ML includes Apache Spark 3.1.2, GPU, Scala 2.12 with Node Type Standard_NC_12 112 GB memory and 2 GPUs). Images were funnelled through the models in chunks of 200 images.</div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="detectron2">Detectron2<a href="#detectron2" class="hash-link" aria-label="Direct link to Detectron2" title="Direct link to Detectron2">​</a></h3><div align="justify">Detectron2 is a Facebook AI Research (FAIR) open-source library that provides object detection and segmentation algorithms [2-5]. It is implemented in PyTorch, and includes state-of-the-art algorithms like RetinaNet, Mask R-CNN, TensorMark and Faster R-CNN. Its training pipeline is GPU-run, making it fast and highly scalable. Furthermore, its modular nature allows for custom models to be created within an existing object detection framework. As a result, it was more suited for the needs of this study than other common object detection libraries like Yolov4.<p>For both the pre-trained and custom models, a Faster R-CNN algorithm was used due to its speed and high performance on small object detection. This algorithm combines two network models: Fast R-CNN (Region based Convolutional Neural Network), a deep learning algorithm which generates a convolutional feature map, and RPN (Regi on Proposal Networks), a convolutional network that proposes regions. </p><p>The specific model used was Faster R-CNN X 101 32x8d FPN 3x, a Faster R-CNN model developed by Detectron2 which uses a FPN (Feature Pyramid Networks) backbone, and a learning rate schedule of 3x. This model was chosen based on the baseline performances reported by FAIR, which highlight this model as having the highest average precision and training memory. For more information. Furthermore, the model was tested and evaluated against the performance of other Faster R-CNN models on objects the models are pre-trained on, against out Validation set.  Figure 6 shows the results.</p></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="manipulating-bounding-boxes">Manipulating Bounding Boxes<a href="#manipulating-bounding-boxes" class="hash-link" aria-label="Direct link to Manipulating Bounding Boxes" title="Direct link to Manipulating Bounding Boxes">​</a></h3><div align="justify">Different technologies (Azure, Detectron2, Tesseract) have different formats when it comes to annotating the bounding boxes [6]. Therefore, it was necessary to manipulate the bounding boxes as follows (figure 10):<p>In order to train the Custom-Trained Models, the Azure format was converted to the Detectron2 format: COCO bounding box (bbox) is defined as <!-- -->[x_min, y_min, width, height]<!-- -->. </p><p>JSON files exported from Azure Data labeling have bounding boxes that are normalised. Detectron2 expect the bounding boxes in absolute values. Therefore, we transformed the attribute 'bbox' of the COCO file downloaded from Azure ML Studio to denormalise it, and added the 'bbox_mode' attribute to be equal to 1 (XYWH_ABS 0). We also add the pair "iscrowd:0" to each annotation <!-- -->[8]<!-- -->.</p><p>In order to limit the region in images for text detection to outside of Logo and Product objects, Detectron2 format was transformed to Tesseract format: Detectron2 format <!-- -->[X, Y, W, H]<!-- --> to the Tesseract format (x1=X, x2=W+X, y1=Y, y2=H+Y).</p></div><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/14-628393196d530b6042faa71f222f59a8.PNG" width="903" height="296" class="img_ev3q"></p><p>Figure 10 - Manipulation of bounding boxes to accommodate each technology. The first transformation (from left to right) was necessary to train Custom-Trained models. The second transformation was necessary to limit the region in images for text detection to anything outside of logo or product objects.</p></div><br><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="detecting-faces-with-open-libraries-with-pre-trained-models">Detecting Faces with Open libraries with Pre-Trained Models<a href="#detecting-faces-with-open-libraries-with-pre-trained-models" class="hash-link" aria-label="Direct link to Detecting Faces with Open libraries with Pre-Trained Models" title="Direct link to Detecting Faces with Open libraries with Pre-Trained Models">​</a></h3><p>To detect Faces and Smiles, we began by exploring open libraries that have pre-trained models for this purpose. Figure 11 shows some of the libraries available, as well as those we tested.</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/15-79f72df3cc6d7763ad5a668aead16560.PNG" width="645" height="322" class="img_ev3q"></p><p>Figure 11 - Open libraries with pre-trained models that detect faces and smiles, and those we tested.</p></div><br><div align="justify">We tested three different open libraries that are optimised for face detection. The Haar Cascades model has two parameters that need to be optimised, so we set up two sets of 52 experiments to find the optimum models that detect Faces and Smiles. Visually, it was clear that the models had a very high rate of False Positives, therefore we enhanced the detection algorithm by limiting the detection zone for smiles to be only inside the bounding boxes of faces, as shown in figure 12.</div><p>&nbsp;</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/16-5c0bf7f491a763270bbfdccd89f815f8.PNG" width="179" height="209" class="img_ev3q"></p><p>Figure 12 - Smiles being detected inside faces only was a necessary methodology we implemented, to improve the accuracy of open libraries that detect faces and smiles.</p></div><br><div align="justify">Furthermore, as there were no ready-made functions in the libraries to assess the performance of predictions, we developed a method that compared the intersection of the bounding boxes between of the manually labeled and predicted objects, as shown in figure 13. All the models tested suffered from having a high False Positive Rate. The best performing model was with library DLIB, with an Accuracy of 59%, a True Positive Rate of 41% and a False Positive Rate of 36%. Therefore, we needed a better methodology for detecting Faces and Smiles, so we tested training a custom Detectron2 model to do so, as described in Part III of this series.</div><p>&nbsp;</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/17-0cf24c301436a4598eacc3d5a664e4f9.PNG" width="903" height="319" class="img_ev3q"></p><p>Figure 13 - Methodology for assessing whether a predicted label for Face or Smile is accurate. Blue boxes indicate the bounding box of manually labeled objects. Yellow boxes indicate those for predicted objects. The grey area indicates the Region of Intersection (ROI). If the grey areas are between 60% and 130% of the size of the manually labeled object’s bounding box, the predicted label was considered to be accurate.</p></div><br>]]></content:encoded>
            <category>Object Detection</category>
            <category>Optical Character Recognition</category>
            <category>Detectron2</category>
            <category>Marketing Mix Modelling</category>
            <category>Deep Learning</category>
        </item>
        <item>
            <title><![CDATA[Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (2/2)]]></title>
            <link>https://ekimetrics.github.io/blog/2022/10/26/Interpretability_sentiment_analysis_II</link>
            <guid>https://ekimetrics.github.io/blog/2022/10/26/Interpretability_sentiment_analysis_II</guid>
            <pubDate>Wed, 26 Oct 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[Two illustrations of how attention coefficients can be a source of interpretability.]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/interpretability_articles_2-cc917aa65ee22681f92d30c33c40d0d0.jpg" width="5731" height="3821" class="img_ev3q"></p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="summary">Summary<a href="#summary" class="hash-link" aria-label="Direct link to Summary" title="Direct link to Summary">​</a></h2><div align="justify"><p>We propose to illustrate how far BERT-type models can be considered as interpretable by design. We show that the attention coefficients specific to BERT architecture constitute a particularly rich piece of information that can be used to perform interpretability. There are mainly two ways to do interpretability: attribution and generation of counterfactual examples. In a first article, we showed how attention coefficients could be the basis of an attribution interpretability method. Here we propose to evaluate how they can also be used to set up counterfactuals. </p></div><div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="work-presented-in-the-previous-article">Work presented in the previous article<a href="#work-presented-in-the-previous-article" class="hash-link" aria-label="Direct link to Work presented in the previous article" title="Direct link to Work presented in the previous article">​</a></h2><p>Previously, the BERT <!-- -->[1]<!-- --> and DistilBERT <!-- -->[2]<!-- --> models have been mobilized to tackle the well-known problem of sentiment analysis. In particular, we have shown that the BERT and DistilBERT models contain within their architecture attention coefficients that can be at the heart of an attribution interpretability method. Starting from an initial text, a visualization of the weight assignment method was proposed. The more red the color, the higher the associated attention coefficient. </p><div align="center"><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/Image_2-fc95973e15821ef99dbc2fd6e4a5b6c8.jpg" width="769" height="91" class="img_ev3q"></p><p>Figure 1 - Attention-Based token importance</p></div><p>&nbsp;</p><p> We saw that the word groups "<em>favorite movie</em>", "<em>it just never gets old</em>", "<em>performance brings tears</em>", or "<em>it is believable and startling</em>" stood out. This explained well why the algorithm evaluated the review as positive and what was the semantic field at the root of this prediction. This work was done using the Hugging Face transformers library <!-- -->[3]<!-- -->.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="interpreting-through-counterfactual-generation">Interpreting through counterfactual generation<a href="#interpreting-through-counterfactual-generation" class="hash-link" aria-label="Direct link to Interpreting through counterfactual generation" title="Direct link to Interpreting through counterfactual generation">​</a></h2><p>Another way to do interpretability is to generate counterfactual examples. According to Judea Pearl, counterfactual "involves answering questions which ask what might have been, had circumstances been different” <!-- -->[4]<!-- -->. Thus, the idea is to understand a prediction by generating a counterfactual example, resulting in an opposite prediction. In the context of natural language processing, it is therefore a matter of changing the right words in the review. In order to generate a counterfactual example, we propose the following methodology:</p><ul><li>Compute the attention coefficients of the tokens in a text corpus on each attention layer (6). The text corpus size must be statistically significant </li><li>Perform token clustering based on their 6-dimensional representation</li><li>Detect clusters associated with positively and negatively charged sentiment words</li><li>Replace the tokens with the highest average attention with their "opposite token" in their "opposite cluster"
This approach allows us to validate the interpretative strength of the tokens put forward by the attention coefficients, while illustrating what a close review would have been with an opposite sentiment.
We apply the methodology on a corpus of 1000 reviews. The clustering method used is the hierarchical ascending classification (HAC) and gives 3 clusters. The obtained clusters and the counterfactual generation procedure can be represented in 2 dimensions as follows:</li></ul><div align="center"><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/Image_3-094ab2c3d6b69a58223f9a733aef845b.jpg" width="416" height="342" class="img_ev3q"></p><p>Figure 2 - Token clusters &amp; replacements</p></div><p>We then generate the counterfactual example of the review tested earlier by changing 2 words: </p><div align="center">delight ➡ torment<p>favorite ➡ worst</p></div><p>This gives us the following counterfactual example:</p><p>“<em>Probably my all time worst movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring . it just never gets old despite my having seen it some 15 or more times in the last 25 years . paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a torment. the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch . and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling . if i had a dozen thumbs they’d all be up for this movie</em>".</p><p>As the text is quite long, 2 tokens are not enough to change the feeling associated with the review. The probability score nevertheless drops significantly by 0.3pts.
One way to assess the quality of the generated counterfactual examples is to evaluate the proportion of reviews in a corpus whose associated sentiment has changed. The result can be represented as a "counterfactual confusion matrix" as follows:</p><p>One way to assess the quality of the generated counterfactual examples is to evaluate the proportion of reviews in a corpus whose associated sentiment has changed. The result can be represented as a "counterfactual confusion matrix" as follows:</p><div align="center"><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/Image_4-91c11a737d5e41b98a8657cd3bb17de7.jpg" width="756" height="74" class="img_ev3q"></p><p>Table 1 - Counterfactual confusion matrix example</p></div><p>Where :</p><ul><li>X<sub>11</sub> represents the share of reviews whose initial associated sentiment and the sentiment of the counterfactual example are positive; sentiment has remained the same </li><li>X<sub>12</sub> represents the share of reviews whose sentiment changed from positive to negative; sentiment did change </li><li>X<sub>21</sub> represents the share of reviews whose sentiment changed from negative to positive; sentiment changed well</li><li>X<sub>22</sub> represents the share of reviews whose initial associated sentiment and the sentiment of the counterfactual example are negative; sentiment has remained the same</li></ul><p>We compute the "counterfactual confusion matrix" on the same text corpus that enabled us to perform clustering, picking 5 tokens for each review. The result is given below:</p><div align="center"><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/Image_5-de802c493bd17b6c40e5c5714774720e.jpg" width="756" height="73" class="img_ev3q"></p><p>Table 2 - Actual counterfactual confusion matrix</p></div><p>&nbsp;</p>Thus, we see that changing the 5 tokens with the highest average attention produces a change in sentiment perception in 44% of cases. In particular, the rate of sentiment change for reviews initially perceived as positive is 31% while the rate of sentiment change for reviews initially perceived as negative is 53%. The change from negative to positive seems to be better achieved with our method.<p>We have shown that attention coefficients can be a source of interpretability. Used in the right way, the attention coefficients allow the detection of tokens with high predictive value. They can also be used to generate counterfactual examples in order to better understand what the sentence should have been in order to be associated with an opposite sentiment. The interest of the attention coefficients is reinforced by the "counterfactual confusion matrix": The high transformation rate of the reviews' sentiments shows that the tokens selected thanks to the attention are strongly meaningful.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="next-step">Next step<a href="#next-step" class="hash-link" aria-label="Direct link to Next step" title="Direct link to Next step">​</a></h2><p>We plan to test other ways to generate counterfactual examples. One way would be to take advantage of the way DistilBert has been trained: the mask language modeling (MLM). The idea would be to mask the tokens with high average attention, and replace them with the tokens with the highest softmax in the "opposite cluster". This would ensure the grammatical correctness of the generated counterfactual example. Finally, the generation of counterfactual examples can have other applications than interpretability. In particular, it becomes possible to perform data augmentation in order to give more examples to a model. It can mitigate biases by balancing the sentiments of biased discriminated populations. This would improve fairness indicators while not degrading accuracy. </p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">​</a></h2><p>[1]<!-- --> VASWANI, Ashish, SHAZEER, Noam, PARMAR, Niki, et al. Attention is all you need. Advances in neural information processing systems, 2017, vol. 30.</p><p>[2]<!-- --> SANH, Victor, DEBUT, Lysandre, CHAUMOND, Julien, et al. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.</p><p>[3]<!-- --> Hugging face library <a href="https://huggingface.co/" target="_blank" rel="noopener noreferrer">https://huggingface.co/</a></p><p>[4]<!-- --> PEARL, Judea et MACKENZIE, Dana. The book of why: the new science of cause and effect. Basic books, 2018</p></div>]]></content:encoded>
            <category>NLP</category>
            <category>Transformers</category>
            <category>BERT</category>
            <category>interpretability</category>
            <category>explainability</category>
            <category>XAI</category>
            <category>attention</category>
        </item>
        <item>
            <title><![CDATA[Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (1/2)]]></title>
            <link>https://ekimetrics.github.io/blog/2022/10/18/Interpretability_sentiment_analysis_I</link>
            <guid>https://ekimetrics.github.io/blog/2022/10/18/Interpretability_sentiment_analysis_I</guid>
            <pubDate>Tue, 18 Oct 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[Two illustrations of how attention coefficients can be a source of interpretability]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/interpretability_articles-f023579cce44d5ba66a1c2cc62310d74.jpg" width="3137" height="2091" class="img_ev3q"></p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="summary">Summary<a href="#summary" class="hash-link" aria-label="Direct link to Summary" title="Direct link to Summary">​</a></h2><div align="justify"><p>We propose to illustrate how far BERT-type models can be considered as interpretable by design. We show that the attention coefficients specific to BERT architecture constitute a particularly rich piece of information that can be used to perform interpretability. There are mainly two ways to do interpretability: attribution and generation of counterfactual examples. Here we propose to evaluate how attention coefficients can form the basis of an attribution method. We will show in a second article how they can also be used to set up counterfactuals. </p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-bert-architecture">The BERT architecture<a href="#the-bert-architecture" class="hash-link" aria-label="Direct link to The BERT architecture" title="Direct link to The BERT architecture">​</a></h2><div align="justify"><p>An artificial neural network is a computer system inspired by the functioning of the human brain and biological neurons to learn specific tasks. The neural networks represent a subset of machine learning algorithms. In order to perform a learning task, the neural network spreads information through an elementary network, called a perceptron. The way in which information is diffused can be formalized through linear algebra and the manipulation of various activation functions. A neural network can be defined as an association of elementary objects called formal neurons, like the perceptron. There are several types of layers that can be part of a neural network:</p><ul><li>Fully connected layers, which receive a vector as input, and produce a new vector as output by applying a linear combination and possibly an activation function;</li><li>Convolution layers, which learn localized patterns in space;</li><li>Attention layers, which model the general relations between different objects.</li></ul></div><div align="justify"><p>Attention mechanisms are particularly effective for natural language processing tasks. This is mainly due to the fact that they allow to properly model a word through mathematical representations. In particular, attention layers make it possible to assign a contextual representation of the word on a case-by-case basis. This makes it a much more efficient tool than Word2vec since the latter only models an average context, but does not adapt to the given situation. Attention mechanisms are at the heart of Transformers-type models as shown in the diagram below. The BERT model corresponds to a stack of the left part of the generic architecture of a Transformer <!-- -->[1]<!-- -->.</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Image_2-3ae3a962b8d25b8df96ed38b648465b2.jpg" width="221" height="312" class="img_ev3q"></p><p>Figure 1 - Transformers architecture</p></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="fine-tuning-of-bert-for-sentiment-analysis">Fine tuning of BERT for sentiment analysis<a href="#fine-tuning-of-bert-for-sentiment-analysis" class="hash-link" aria-label="Direct link to Fine tuning of BERT for sentiment analysis" title="Direct link to Fine tuning of BERT for sentiment analysis">​</a></h2><div align="justify"><p>To illustrate how attention coefficients can be a source of interpretability in natural language processing, we propose to fine tune a DistilBERT for sentiment analysis. A DistilBERT is a distilled version of BERT. It is smaller, faster, cheaper, lighter and recovers 97% of BERT’s performance on GLUE <!-- -->[2]<!-- -->. A perfect compromise, in fact. Most transformers are available pre-trained on the Hugging Face transformers library <!-- -->[3]<!-- -->. The objective is to perform supervised classification on the IMDB database to assess the sentiment associated with a movie review. An illustration of the dataset is shown below:</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Image_3-5154784fda191b99bf74ac11a362d962.jpg" width="408" height="269" class="img_ev3q"></p><p> Figure 2 - IMDB sample</p></div><p>&nbsp;</p>To do so, we import all the libraries needed.  In particular, the tokenizer DistilBertTokenizer and the pre-trained hugging face model TFDistilBertForSequenceClassification are used.<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">sentence_encoder = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', output_attentions = True)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The parameter "output_attention" must be equal to "True". It will allow us to retrieve the attention coefficients of the model. We add a dense layer with a softmax activation to fine tune the model to do sentiment analysis. In order to train the model, we use the following hyperparameters:</p><ul><li>initial_lr  = 1e-5</li><li>n_epochs    = 15</li><li>batch_size  = 64</li><li>random_seed = 42</li></ul><p>Finally, we make evolve the learning and stop the learning process if the val_loss does not decrease after a certain number of iterations. </p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose = 1,min_delta=0.005,patience=3, min_lr=3e-7)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=6, verbose=1, mode='auto',baseline=None, restore_best_weights=True)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>We can finally fine tune the DistilBert.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">history = model.fit(X_train, y_train, batch_size=bs, epochs=n_epochs, validation_data=(X_test, y_test), </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                    verbose=1,callbacks=[early_stop, reduce_lr])</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>We obtain a val_accurcay of 85%, which is sufficient for our further analysis. Note that a BERT or a RoBERTa would have certainly had a better val_loss, as they are more heavy and complex.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="recovery-of-attention-coefficients">Recovery of attention coefficients<a href="#recovery-of-attention-coefficients" class="hash-link" aria-label="Direct link to Recovery of attention coefficients" title="Direct link to Recovery of attention coefficients">​</a></h2><p>We are now able to analyze the attention coefficients related to movie reviews. In order to retrieve it, We need to predict the sentiment associated to a review.
Then, we select the layer(s) of attention to analyze. We focus here on the last layer of attention.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">inputs = tokenizer.batch_encode_plus(reviews,truncation=True, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                                     add_special_tokens = True, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                                     max_length = max_len, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                                     pad_to_max_length = True)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">tokenized = np.array(inputs["input_ids"]).astype("int32")</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">attention_mask = np.array(inputs["attention_mask"]).astype("int32")</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">encoded_att = model.layers[2](tokenized,attention_mask =attention_mask)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#last attention layer</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">last_attention=encoded_att.attentions[-1]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>We finally recovered the 12 attention matrices from the last layer of the DistilBert.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="interpreting-through-attention-attribution">Interpreting through attention attribution<a href="#interpreting-through-attention-attribution" class="hash-link" aria-label="Direct link to Interpreting through attention attribution" title="Direct link to Interpreting through attention attribution">​</a></h2><p>A first way to take advantage of the attention coefficients is to directly look at their value in order to evaluate if the right words stand out. We choose to calculate the average attention on all attention layers and heads. A more in-depth work of selection of the most relevant layer would allow to refine the interpretability method. Here, we limit ourselves to the most basic case.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">a,b = [], []</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">for head in range(0,12) :</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    for i, elt in enumerate(inputs['input_ids'][0]):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        if np.array(elt) != 1:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            att = last_attention.numpy()[0,head][0][i]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            a.append(tokenizer.decode([elt]) + '_' + str(i))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            b.append(att)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">attention_all_head=pd.DataFrame({"Token":a,"Attention coefficient":b})</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>In order to have the average attention, we group by the attention score on all the layers and heads.
We finally have the average attention coefficients associated with the words of the film review. As an example, the attention coefficients associated with the following positive review is calculated:</p><p>“<em>Probably my all time favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring . it just never gets old despite my having seen it some 15 or more times in the last 25 years . paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight . the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch . and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling . if i had a dozen thumbs they’d all be up for this movie</em>".</p><p>The review being long, we represent the text in color. The more red the color, the higher the associated attention coefficient. The result is shown below:</p><div align="center"><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/Image_4-fc95973e15821ef99dbc2fd6e4a5b6c8.jpg" width="769" height="91" class="img_ev3q"></p><p>Figure 3 - Attention-Based token importance</p></div><p>&nbsp;</p>We see that the word groups "favorite movie", "it just never gets old", "performance brings tears", or "it is believable and startling" stand out. This explains well why the algorithm evaluated the review as positive and what was the semantic field at the root of this prediction.<p>&nbsp;</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="next-step">Next step<a href="#next-step" class="hash-link" aria-label="Direct link to Next step" title="Direct link to Next step">​</a></h2><p>We will show in a future article how attention coefficients are useful for generating counterfactual examples to explain the model prediction.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">​</a></h2><p>[1]<!-- --> VASWANI, Ashish, SHAZEER, Noam, PARMAR, Niki, et al. Attention is all you need. Advances in neural information processing systems, 2017, vol. 30.</p><p>[2]<!-- --> SANH, Victor, DEBUT, Lysandre, CHAUMOND, Julien, et al. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.</p><p>[3]<!-- --> Hugging face library <a href="https://huggingface.co/" target="_blank" rel="noopener noreferrer">https://huggingface.co/</a></p></div>]]></content:encoded>
            <category>NLP</category>
            <category>Transformers</category>
            <category>BERT</category>
            <category>interpretability</category>
            <category>explainability</category>
            <category>XAI</category>
            <category>attention</category>
        </item>
        <item>
            <title><![CDATA[Newsletter for September 2022]]></title>
            <link>https://ekimetrics.github.io/blog/2022/09/20/newsletter_Sept-2022</link>
            <guid>https://ekimetrics.github.io/blog/2022/09/20/newsletter_Sept-2022</guid>
            <pubDate>Tue, 20 Sep 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[Hi everyone, we are now in September and we release our 6th Newsletter! Ranging from podcasts to tutorials, this Newsletter is made for practicioners!]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/header-0368178a148036c79f042ff7e5b391c0.png" width="3264" height="4080" class="img_ev3q"></p></div><div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="data-science">Data Science<a href="#data-science" class="hash-link" aria-label="Direct link to Data Science" title="Direct link to Data Science">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="why-you-should-warn-customers-when-youre-running-low-on-stock">Why You Should Warn Customers When You’re Running Low on Stock<a href="#why-you-should-warn-customers-when-youre-running-low-on-stock" class="hash-link" aria-label="Direct link to Why You Should Warn Customers When You’re Running Low on Stock" title="Direct link to Why You Should Warn Customers When You’re Running Low on Stock">​</a></h3><p><img loading="lazy" src="/assets/images/DS_article_1-e90bd88a7a5908f1218fc44c8eda5ef6.jpg" width="6000" height="4000" class="img_ev3q"></p><div align="justify">The supply-chain disruptions due to the pandemic and the Ukraine war caused the retailers to face unprecedented stockouts risks. To overcome this challenge, Instacart suggest that honesty is the best policy. By using a Machine Learning model to predict that an item is likely out-of-stock and therefore warning clients when it’s the case, they reduced the proportion of replacements and refunds in the short term and increased the total revenue per customer in the long term.</div><p>&nbsp;</p><p><a href="https://hbr.org/2022/09/why-you-should-warn-customers-when-youre-running-low-on-stock" target="_blank" rel="noopener noreferrer">Why You Should Warn Customers When You’re Running Low on Stock - Harvard Business Review</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="machine-learning">Machine Learning<a href="#machine-learning" class="hash-link" aria-label="Direct link to Machine Learning" title="Direct link to Machine Learning">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="hopular-modern-hopfield-networks-for-tabular-data">Hopular: Modern Hopfield Networks for Tabular Data<a href="#hopular-modern-hopfield-networks-for-tabular-data" class="hash-link" aria-label="Direct link to Hopular: Modern Hopfield Networks for Tabular Data" title="Direct link to Hopular: Modern Hopfield Networks for Tabular Data">​</a></h3><p><img loading="lazy" src="/assets/images/ML_DL-940d62772895a4f7012e4d2b739c908c.jpg" width="5760" height="3840" class="img_ev3q"></p><div align="justify">While Deep Learning excels in structured data as encountered in vision and natural language processing, it failed to meet its expectations on tabular data. For tabular data, Support Vector Machines (SVMs), Random Forests, and Gradient Boosting are the best performing techniques with Gradient Boosting in the lead.</div><div align="justify">"Hopular" is a novel Deep Learning architecture for medium- and small-sized datasets, where each layer is equipped with continuous modern Hopfield networks.</div><div align="justify">In experiments on small-sized tabular datasets with less than 1,000 samples, Hopular surpasses Gradient Boosting, Random Forests, SVMs, and in particular several Deep Learning methods. In experiments on medium-sized tabular data with about 10,000 samples, Hopular outperforms XGBoost, CatBoost, LightGBM and a state-of-the art Deep Learning method designed for tabular data. Thus, Hopular is a strong alternative to these methods on tabular data.</div><p>&nbsp;</p><p><a href="https://ml-jku.github.io/hopular/" target="_blank" rel="noopener noreferrer">GitHub - Hopular</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="data-engineering--architecture">Data Engineering &amp; Architecture<a href="#data-engineering--architecture" class="hash-link" aria-label="Direct link to Data Engineering &amp; Architecture" title="Direct link to Data Engineering &amp; Architecture">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-to-learn-data-engineering">How to learn data engineering<a href="#how-to-learn-data-engineering" class="hash-link" aria-label="Direct link to How to learn data engineering" title="Direct link to How to learn data engineering">​</a></h3><p><img loading="lazy" src="/assets/images/DE_article_1-8494ff059bb5e1a03dc20dbb08627e45.jpg" width="4032" height="3024" class="img_ev3q"></p><div align="justify">Like data scientists, data engineers write code. They’re highly analytical, and are interested in data visualization. Unlike data scientists — and inspired by our more mature parent,&nbsp;software engineering&nbsp;— data engineers build tools, infrastructure, frameworks, and services. In fact, it’s arguable that data engineering is much closer to software engineering than it is to a data science.</div><div align="justify">This post introduces the role of a data engineer and the challenges he faces on a daily basis. It also provides some very interesting links to acquire the basics or to consolidate your knowledge in this domain.</div><p>&nbsp;</p><p><a href="https://www.blef.fr/learn-data-engineering/" target="_blank" rel="noopener noreferrer">How to learn data engineering | Blef.fr</a></p><p>&nbsp;</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="dos-and-donts-of-data-mesh">Do’s and Don’ts of Data Mesh<a href="#dos-and-donts-of-data-mesh" class="hash-link" aria-label="Direct link to Do’s and Don’ts of Data Mesh" title="Direct link to Do’s and Don’ts of Data Mesh">​</a></h3><p><img loading="lazy" src="/assets/images/DE_article_2-746470f3aec65a53368228c3054ef2ee.jpg" width="3072" height="3888" class="img_ev3q"></p><div align="justify">Many enterprises are investing in their next generation data lake, with the hope of democratizing data at scale to provide business insights and ultimately make automated intelligent decisions. Data platforms based on the data lake architecture have common failure modes that lead to unfulfilled promises at scale. To address these failure modes, a new paradigm saw the light : Data Mesh.</div><div align="justify">This article introduces Data Mesh and offers a series of advices, Do’s and Don’ts as a result of its implementation in BlaBlaCar.</div><p>&nbsp;</p><p><a href="https://medium.com/blablacar/dos-and-don-ts-of-data-mesh-e093f1662c2d" target="_blank" rel="noopener noreferrer">Do’s and Don’ts of Data Mesh | Kineret Kimhi | Medium</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="app-and-web-development">App and Web Development<a href="#app-and-web-development" class="hash-link" aria-label="Direct link to App and Web Development" title="Direct link to App and Web Development">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="introducing-signals">Introducing Signals<a href="#introducing-signals" class="hash-link" aria-label="Direct link to Introducing Signals" title="Direct link to Introducing Signals">​</a></h3><p><img loading="lazy" src="/assets/images/App_article_1-9e30f8f36e826981c668a6b259ee2e00.jpg" width="4077" height="3203" class="img_ev3q"></p><p>Signals are a way of expressing state that ensure apps stay fast regardless of how complex they get. Signals are based on reactive principles and provide excellent developer ergonomics, with a unique implementation optimized for Virtual DOM.</p><p><a href="https://preactjs.com/blog/introducing-signals/" target="_blank" rel="noopener noreferrer">Introducing Signals | Preactjs.com</a></p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="best-practices-for-creating-a-modern-npm-package">Best practices for creating a modern npm package<a href="#best-practices-for-creating-a-modern-npm-package" class="hash-link" aria-label="Direct link to Best practices for creating a modern npm package" title="Direct link to Best practices for creating a modern npm package">​</a></h3><p><img loading="lazy" src="/assets/images/App_article_2-bcb3b3859a5cc71ff4e0441988e7a445.jpg" width="6336" height="9504" class="img_ev3q"></p><div align="justify">NPM is an online repository for the publishing of open-source Node.js projects, it also allows to interact with the repository through a command-line utility for package installation, version management and dependency management.</div><div align="justify">This article details the best practices for creating and managing an npm package such as security checks, automated semantic version management etc.</div><p>&nbsp;</p><p><a href="hhttps://snyk.io/blog/best-practices-create-modern-npm-package/" target="_blank" rel="noopener noreferrer">Best practices for creating a modern npm package | Brian Clark | Synk.io</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="special-section-responsible-ai">Special Section: Responsible AI<a href="#special-section-responsible-ai" class="hash-link" aria-label="Direct link to Special Section: Responsible AI" title="Direct link to Special Section: Responsible AI">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="openrail-towards-open-and-responsible-ai-licensing-frameworks">OpenRAIL: Towards open and responsible AI licensing frameworks<a href="#openrail-towards-open-and-responsible-ai-licensing-frameworks" class="hash-link" aria-label="Direct link to OpenRAIL: Towards open and responsible AI licensing frameworks" title="Direct link to OpenRAIL: Towards open and responsible AI licensing frameworks">​</a></h3><p><img loading="lazy" src="/assets/images/responsible-bd81e67654846dc5ebe64a63310811a9.jpg" width="3024" height="4032" class="img_ev3q"></p><div align="justify">Advances in machine learning and other AI-related areas have flourished these past years partly thanks to thethe open source culture. It has in fact allowed knowledge sharing and created communities that fostered innovation. Nevertheless, recent events related to the ethical and socio-economic concerns of development and use of machine learning models have spread a clear message: Making sure AI is responsible is incompatible with open-source. Yet, closed systems are not the answer, as the problem persists under the opacity of firms' private AI development processes.</div><div align="justify">In this context, the OpenRAIL approach suggests a new type of licensing that embed a specific set of restrictions to make sure of the good usage of the models. Therefore, while benefiting from an open access to the ML model, the user will not be able to use the model for the specified restricted scenarios.</div><p>&nbsp;</p><p><a href="https://huggingface.co/blog/open_rail" target="_blank" rel="noopener noreferrer">OpenRAIL: Towards open and responsible AI licensing frameworks | Carlos Munoz Ferrandis | huggingface.co</a></p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="credits">Credits<a href="#credits" class="hash-link" aria-label="Direct link to Credits" title="Direct link to Credits">​</a></h2><ul><li>Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a></li></ul>]]></content:encoded>
            <category>Data Science</category>
            <category>Data Engineering</category>
            <category>Data Mesh</category>
            <category>NPM</category>
            <category>Hopular</category>
        </item>
        <item>
            <title><![CDATA[Deep RL and Optimization applied to Operations Research problem - 2/2 Reinforcement Learning approach]]></title>
            <link>https://ekimetrics.github.io/blog/2022/09/06/deep_rl</link>
            <guid>https://ekimetrics.github.io/blog/2022/09/06/deep_rl</guid>
            <pubDate>Tue, 06 Sep 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[This article is part of a series of articles which will introduce several optimization techniques, from traditional (yet advanced) Mathematical Optimization solvers and associated packages to Deep Reinforcement Learning algorithms, while tackling a very famous Operations Research problem: the multi-knapsack problem. Here, the focus is on an approach based on two famous reinforcement learning algorithms: Q-Learning and Policy Gradient.]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/slovenia_bled_lake-3c33ada4728f73d897ac08fb9d82593e.jpg" width="4000" height="2525" class="img_ev3q"></p></div><div align="justify"><p>This article is the second part of the serie of articles introducing optimization techniques for solving the classical Operations Research problem of multi-knapsack. The main objective of this article is to introduce Reinforcement Learning as a way to solve combinatorial optimization problems (Reinforcement Learning can actually be used to solve a much wider range of optimization problems). </p><p>First, the classical Reinforcement Learning framework will be briefly presented. Then, we'll see how to frame the multi-knapsack problem for Reinforcement Learning, followed by explanations on why we chose to explore RL for this combinatorial optimization problem. Eventually, the Q-learning (no neural networks) and Policy Gradient (with neural networks) approaches will be introduced and their performance will be evaluated on the knapsack problem.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="reinforcement-learning-for-the-knapsack-problem">Reinforcement Learning for the Knapsack problem<a href="#reinforcement-learning-for-the-knapsack-problem" class="hash-link" aria-label="Direct link to Reinforcement Learning for the Knapsack problem" title="Direct link to Reinforcement Learning for the Knapsack problem">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-is-reinforcement-learning">What is Reinforcement Learning?<a href="#what-is-reinforcement-learning" class="hash-link" aria-label="Direct link to What is Reinforcement Learning?" title="Direct link to What is Reinforcement Learning?">​</a></h3><p>The image below represents the Reinforcement Learning framework. It describes in a simple, yet accurate manner, one of the main ideas behind Reinforcement Learning. </p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/reinforcement-learning-framework-d89af397a278199d33979dfa5541b2ef.jpg" width="700" height="270" class="img_ev3q"></p><p>Figure 1 : The Reinforcement Learning framework</p></div><br><p>Basically, an agent receives information about the state of an environment he evolves in, information we will call S<sub>t</sub> as it describes the state at timestep t.</p><p>Based on this information it receives, the RL agent will choose an action among all the actions it has the right to take at each timestep. We will call such action A<sub>t</sub>, the action at time t, with A<sub>t</sub> belonging to AA<sub>t</sub>(s<sub>t</sub>) the set of available actions given the state S<sub>t</sub>. When an action is taken, it has an impact on the environment and the agent will receive information about the new state of the environment S<sub>t+1</sub> but also a reward to incentivize it to take actions which will maximize the total rewards it expects to obtain at the end of an episode.</p><p>To apply reinforcement learning to solve business problems, these problems have to be framed as a Markov Decision Process, as seen above. More details can be found on how to rigorously define the Reinforcement Learning in the <a href="https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver" target="_blank" rel="noopener noreferrer">excellent course given by David Silver</a> (principal research scientist at DeepMind, now owned by Google). You may find his lectures using the previous link, with lectures 1 and 2 being the most pertinent.</p><p>Now, to get a better grasp on how to frame a problem for Reinforcement Learning, let’s consider two practical examples.</p><p>As a first example, we can consider for instance an AI trader. It could have as available actions the possibility to buy or sell many different products. Its actions have an impact on the environment. First, the money it has and the products it owns will be modified, but also if it buys a massive amount of a certain product, it may have an important impact on the future prices. The final goal for it may be to earn as much money as possible. The description of this first example with the prism of Reinforcement Learning is given in figure 2.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Example_AI_Trader_no_logo-c97aec8b49d02c0c4046c4a7da82b008.png" width="1205" height="358" class="img_ev3q"></p><div align="center"> Figure 2 - Example of an AI trader described through the prism of Reinforcement Learning</div><br><p>For the case of a self driving car as the AI agent, the actions it can take could be turning, stopping, accelerating. The information it will receive at each timestep are the speed of the car, its geolocation and probably many others. The environment can be the real world around the car, or just a simulator. The final reward will take into account how fast the car has reached a certain goal position, without damaging things or killing people for instance. Should it damage objects, it could for instance receive negative rewards. This information is summarized in the figure 3 below.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Example_self_driving_car_no_logo-dbd8ade71962beb920fe6122073a96d0.png" width="1205" height="358" class="img_ev3q"></p><div align="center"> Figure 3 - Example of a self driving car described through the prism of Reinforcement Learning</div><br><p>Let’s now tackle the case of the multi-knapsack problem!</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-to-adapt-the-multi-knapsack-problem-for-solving-with-reinforcement-learning">How to adapt the multi-Knapsack problem for solving with Reinforcement Learning<a href="#how-to-adapt-the-multi-knapsack-problem-for-solving-with-reinforcement-learning" class="hash-link" aria-label="Direct link to How to adapt the multi-Knapsack problem for solving with Reinforcement Learning" title="Direct link to How to adapt the multi-Knapsack problem for solving with Reinforcement Learning">​</a></h3><p>The precise definition of the multi-knapsack problem was given in the first part of this serie of articles on the knapsack problem. The figure below describes visually the problem at stake.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Knapsack_problem_5-31f9629281a4c78ff57ea1b68c6f753e.png" width="1306" height="1033" class="img_ev3q"></p><div align="center"> Figure 4 - Description of the multi-knapsack problem</div><br><p>In our case, one could think about the agent as a person trying to carefully choose among the many clothes he/she possesses before going on a long trip. The environment would be the empty bags and all the clothes to choose from. At each timestep, the person would have the choice to take one element among the available clothes to put it inside one of the bags, the bags needing to be closed (and thus not to full) before leaving for the trip.</p><p>The objective is to maximize the value of the clothes chosen for the trip.</p><p>And that’s it! Our problem is framed for Reinforcement Learning.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="why-we-chose-to-explore-rl-for-combinatorial-optimization-problems">Why we chose to explore RL for combinatorial optimization problems<a href="#why-we-chose-to-explore-rl-for-combinatorial-optimization-problems" class="hash-link" aria-label="Direct link to Why we chose to explore RL for combinatorial optimization problems" title="Direct link to Why we chose to explore RL for combinatorial optimization problems">​</a></h3><p>The last two decades have known the breakthrough of Deep Learning which is now massively entering all fields of industry whether this is for Computer Vision, disease predictions, product recommendation, Natural Language Processing applications, etc. Massive investments follow in the field of Machine Learning implying a virtuous circle with more results and regular new breakthroughs. Due to these developments, Deep Reinforcement Learning has emerged from the field of Reinforcement Learning which has been studied for a long time and whose goal is to take actions in an environment in order to maximize a defined cumulative reward. This allowed new recent breakthroughs, such as the AI AlphaGo beating professional Go players in 2016 and more recently AlphaStar beating world champions of the video game Starcraft (more on that in <a href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii" target="_blank" rel="noopener noreferrer">DeepMind blog article</a> or in the <a href="https://www.nature.com/articles/s41586-019-1724-z.epdf?author_access_token=lZH3nqPYtWJXfDA10W0CNNRgN0jAjWel9jnR3ZoTv0PSZcPzJFGNAZhOlk4deBCKzKm70KfinloafEF1bCCXL6IIHHgKaDkaTkBcTEv7aT-wqDoG1VeO9-wO3GEoAMF9bAOt7mJ0RWQnRVMbyfgH9A%3D%3D" target="_blank" rel="noopener noreferrer">Nature paper</a>). </p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/AlphaStar_Image-a20c844be2e52c1c27cfe7ac9e713edf.png" width="800" height="450" class="img_ev3q"></p><p>Figure 5 - Visualization highlighting the trained AlphaStar performing against a top Starcraft human players</p></div><br><p>We’ll say a few words about this video game environment, because this achievement is one of the reasons which motivates the use of Reinforcement Learning for solving combinatorial optimization problems. Indeed, with the knapsack problem, we have a discrete action space with a limited number of actions, although the range of available actions can become extremely high by changing the number of available items and knapsacks. </p><p>Having an AI agent beating the world’s best players on this game is an important breakthrough as this video game environment is extremely complex, with only imperfect information being provided to the agent, the action space being enormous with a choice between up to 10<sup>26</sup> different actions, and actions being taken almost in real time, every 0.2 seconds. Eventually, planning is made on long term and the agent doesn’t know until the end of the game whether it has won the game or not. While applying Deep Reinforcement Learning to video games allows to test the performance of the algorithms very accurately, allowing to judge how it performs in very different environments, applications also begin to appear in other fields, opening the perspective of using these techniques in different industry fields in the next few years. Deep Reinforcement Learning is definitely a field with high potential, and proofs have been shown that it can solve very well high-dimensional problems. Especially, many articles were published where these algorithms were applied to finance problems.</p><p>Now that we have seen the potential of Reinforcement Learning for solving a problem such as the knapsack problem, it is important to keep in mind some important characteristics of Deep Reinforcement Learning approaches: </p><ul><li>Reinforcement Learning algorithms provides us with approximations of the optimal solutions on the contrary to the solutions that could provide Mixed Integer Programming solvers as the ones introduced in the first part of this series of articles on the knapsack problem;</li><li>For the same reason, Reinforcement Learning algorithms will always provide us solutions to the problem, on the contrary to exact methods which could be unable to provide any solution for very complex problems. For that reason, Reinforcement Learning approaches are for instance being developed in order to solve partial differential equations of very high dimensionality, where usual solvers are unable to provide a solution;</li><li>Reinforcement Learning algorithms perform online optimization, meaning that once they have been trained, they are able to solve very complex problems immediately. They have thus tremendous potential for applications which require to solve problems very frequently in a limited time window, such as in trading or product recommendation for instance.</li></ul><p>As seen in the beginning of this section, Reinforcement Learning algorithms have a very high potential for a wide range of business problems. Let's now introduce one of the two Reinforcement Learning approaches used in this notebook. The first one, the Q-learning approach, isn't based on neural networks and doesn't scale well when the dimensionality of the problem increases. We have studied it as it is at the core of other important algorithms such as Deep Q Learning, much more powerful. We will thus concentrate on another promising approach, based on neural networks: the Policy Gradient approach.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="using-q-learning-and-policy-gradient-algorithms-on-the-knapsack-problem">Using Q-Learning and Policy Gradient algorithms on the Knapsack problem<a href="#using-q-learning-and-policy-gradient-algorithms-on-the-knapsack-problem" class="hash-link" aria-label="Direct link to Using Q-Learning and Policy Gradient algorithms on the Knapsack problem" title="Direct link to Using Q-Learning and Policy Gradient algorithms on the Knapsack problem">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="a-simple-introduction-to-policy-gradient">A simple introduction to Policy Gradient<a href="#a-simple-introduction-to-policy-gradient" class="hash-link" aria-label="Direct link to A simple introduction to Policy Gradient" title="Direct link to A simple introduction to Policy Gradient">​</a></h3><p>The basic principle with a policy gradient approach is that, for each state <em>s</em> received as input, our algorithm will provide us a probability distribution for the actions to take, allowing us to know which object our algorithm recommands us to put first in the knapsack.</p><p>In the formula below, 𝜋 gives us this probability. More precisely, 𝜋 gives us the probability to take action <em>a</em> knowing that we currently at state <em>s</em> and given the 𝜃 values of the model parameters, neurons in our case as we use neural networks. </p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/policy_gradient_equation_2-54428e6e49a7f2032f1854572929639b.svg" width="168" height="45" class="img_ev3q"></p></div><br><p>The use of neural networks isn't mandatory here, but very frequent to obtain good results on complex problems. An example of a representation of a simple neural network is given recalled below.  </p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/classical_neural_network_image_2-bc3d584b1817a99f9d27d36fd1ab8d75.png" width="692" height="411" class="img_ev3q"></p><p>Figure 6 - Representation of a simple neural network</p></div><br><p>Our model architecture can be visualized with this representation. If we dig a bit deeper into the details of our model's architecture, we have:</p><ul><li><p>As input of the neural network, a description of the current state of the system, that is the value and price information for all items, information about the already selected items and about the current and maximum weight limit inside the different bags;</p></li><li><p>Thanks to this information, our model associates to each possible action a probability and we can then select the action that the algorithm recommends us to take first, that is which object should be stored in which knapsack at the current timestep. This is the output of our model;</p></li><li><p>The parameters of this model, the neurons, are updated at the end of each episode, an episode beginning when all the objects are available and ending when the bags are sufficiently full (or all objects have been selected...). The update of the parameters (𝜃) of the model taking place at the end of each episode only and not each time an action is proposed by the model, the approach is called a <strong>Monte Carlo approach</strong>;</p></li><li><p>The updates of the parameters are made in order to maximize the value of the items stored inside the knapsacks and this approach is based on techniques such as <strong>stochastic gradient descent</strong>.</p></li></ul><p>Now that the Policy Gradient has been described, let's see how our algorithms performed on the knapsack problem!</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="evaluating-the-performance-of-the-rl-algorithms">Evaluating the performance of the RL algorithms<a href="#evaluating-the-performance-of-the-rl-algorithms" class="hash-link" aria-label="Direct link to Evaluating the performance of the RL algorithms" title="Direct link to Evaluating the performance of the RL algorithms">​</a></h3><p>As summarized in figure 7 below, in order to evaluate the performance of the different algorithms, we chose to apply our two RL algorithms (Q-Learning and Policy Gradient) to 3 different environments of increasing difficulty. We trained each algorithm over 400 episodes.</p><p>At the beginning of a new experience, the algorithm had all its coefficients reinitialized. We perform several experiences in order to evaluate how robust is the algorithm. </p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/evaluation_rl_algos_2-4e81b4faf5f33e55158b446b50a0d583.png" width="3104" height="1218" class="img_ev3q"></p><div align="center"> Figure 7 - Description of the evaluation process for the different algorithms</div><br><p>Eventually, we evaluated the performance of the algorithms with 3 metrics:</p><ul><li>The mean reward shows how good on average the algorithm is;</li><li>The standard deviation highlights the potential lack of robustness of the algorithm;</li><li>The performance ratio RL vs MILP tells us how close is the RL algorithm to the optimal solution provided by a MILP solver (details on how to obtain such a solution are given in the notebook). </li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="results-with-q-learning-no-neural-networks">Results with Q-Learning (no neural networks)<a href="#results-with-q-learning-no-neural-networks" class="hash-link" aria-label="Direct link to Results with Q-Learning (no neural networks)" title="Direct link to Results with Q-Learning (no neural networks)">​</a></h3><p>The graphs on the left show that overall our Q-Learning algorithm does indeed improve through training as its reward improved over time. However, we can see that the performance ratio RL vs MILP is very low, meaning that it is far from achieving as good results as what we could get using state-of-the-art MILP solvers.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/performance_q_learning_knapsack_4-9dde740d1774022a4a3d0668e3ae8e78.png" width="3262" height="1697" class="img_ev3q"></p><div align="center"> Figure 8 - Performance of the Q-Learning algorithm</div><br><p>Furthermore, environments of increasing complexity / dimensionality will be much more difficult to handle for Q-Learning, as its Q-value matrix has as number of columns the number of items multiplied by the number of knapsacks and as rows all the possible states which could exist. Increasing only slightly the number of knapsacks or bags will thus quickly make the Q-Learning algorithm unusable.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="results-with-policy-gradient-based-on-neural-networks">Results with Policy Gradient (based on neural networks)<a href="#results-with-policy-gradient-based-on-neural-networks" class="hash-link" aria-label="Direct link to Results with Policy Gradient (based on neural networks)" title="Direct link to Results with Policy Gradient (based on neural networks)">​</a></h3><p>The results of the learning process with Policy Gradient are much better than with Q-learning. At the beginning, due to random initialization of the neural network parameters, the actions are taken at random and the reward is very low, but it quickly improves until reaching a local maximum, not global as it is still lower than the solution obtained with the MILP solver. </p><p>The performance ratio is quite good on the three different environments, reaching approximately 80% for each. The algorithm scales well when the complexity increases.</p><p>We see however that the standard deviation is quite high, which highlights the fact that each time a model is initialized, it can converge to quite different values. It is thus not extremely robust and several initializations are required before finding good results approaching the optimal solution.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/performance_policy_gradient_knapsack_2-dc5ac694619364f3bad0e9927fe54ad1.png" width="2944" height="1674" class="img_ev3q"></p><div align="center"> Figure 9 - Performance of the REINFORCE (policy-gradient approach) algorithm</div><br><p>On the graph below are highlighted some of the limitations we have witnessed with Policy Gradient algorithms such as the REINFORCE algorithm. We have a lack of robustness, having our algorithm sometimes working very well, sometimes leading to a poorer reward.</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/robustness_pb_policy_gradient_2-ccd31fb5440a6da8290fa43122747787.png" width="713" height="394" class="img_ev3q"></p><p>Figure 10 - REINFORCE algorithm (gradient-policy approach) appears as lacking robustness</p></div><br>For that reason, the hyperparameter tuning is made more complicated. Indeed comparing one combination of hyperparameters with another one isn't enough to be certain about which combination of hyperparameters is the best, because of the high variability of results for fixed hyperparameters.<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="opening">Opening<a href="#opening" class="hash-link" aria-label="Direct link to Opening" title="Direct link to Opening">​</a></h2><p>For obtaining the results given in this article, we reproduced classical Reinforcement Learning algorithms: the Q-learning algorithm which does not rely on the use of neural networks, and a Policy Gradient algorithm which relies on neural networks. We saw that the latter obtained much better results than the former as we could expect. </p><p>We also built our own multi-knapsack environment file, allowing us to easily modify greatly the complexity of the environment by increasing both the number of available items to put in the knapsacks and the number of knapsacks. </p><p>While creating the environment file, we followed the nomenclature proposed by Open AI Gym for building Reinforcement Learning environments, using the same method names used to define an Open AI Gym environment. The objective was to be able to experiment much more quickly in the future by making use of one of the different Deep Reinforcement Learning libraries (Stable Baselines, TF Agents, Tensorforce…). Indeed, these libraries allow access to many different advanced Deep Reinforcement Learning algorithms already implemented, which can directly be used on new problems if the environment file describing the problem has been built using Open AI Gym nomenclature.</p><p>Another article will be written soon to tell more about how to perform hyperparameter tuning for RL using the hyperparameter optimization framework <a href="https://optuna.org/" target="_blank" rel="noopener noreferrer">Optuna</a> and how to compare and evaluate the efficiency of many different RL algorithms using <a href="https://stable-baselines3.readthedocs.io/en/master/" target="_blank" rel="noopener noreferrer">Stable Baselines</a>!</p></div>]]></content:encoded>
            <category>Operational Research</category>
            <category>Optimization</category>
            <category>Knapsack problem</category>
            <category>Deep Reinforcement Learning</category>
        </item>
        <item>
            <title><![CDATA[Deep RL and Optimization applied to Operations Research problem - 1/2 Traditional Optimization techniques]]></title>
            <link>https://ekimetrics.github.io/blog/2022/08/27/traditional_or</link>
            <guid>https://ekimetrics.github.io/blog/2022/08/27/traditional_or</guid>
            <pubDate>Sat, 27 Aug 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[This article is part of a series of articles which will introduce several optimization techniques, from traditional (yet advanced) Mathematical Optimization solvers and associated packages to Deep Reinforcement Learning algorithms, while tackling a very famous Operations Research problem: the multi-knapsack problem. Here, the focus is on traditional optimization techniques.]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/plitvice_lakes-42deebd7f534beb1bbb32a4b95939b6c.jpg" width="5000" height="3654" class="img_ev3q"></p></div><div align="justify"><p>In this first article is introduced a systematic way to approach and solve optimization problems. Then, the multi-knapsack problem itself is introduced. Then we apply the rules defined before on how to solve optimization problems and obtain the optimal solution to the multi-knapsack problem, formulated as a Mixed Integer problem and using Python-MIP package. Let's now introduce simple steps one can follow to approach optimization problems with optimization solvers.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="main-steps-while-creating-an-optimization-model-to-solve-a-business-problem">Main steps while creating an optimization model to solve a business problem<a href="#main-steps-while-creating-an-optimization-model-to-solve-a-business-problem" class="hash-link" aria-label="Direct link to Main steps while creating an optimization model to solve a business problem" title="Direct link to Main steps while creating an optimization model to solve a business problem">​</a></h2><p>Once a business problem that could benefit from optimization has been identified, we can define a systematic approach based on 3 steps for solving all kind of optimization problems with optimization solvers. These 3 steps are highlighted in the figure below.</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/3_steps_math_modelling_4-679cfdabcdf2ab022e54b439d00a8992.png" width="2670" height="567" class="img_ev3q"></p><p>Figure 1 : The 3 main steps for solving a business problem through optimization</p></div><br><p>In more details, these 3 steps are: </p><ol><li><p><strong>Create the conceptual mathematical model</strong> that defines the different variables, constraints, etc. in the business problem. This step consists in writing down on paper the equations that define our problem. </p></li><li><p><strong>Translate the conceptual mathematical model into a computer program</strong>. For most programming languages used for optimization, the computer program will largely resembles the mathematical equations one would write on paper.</p></li><li><p><strong>Solve the mathematical model using a math programming solver</strong>. The solver available for Mathematical Programming (solvers such as GLPK, Gurobi, CPLEX...) relies on very sophisticated algorithms. Important algorithms and ideas used in these solvers are, among many others: simplex method, branch &amp; bound, use of heuristics...</p></li></ol><p>Let's see those 3 steps for the case of the multi-knapsack problem.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-multi-knapsack-problem">The multi-knapsack problem<a href="#the-multi-knapsack-problem" class="hash-link" aria-label="Direct link to The multi-knapsack problem" title="Direct link to The multi-knapsack problem">​</a></h2><p>The objective here is, given a set of <em>n</em> items and a set of <em>m</em> knapsacks, to <strong>maximize</strong> the total value of the items put in the knapsacks without exceeding their capacity.</p><p>Below,  w<sub>i</sub> represents the weight of item i,  p<sub>i</sub> the value of item i while  c<sub>j</sub> represents the capacity of knapsack j.</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Knapsack_problem_5-31f9629281a4c78ff57ea1b68c6f753e.png" width="1306" height="1033" class="img_ev3q"></p><p>Figure 2: Description of the multi-knapsack problem</p></div><br><p>The multi-knapsack is an extension of the classical knapsack problem where instead of considering only one knapsack, we consider as many as we want. This allows to easily extend the complexity of this problem.</p><p>While the problem is relatively easy to define mathematically, it belongs to the class of NP-hard problems. Without going into the details of what defines NP-hard problems, we can easily see that the complexity of the knapsack problems explodes when the number of knapsacks and items increases. Indeed, we have m<sup>n</sup> available combinations we would need to test should we want to apply a brute-force approach for solving this problem. Just with 10 knapsacks and 80 items, there are 10<sup>80</sup> combinations, which is the estimation of the number of atoms in the universe! And 10 knapsacks and 80 items is still quite limited... Let's now try to create the conceptual mathematical model by defining the problem with equations.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="creating-the-conceptual-mathematical-model">Creating the conceptual mathematical model<a href="#creating-the-conceptual-mathematical-model" class="hash-link" aria-label="Direct link to Creating the conceptual mathematical model" title="Direct link to Creating the conceptual mathematical model">​</a></h3><p>A quick translation of the multi-knapsack problem with equation can be written as the following: </p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/equations_1-2820b99a846626f6fcc64d2dceada7cb.svg" width="262" height="161" class="img_ev3q">
<img loading="lazy" alt="screenshot-app" src="/assets/images/equations_3-1bdaad7b88386e3cf9e22918cfb14960.svg" width="201" height="19" class="img_ev3q"></p></div><p>Now that we managed to translate the problem into a set of equations, let's translate this mathematical model so that it is understood by a computer program. Below, we will make use of the Python package <a href="https://www.python-mip.com/" target="_blank" rel="noopener noreferrer">Python-MIP</a> which is open-source and provides tools for modeling and solving Mixed-Integer Linear Programming Problems (MIP), relying on fast open source solvers.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="translating-the-mathematical-model-into-a-computer-program-with-python-mip">Translating the mathematical model into a computer program with Python-MIP<a href="#translating-the-mathematical-model-into-a-computer-program-with-python-mip" class="hash-link" aria-label="Direct link to Translating the mathematical model into a computer program with Python-MIP" title="Direct link to Translating the mathematical model into a computer program with Python-MIP">​</a></h3><p>Before solving the problem, we have to generate an instance for it (have data defining the problem). To do so, you can use the following code that will generate an instance of this problem with 40 items to store in 5 bags.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">import pandas as pd</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">import numpy as np</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">import pickle</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">def data_generator_knapsack(number_bags, number_items, minimum_weight_item, maximum_weight_item, minimum_value_item, maximum_value_item, max_weight_bag):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data = {}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    weights = np.random.randint(minimum_weight_item, maximum_weight_item, size = number_items)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    values = np.random.randint(minimum_value_item, maximum_value_item, size = number_items)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data['weights'] = weights</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data['values'] = values</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data['items'] = list(range(len(weights)))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data['num_items'] = len(weights)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data['bins'] = list(range(number_bags))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data['bin_capacities'] = np.random.randint(0, max_weight_bag, size = number_bags) + np.int(np.mean(data['weights']))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return(data)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">number_bags = 5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">number_items = 40</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">minimum_weight_item = 0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">maximum_weight_item = 75</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">minimum_value_item = 0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">maximum_value_item = 75</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">max_weight_bag = 150</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">data = data_generator_knapsack(number_bags, number_items, minimum_weight_item, maximum_weight_item, minimum_value_item, maximum_value_item, max_weight_bag)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Let's now import the package used to have access to the MIP solver, here using the python package Python-MIP:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">from mip import Model, xsum, maximize, BINARY</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Now, we can translate the mathematical model so that it is understood by Python-MIP. </p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">def mip_solve_knapsack(data):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  model = Model("knapsack")</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  x = [[model.add_var(var_type=BINARY) for i in data['items']] for j in data['bins']]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  model.objective = maximize(xsum((xsum(data['values'][i] * x[j][i] for i in data['items']) for j in data['bins'])))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  for j in data['bins']:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      model += xsum(data['weights'][i] * x[j][i] for i in data['items']) &lt;= data['bin_capacities'][j]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  # Each item can be in at most one bin</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  for i in data['items']:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      model += xsum(x[j][i] for j in data['bins']) &lt;= 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  model.optimize()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  return(model)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Remark how close it is from the original equations! These solvers are very powerful and yet easy to use directly in Python. The code is indeed very close to the original equations. </p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="solving-the-mathematical-model-with-python-mip">Solving the mathematical model with Python-MIP<a href="#solving-the-mathematical-model-with-python-mip" class="hash-link" aria-label="Direct link to Solving the mathematical model with Python-MIP" title="Direct link to Solving the mathematical model with Python-MIP">​</a></h3><p>Using the <strong>mip_solve_knapsack</strong> function defined in the previous section, we can access to important information regarding the problem, such as the final objective value and the values of x<sub>ij</sub> telling us what were the best combinations of items inside knapsacks.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="some-mathematical-optimization-packages">Some Mathematical Optimization packages<a href="#some-mathematical-optimization-packages" class="hash-link" aria-label="Direct link to Some Mathematical Optimization packages" title="Direct link to Some Mathematical Optimization packages">​</a></h3><p>In the notebook associated to this article, the package Python-MIP was used. <strong>Python-MIP</strong> is free, but many other packages exist for solving optimization problems on Python (and other languages of course like Julia). For instance <strong>OR-Tools</strong> from Google is a well-recognized free solver, with <a href="https://developers.google.com/optimization/introduction/overview" target="_blank" rel="noopener noreferrer">detailed documentation</a>. </p><p>On the other side, <strong>Gurobi</strong> is a very popular commercial solution for mathematical optimization and its documentation is extremely rich, with quick introductions about <a href="https://www.gurobi.com/resource/modeling-basics/" target="_blank" rel="noopener noreferrer">Mathematical Programming</a>, <a href="https://www.gurobi.com/resource/mip-basics/" target="_blank" rel="noopener noreferrer">Linear Programming</a> and <a href="https://www.gurobi.com/resource/mip-basics/" target="_blank" rel="noopener noreferrer">Mixed-Integer Programming</a>. Importantly, it has a <a href="https://www.gurobi.com/resource/modeling-examples-using-the-gurobi-python-api-in-jupyter-notebook/" target="_blank" rel="noopener noreferrer">large number of modeling examples from all industry fields</a> directly available on Google Colab allowing to better grasp notions of Mathematical Modelling and to improve modeling skills to tackle all kind of optimization problems with Python. This resource can be of use even if one doesn't plan to use this commercial software but rather a free package such as OR-Tools.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><p>In this article was introduced the multi-knapsack problem, an NP-complete problem, very difficult to solve when taking many items and bags. </p><p>The approach to solve the multi-knapsack problem relied on Python-MIP, a free optimization package using powerful MILP solvers to solve very efficiently all kinds of optimization problems.</p><p>In the next part of this series on the multi-knapsack problem, well studied in the field of Operations Research and at the heart of many real optimization problems, we'll highlight how Deep Reinforcement Learning can be used in order to solve combinatorial optimization problems such as this one. Stay tuned!</p></div>]]></content:encoded>
            <category>Operational Research</category>
            <category>Optimization</category>
            <category>Knapsack problem</category>
            <category>Solvers</category>
        </item>
        <item>
            <title><![CDATA[Exploring neural ordinary differential equations for time series forecasting applications]]></title>
            <link>https://ekimetrics.github.io/blog/2022/07/11/neural_ode</link>
            <guid>https://ekimetrics.github.io/blog/2022/07/11/neural_ode</guid>
            <pubDate>Mon, 11 Jul 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[Enhance your knowledge on Deep Learning techniques by understanding what neural ODEs are and how we could benefit from them.]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/neural_ode_cover-d93f3ae869aeba9fcfc6c2b522a9665e.png" width="815" height="543" class="img_ev3q"></p></div><div align="justify"><p>For decades, time series forecasting has been a popular topic among the scientific community related to data science and artificial intelligence (AI). The reason behind this is that time series, data measured over time, are omnipresent in our day-to-day life and are used in a wide range of industries. For instance, in the retail industry, forecasting is a crucial task to optimize processes and increase efficiency. Many examples can be found in the literature mainly focusing on demand forecasting <!-- -->[1]<!-- --> and sales forecasting <!-- -->[2]<!-- -->.</p><p>In the past, retail companies have relied on traditional time series forecasting approaches based on statistics. More recently, companies and researchers have grown interest in machine learning and deep learning techniques and algorithms to predict the demand more accurately among other requirements. To evolve as a leader in data science, at Ekimetrics, we keep an eye on the state-of-the-art of time series forecasting and investigate new techniques to eventually, adopt them and improve our solutions in the long run.</p><p>This article will focus on our journey with <strong>neural ordinary differential equations</strong> (neural ODEs) applied to time series forecasting and specifically focused on a client’s use case.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="selling-magazines">Selling magazines<a href="#selling-magazines" class="hash-link" aria-label="Direct link to Selling magazines" title="Direct link to Selling magazines">​</a></h2><p>Before entering into the subject, if you have not read our article on how we use Bayesian inference to increase our predictions of magazines sales have a look at it <a href="https://ekimetrics.github.io/blog/2022/06/14/bayesian_inference/" target="_blank" rel="noopener noreferrer">here</a>. It is an interesting application of a Bayesian framework to enhance sales prediction for a publisher. Our exploration of neural ODEs focuses on the same use case.</p><p>In the world of newspapers and magazines, it is crucial to be able to estimate the final volume of sales for accounting and management purposes. Indeed, publishers adjust their production and distribution of issues based on these forecasts. Conventionally, the projections rely strongly on the domain expertise of the people in charge. But nowadays, with all recent advances in the field of data science and improvements with regards to data availability, publishers can leverage modern forecasting techniques to enhance their predictions.  </p><p>The figure below, Fig. 1, represents the cumulative sales of a magazine composed by a set of 14 different issues. Note that the data have been anonymized for confidentiality purposes. Our task is to use the observed data, including the sales curves of all historical issues and the beginning part of the current issue’s curve, to predict the final sales of the current issue.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/image_1_bis-3e321c685521a1b2a048616b9c8b7bfd.png" width="1498" height="472" class="img_ev3q"></p><div align="center"> Fig. 1: Example of magazine sales curves</div><br><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="introduction-to-neural-odes">Introduction to neural ODEs<a href="#introduction-to-neural-odes" class="hash-link" aria-label="Direct link to Introduction to neural ODEs" title="Direct link to Introduction to neural ODEs">​</a></h2><p>In this section we provide an overview of what neural ODEs are, and what makes them so special. For the curious readers, complementary resources and blog posts that present this topic in detail, are provided at the end of this article.</p><p>To understand neural ODEs, we need to introduce some basic concepts on neural networks (NNs). In essence, NNs are a succession of input, hidden and output layers originally inspired by the human brain. The Multilayer Perceptron (MLP) is one the most popular feedforward neural network due to its simplicity. By means of activation functions and linear combinations of the input data, these networks introduce non linearities and become universal approximators <!-- -->[3]<!-- -->. The equation h(t+1) = f(ht, θt), captures the evolution of the hidden layers of a MLP network with ht referring to the previous layer and θt to a parameter of the network. Without entering too much into the details, these networks are usually trained by minimizing a loss function and backpropagation, propagating the computed error, and updating the weights accordingly.</p><p>What we must keep in mind here is that these networks have a limited number of layers and thus, of evaluation points. Could we achieve an infinite number of evaluation points for time continuous applications? The instinctive answer is no since we cannot have an infinite number of layers. However, we are going to review some interesting properties of residual networks (ResNet) that will help us answer this question.</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/image_2-0a89ff1fb52a6ccd12d1667b711b9f23.png" width="385" height="216" class="img_ev3q"></p><p>Fig. 2: ResNet schema <!-- -->[4]</p></div><br><p>A ResNet is a neural network that has an architecture based on the pattern pictured in Fig. 2 which entails that the network follows the equation h<sub>t+1</sub> = h<sub>t</sub> + f(h<sub>t</sub>, θ<sub>t</sub>). This equation might be familiar to some readers with a background in mathematics since it resembles Euler’s method. In short, this method is useful to solve initial value problems, a problem modelled by an ordinary differential equation with an initial condition. Interestingly, a residual network adds hidden states between the input and the output and discretizes the continuous relationship between them, which is similar to Euler's method. In <!-- -->[5]<!-- --> the authors take a different approach by considering the continuous limit of each discrete layer of the network. Thus, instead of having a discrete number of layers, the progression of the hidden states is continuous, obtaining the following equation where h(t) is the value of the hidden state evaluated for some t.</p><div align="center"> dℎ(𝑡)/𝑑𝑡 = 𝑓(ℎ(𝑡), 𝜃<sub>t</sub>, 𝑡)</div><br><p>Then, in a neural ODE the hidden state dynamic can be parametrized by the equation above where 𝑓(ℎ(𝑡), 𝜃<sub>t</sub>, 𝑡) is a neural network parametrized by 𝜃<sub>t</sub> at layer t. Therefore, it is possible to solve this ODE by solving its integral.</p><div align="center"> h(t) = ∫ f(h(t), 𝜃<sub>t</sub>, t)</div><br><p>By means of a numerical ODE method, it is possible to evaluate the network at any desired depth. And thus, it is possible to approximate functions over these hidden state dynamics by using ODE solvers such as:</p><div align="center"> ŷ = h(t<sub>1</sub>) = ODESolve(h(t<sub>0</sub>), t<sub>0</sub>, t<sub>1</sub>, 𝜃<sub>t</sub>, f)</div><br><p>Now, if we go back to our initial question regarding the possibility of having an infinite or quasi-infinite number of evaluation points, we can state that neural ODEs enable us to achieve that Fig. 3 illustrates this idea by comparing the vector field created by a traditional residual network and an ODE network. In essence, an ODE network defines a continuous vector field since this network can be evaluated at any depth. Whereas, on the other hand, the residual network is limited to a discrete number of layers.</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/image_3-2d9dce2581ec5d66cb7908b0f4c091bb.png" width="472" height="333" class="img_ev3q"></p><p>Fig. 3: Comparison between a ResNet and an ODE network vector fields transformations <!-- -->[5]</p></div><br><p>Before moving on to the applications of neural ODEs, it is important to note the endeavor of performing backpropagation. Essentially, the more evaluation points you have in your network, the higher the number of intermediate forward passes to store. To overcome this, the authors of the original paper <!-- -->[5]<!-- --> which received the 2018 NeurIPS best paper award, introduced a mathematical trick known as the Adjoint method. We will not get into the details here, but you can dig deeper into this concept by checking the resources we have left at the end of this article.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="applications-of-neural-odes">Applications of neural ODEs<a href="#applications-of-neural-odes" class="hash-link" aria-label="Direct link to Applications of neural ODEs" title="Direct link to Applications of neural ODEs">​</a></h2><p>The main advantage of neural ODEs is the possibility of working with continuous-time series models. In addition, they are generally more memory efficient than other networks such as the MLP and can gain from adaptive computation since modern ODE solvers allow to monitor the level of error and adapt the evaluation strategy. For example, we can reduce accuracy for low power applications.</p><p>All these benefits make neural ODEs a great tool for applications dealing with continuous time systems, irregularly sampled data, and scalable and invertible normalizing flows. Besides, neural ODEs are very interesting for time series forecasting too. Basically, they can be used to fit time series and then extrapolate them. Since 2018, we have seen an increasing number of scientific papers presenting neural ODEs to deal with forecasting challenges in the energy <!-- -->[6]<!-- --> and the healthcare <!-- -->[7]<!-- --> sectors.</p><p>Coming back to our specific use case, although Fig. 1 introduced at the beginning of this article shows that applying a curve fitting model does not seem like a feasible option, we considered our sales curves as trajectories to employ neural ODE models to fit the curves and forecast the final volumes of sales. We can formulate our problem with the following equation.</p><div align="center"> xθ<sub>t+1</sub> = xθ<sub>0</sub> + ∫ b(xθ<sub>u</sub>, u) du</div><br><p>Where x is an issue belonging to a magazine and b in our case is a neural network, something resembling to b(xθ<sub>u</sub>, u) = MLP(xθ<sub>u</sub>), with its corresponding parameters.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="results">Results<a href="#results" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results">​</a></h2><p>Initially, we have used a standard neural ODE network to deal with our time series forecasting use case. The figure below, Fig. 4, is an animation representing the fitting of a curve of sales of a specific issue. Bear in mind that the data have been anonymized for privacy purposes.</p><p>In this figure, we can observe that the model is able to fit the curve successfully after some iterations. The idea then is to fit the sales curves from all historical issues and the beginning part of the current issue’s curve to predict the final sales by extrapolation. Although we have obtained promising results, they are not good enough. The main reason being that neural ODEs are well suited for deterministic systems and in our case, we deal with a non-deterministic one.</p><div align="center"><p><img loading="lazy" alt="screenshot-app" src="/assets/images/curve_fitting-caed00a370c6fac7f7475e43da4b16aa.gif" width="864" height="288" class="img_ev3q"></p><p>Fig. 4: Curve fitting of the sales curve of an issue</p></div><br><p>Inspired by the ideas presented in <!-- -->[8]<!-- -->, instead of a standard neural ODE network like the one introduced before, we have been recently working with stochastic differential neural networks (SDEs). SDEs are a type of continous neural network enabling to introduce a stochastic component and consequently, work with non-deterministic systems and consider external factors. We have obtained promising preliminary results with this procedure, but they require further analysis.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><p>In this article, we have introduced you into the world of neural ordinary differential equations applied to time series forecasting. We have gone through a short theoretical explanation and presented you our exploration with neural ODEs to strengthen predictions for a magazine publisher.</p><p>At Ekimetrics, we accompany a large portfolio of clients across a wide range of industries to help them steer their data opportunity, build capabilities, and deploy actionable DS solutions to power up a sustainable growth. As a leader in the field of data science, we perform recurring technological watches and stay tuned on the state-of-the-art of the fields of machine learning and deep learning. As stated during this article, we are particularly interested in time series and investigate recent advances on the subject to eventually, embrace them and expand our solutions.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">​</a></h2><p>[1]<!-- --> J. Wang, G. Q. Liu, and L. Liu, "A selection of advanced technologies for demand forecasting in the retail industry", <a href="https://ieeexplore.ieee.org/document/8713196" target="_blank" rel="noopener noreferrer">A Selection of Advanced Technologies for Demand Forecasting in the Retail Industry | IEEE Conference Publication | IEEE Xplore.</a></p><p>[2]<!-- --> X. dairu and Z. Shilong, "Machine learning model for sales forecasting by using xgboost", <a href="https://ieeexplore.ieee.org/document/9342304" target="_blank" rel="noopener noreferrer">Machine Learning Model for Sales Forecasting by Using XGBoost | IEEE Conference Publication | IEEE Xplore.</a></p><p>[3]<!-- --> K. Hornik, M. Stinchcombe, H. White, “Multilayer feedforward networks are universal approximators”, <a href="https://www.sciencedirect.com/science/article/abs/pii/0893608089900208" target="_blank" rel="noopener noreferrer">Multilayer feedforward networks are universal approximators.</a></p><p>[4]<!-- --> Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, "Deep Residual Learning for Image Recognition", <a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener noreferrer">Deep Residual Learning for Image Recognition.</a></p><p>[5]<!-- --> Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud, "Neural ordinary differential equations", <a href="https://arxiv.org/abs/1806.07366" target="_blank" rel="noopener noreferrer">Neural Ordinary Differential Equations.</a></p><p>[6]<!-- --> X. Xie, A. K. Parlikad, and R. Puri, "A neural ordinary differential equations based approach for demand forecasting within power grid digital twins", <a href="https://ieeexplore.ieee.org/document/8909789" target="_blank" rel="noopener noreferrer">A Neural Ordinary Differential Equations Based Approach for Demand Forecasting within Power Grid Digital Twins | IEEE Conference Publication | IEEE Xplore.</a></p><p>[7]<!-- --> Intae Moon, Stefan Groha, Alexander Gusev, "SurvLatent ODE : A Neural ODE based time-to-event model with competing risks for longitudinal data improves cancer-associated Deep Vein Thrombosis (DVT) prediction", <a href="https://arxiv.org/pdf/2204.09633.pdf" target="_blank" rel="noopener noreferrer">2204.09633.pdf (arxiv.org).</a> </p><p>[8]<!-- --> Xuechen Li, Ting-Kam Leonard Wong, Ricky T. Q. Chen, David Duvenaud, "Scalable Gradients for Stochastic Differential Equations", <a href="https://arxiv.org/abs/2001.01328" target="_blank" rel="noopener noreferrer">Scalable Gradients for Stochastic Differential Equations.</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="additional-resources">Additional resources<a href="#additional-resources" class="hash-link" aria-label="Direct link to Additional resources" title="Direct link to Additional resources">​</a></h2><ul><li><p>In-depth blog post: <a href="https://jontysinai.github.io/jekyll/update/2019/01/18/understanding-neural-odes.html" target="_blank" rel="noopener noreferrer">Understanding Neural ODE's - Jonty Sinai</a></p></li><li><p>Presentation on mathematics behind neural ODEs: <a href="https://voletiv.github.io/docs/presentations/20200402_Guelph_Neural_ODEs_tutorial.pdf" target="_blank" rel="noopener noreferrer">20200402_Guelph_Neural_ODEs_tutorial.pdf (voletiv.github.io)</a></p></li><li><p>Interesting blog post: <a href="https://towardsdatascience.com/neural-odes-breakdown-of-another-deep-learning-breakthrough-3e78c7213795" target="_blank" rel="noopener noreferrer">Neural ODEs: breakdown of another deep learning breakthrough | by Alexandr Honchar | Towards Data Science</a></p></li><li><p>Neural ODEs Github repository: <a href="https://github.com/msurtsukov/neural-ode" target="_blank" rel="noopener noreferrer">GitHub - msurtsukov/neural-ode: Jupyter notebook with Pytorch implementation of Neural Ordinary Differential Equations</a></p></li><li><p>SDEs Github repository: <a href="https://github.com/google-research/torchsde" target="_blank" rel="noopener noreferrer">GitHub - google-research/torchsde: Differentiable SDE solvers with GPU support and efficient sensitivity analysis.</a></p></li></ul></div>]]></content:encoded>
            <category>Time Series Forecasting</category>
            <category>Sales Prediction</category>
            <category>Neural Ordinary Differential Equations</category>
            <category>Deep Learning</category>
            <category>Newspaper Industry</category>
        </item>
        <item>
            <title><![CDATA[Newsletter for June 2022]]></title>
            <link>https://ekimetrics.github.io/blog/2022/07/05/newsletter_June-2022</link>
            <guid>https://ekimetrics.github.io/blog/2022/07/05/newsletter_June-2022</guid>
            <pubDate>Tue, 05 Jul 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[We are now at the start of July and we release our 5th Newsletter! Ranging from podcasts to tutorials, this Newsletter is made for practicioners!]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/beach_2-3431eb2886704dafab182b023153a8cd.jpg" width="3944" height="5916" class="img_ev3q"></p></div><div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="data-science">Data Science<a href="#data-science" class="hash-link" aria-label="Direct link to Data Science" title="Direct link to Data Science">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="distributed-computing-how-to-leverage-fugue-to-scale-your-code">Distributed computing: how to leverage Fugue to scale your code?<a href="#distributed-computing-how-to-leverage-fugue-to-scale-your-code" class="hash-link" aria-label="Direct link to Distributed computing: how to leverage Fugue to scale your code?" title="Direct link to Distributed computing: how to leverage Fugue to scale your code?">​</a></h3><p><img loading="lazy" src="/assets/images/Image_1-0806ff5e392a0687b932701ff2f722b3.png" width="481" height="292" class="img_ev3q"></p><p>As python users and data lovers, we are using pandas in our daily life. However, pandas has inner limitations when working with large datasets. Alternatives for distributed computing exist but each of them requires us to skill up. The main barrier is the syntax required by the language which is often complex. Different packages tried to simplify the process (koalas for instance) but we want to focus ourselves on Fugue. Fugue is described as a “unified interface for distributed computing that lets users execute Python, pandas, and SQL code on Spark and Dask without rewrites”. Fugue does not require a lot of time to get used to it and is a good common tool for SQL users and python ones. We invite you to watch the video here under which contextualizes Fugue and provides some useful background. </p><p><a href="https://towardsdatascience.com/why-pandas-like-interfaces-are-sub-optimal-for-distributed-computing-322dacbce43" target="_blank" rel="noopener noreferrer">Why Pandas-like Interfaces are Sub-optimal for Distributed Computing | by Kevin Kho | Jun, 2022 | Towards Data Science</a></p><p><a href="https://www.youtube.com/watch?v=b3ae0m_XTys" target="_blank" rel="noopener noreferrer">Talk - Kevin Kho/Han Wang: Comparing the Different Ways to Scale Python and Pandas Code - YouTube</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="machine-learning">Machine Learning<a href="#machine-learning" class="hash-link" aria-label="Direct link to Machine Learning" title="Direct link to Machine Learning">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="in-hand-nlp-search-tool-backed-by-transformers-and-qa-haystack">In-hand NLP search tool backed by Transformers and Q&amp;A: Haystack<a href="#in-hand-nlp-search-tool-backed-by-transformers-and-qa-haystack" class="hash-link" aria-label="Direct link to In-hand NLP search tool backed by Transformers and Q&amp;A: Haystack" title="Direct link to In-hand NLP search tool backed by Transformers and Q&amp;A: Haystack">​</a></h3><div align="center"><p><img loading="lazy" src="/assets/images/Image_2-8104a827965e0342baa852f41da7e42d.png" width="508" height="339" class="img_ev3q"></p></div><p>Deepset, a German NLP start-up, released a bit more than a year ago a powerful and simple tool: Haystack. Haystack is a comprehensive framework enabling its users to use state-of-the-art Question Answering model to browse large textual datasets. Haystack is a local and little search engine. It is meant to be used by a broad range of users (from beginners to expert). You can tailor some part of the interface and even modify back-end specificities such as the backbone model used (Bert, Roberta, Luke…). </p><p><a href="https://github.com/deepset-ai/haystack" target="_blank" rel="noopener noreferrer">GitHub - deepset-ai/haystack</a></p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="liquid-neural-network-next-generation-model-for-sequential-data">Liquid Neural Network: Next generation model for sequential data<a href="#liquid-neural-network-next-generation-model-for-sequential-data" class="hash-link" aria-label="Direct link to Liquid Neural Network: Next generation model for sequential data" title="Direct link to Liquid Neural Network: Next generation model for sequential data">​</a></h3><div align="center"><p><img loading="lazy" src="/assets/images/Image_3-36bf56e604a667be299e3f565ef6b799.png" width="442" height="293" class="img_ev3q"></p></div><p>Recurrent Neural Network (RNN) demonstrated a strong performance when used for sequential data. RNN are made of complex recurrent units enabling the network to keep track of past information while new inputs are fed in. In 2018, Neural Ordinary Differential Equations appeared and were seen as a major change in how we are modelling data. Instead of modeling the true generative function of Y, we are looking to model its derivative. This article was seen as a major change in how we can shift from a discrete model state to a continuous one.
Liquid Time Constant Networks (LTC) were introduced in 2020 with a strong emphasis on the concept of hidden state in the model. In previous models, hidden state at time t is optimized according to the output at time t (maximizing the accuracy). In LTCs, hidden state is used both to predict the output, but also the next hidden state t+1. It means that at each time-step, you update your memory, according to what you saw, see and expect to see in the future. This application is particularly powerful when you need to generate data at a given horizon. If you want a quick deep dive in LTC you can follow the first link. The second one is the original paper that might be hard to read the in first instance.</p><p><a href="https://towardsdatascience.com/liquid-neural-networks-in-computer-vision-4a0f718b464e" target="_blank" rel="noopener noreferrer">Liquid Neural Networks in Computer Vision | by Jacob Solawetz | Towards Data Science</a></p><p><a href="https://arxiv.org/pdf/2006.04439.pdf" target="_blank" rel="noopener noreferrer">Article - Liquid Time-constant Networks</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="data-engineering--architecture">Data Engineering &amp; Architecture<a href="#data-engineering--architecture" class="hash-link" aria-label="Direct link to Data Engineering &amp; Architecture" title="Direct link to Data Engineering &amp; Architecture">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="mongodb-vs-elasticsearch-vs-redshift">MongoDB vs ElasticSearch vs Redshift<a href="#mongodb-vs-elasticsearch-vs-redshift" class="hash-link" aria-label="Direct link to MongoDB vs ElasticSearch vs Redshift" title="Direct link to MongoDB vs ElasticSearch vs Redshift">​</a></h3><div align="center"><p><img loading="lazy" src="/assets/images/Image_4-86397f57fd69a12855f43f28289caf3e.png" width="475" height="317" class="img_ev3q"></p></div><p>Different tools exist when required to structure and organize a data architecture. This blog post presents three contenders to be THE tool to use. The author compares three technologies widely used: MongoDB, ElasticSearch and Amazon Redshift. This comparison will look at how they index, shard and aggregate data. While MongoDB has the biggest popularity and ElasticSearch gains some hype, Amazon Redshift is a tool to be considered.</p><p><a href="https://www.toptal.com/data-science/data-engineering-guide-to-storages" target="_blank" rel="noopener noreferrer">Storage for Data Engineering: Which is the Best? | Toptal</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="app-and-web-development">App and Web Development<a href="#app-and-web-development" class="hash-link" aria-label="Direct link to App and Web Development" title="Direct link to App and Web Development">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="using-properly-map-in-javascript">Using properly Map in Javascript<a href="#using-properly-map-in-javascript" class="hash-link" aria-label="Direct link to Using properly Map in Javascript" title="Direct link to Using properly Map in Javascript">​</a></h3><div align="center"><p><img loading="lazy" src="/assets/images/Image_5-822038c31378010232f72d6fef479b00.png" width="457" height="304" class="img_ev3q"></p></div><p>A “map” object is a new addition coming into Javascript since ES6. While the “object” is widely used for recording values, they should be avoided when you have a variable number entries (that can evolve along time). In this case, using a “map” object is appropriate and results in a significant memory saving and computational performance improvement. </p><p><a href="https://www.zhenghao.io/posts/object-vs-map" target="_blank" rel="noopener noreferrer">When You Should Prefer Map Over Object In JavaScript (zhenghao.io)</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="special-section-yolos-creator-history">Special Section: Yolo’s creator history<a href="#special-section-yolos-creator-history" class="hash-link" aria-label="Direct link to Special Section: Yolo’s creator history" title="Direct link to Special Section: Yolo’s creator history">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="jon-redmon-the-poney-lovers-and-computer-vision-boss-">Jon Redmon, the poney lovers and Computer Vision boss !<a href="#jon-redmon-the-poney-lovers-and-computer-vision-boss-" class="hash-link" aria-label="Direct link to Jon Redmon, the poney lovers and Computer Vision boss !" title="Direct link to Jon Redmon, the poney lovers and Computer Vision boss !">​</a></h3><div align="center"><p><img loading="lazy" src="/assets/images/Image_6-e013cab3ba5cfc7322fa550f7ebeb8d1.png" width="605" height="251" class="img_ev3q"></p></div><p>For anyone who worked on Computer Vision during studies or even in professional environments, Object Detection always raises as a major challenge. The task consists in detecting in a picture multiple instances and drawing a bounding box around it. Back in 2015, the state-of-the-art architecture is a Region-based Convolutional Neural Network or RCNN (followed by different improvement called Fast-RCNN, Faster-RCNN…). To make it brief and simple, RCNN takes Regions Of Interest (ROIs), which are little rectangle cropped from the image, fed to a Neural Network. Eventually ROIs are pooled together to produce the true bounding boxes we want. The biggest limit of R-CNN is the ROIs generation part which can take a lot of time before getting a prediction (therefore improvements of R-CNN always have a speed related word).</p><p>Jon Redmon, who at the time was in the early years of his PhD, proposed a new architecture called YOLO (You Only Look Once). The framework approaches the task of object detection as a regression problem rather than a classification one (R-CNN uses a classifier to predict bounding boxes). He also modifies the framework to get a single network that can be trained and optimized. This change of loss enables the network to be fully trainable on the whole image. The model shows exceptional performance both for real-time detection, but also in terms of accuracy. </p><p>Following this breakthrough, Jon Redmon worked on several new versions of YOLO, all becoming state-of-the-art in the field. However, in early 2020, he decided to stop his research in the field of Computer Vision as he saw a rise of ethical concerns around his work (especially in the military field). </p><p>I strongly invite you to read (at least) his resume which is by far the best one in the AI field! Check out his website to deepen your knowledge about his research. If you want, you can also check his first Yolo paper which is amazing!</p><p><a href="https://pjreddie.com/static/Redmon%20Resume.pdf" target="_blank" rel="noopener noreferrer">Redmon Resume (pjreddie.com)</a></p><p><a href="https://pjreddie.com/" target="_blank" rel="noopener noreferrer">Survival Strategies for the Robot Rebellion (pjreddie.com)</a></p><p><a href="https://arxiv.org/pdf/1506.02640.pdf" target="_blank" rel="noopener noreferrer">You Only Look Once: Unified, Real-Time Object Detection</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="credits">Credits<a href="#credits" class="hash-link" aria-label="Direct link to Credits" title="Direct link to Credits">​</a></h2><ul><li>Cover Photo by <a href="https://unsplash.com/@cristofer?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank" rel="noopener noreferrer">Cristofer Maximilian</a> on <a href="https://unsplash.com/@cristofer?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank" rel="noopener noreferrer">Unsplash</a></li></ul></div>]]></content:encoded>
            <category>Distributed Computing</category>
            <category>NLP</category>
            <category>Liquid Neural Network</category>
            <category>Data Engineering</category>
        </item>
        <item>
            <title><![CDATA[Bayesian inference for better predictions of magazine sales]]></title>
            <link>https://ekimetrics.github.io/blog/2022/06/14/bayesian_inference</link>
            <guid>https://ekimetrics.github.io/blog/2022/06/14/bayesian_inference</guid>
            <pubDate>Tue, 14 Jun 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[This article describes how Ekimetrics helped major press publishers using Bayesian inference for sales prediction.]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/press_printing-275d353e93a20535ba989a9392a32b02.jpg" width="6016" height="4016" class="img_ev3q"></p></div><div align="justify"><p>Whether it’s a tabloid that we quickly grabbed on the way to work, or a magazine with a free toy that we carefully selected for our kids on the way home, print newspapers have always been an integral ingredient in many people’s lives. That feeling of getting to read something fresh off the press. That powdery smell of paper. That daily reading routine to get you awake every morning. All this, however, seems to bring a sense of nostalgia to most people now. </p><p>The newspaper sector is facing a tough time. The rapid and inevitable shift to digital content across domains has significantly reduced the audience size of the traditional press. At the same time, production costs have become more and more expensive. Covid-19 and lockdowns are changing customer behaviour. Altogether, these factors are rendering the industry’s current distribution strategies obsolete. </p><p>More than ever, the industry needs to adapt itself to stay in the game. Many publishers are using data and data science techniques such as demand forecasting, sales prediction, supply optimization, and management dashboard to assist their business decisions.</p><p>In this article, we will shed a light on one of those tools: <strong>sales prediction.</strong></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="mobilis-in-mobile-jules-verne">Mobilis in Mobile (Jules Verne)<a href="#mobilis-in-mobile-jules-verne" class="hash-link" aria-label="Direct link to Mobilis in Mobile (Jules Verne)" title="Direct link to Mobilis in Mobile (Jules Verne)">​</a></h2><p><img loading="lazy" alt="screenshot-app" src="/assets/images/pen_paper-10343240b46de3a44291a0301ee5bd41.png" width="1080" height="675" class="img_ev3q"></p><p>When a publisher sells a magazine issue, they need to estimate the total sales for accounting, financial and management purposes. For example, in accounting, the total sales are needed to calculate the estimated revenue and profit, which may later inform decision-makers to adjust the production and distribution of the next issues. Traditionally, the predictions are based on the domain expertise of the people in charge. However, with data becoming more and more available, we can leverage recent advances in machine learning to improve our predictions.</p><p>Fig. 1 shows the cumulative sales of a magazine (the data have been anonymized for confidential purposes). The objective is to use the observed data, including the sales curves of all historical issues and the beginning part of the current issue’s curve, to predict the final sales.  </p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/sales_curves_3-3e321c685521a1b2a048616b9c8b7bfd.png" width="1498" height="472" class="img_ev3q"></p><div align="center"> Fig. 1: Example of magazine sales curves</div><br><p>As we can see from the figure, there is a high variability in the sales curves. Averaging the sales of historical issues does not provide a good prediction of the current issue. Naively applying a curve fitting model does not seem like a feasible option either. Furthermore, we do not have that much data to train a data-hungry model such as deep neural network. </p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="context-and-domain-expertise-are-crucial">Context and domain expertise are crucial!<a href="#context-and-domain-expertise-are-crucial" class="hash-link" aria-label="Direct link to Context and domain expertise are crucial!" title="Direct link to Context and domain expertise are crucial!">​</a></h2><p>In the real world, the sales of a magazine issue depends on several factors, such as the content and the cover of the issue (e.g. the better the content and the covers are, the more sales), the season (e.g. some magazine may sell better in summer), the holidays during the sale period (e.g. people may buy a magazine at the train station when they go on vacations). A good prediction model needs to consider those factors to be able to make precise forecasts.</p><p>There are two big questions: <em>which factors to take into account and how to encode them?</em></p><p>To answer the first question, we need the insights of domain experts. They have been working in the industry for many years. More than anyone else, they understand their magazines, the market and customer behaviour. That knowledge is invaluable!</p><p>For the second question, there is a mathematical framework that allows us to incorporate the prior knowledge (provided by the domain experts and extracted from the data of historical issues) and the observations (the daily reported sales of the current issue) to make a better prediction: <strong>Bayesian inference</strong>. </p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="bayesian-inference-as-a-mathematical-tool-to-combine-the-prior-expectation-and-the-new-observed-evidence">Bayesian inference as a mathematical tool to combine the prior expectation and the new observed evidence<a href="#bayesian-inference-as-a-mathematical-tool-to-combine-the-prior-expectation-and-the-new-observed-evidence" class="hash-link" aria-label="Direct link to Bayesian inference as a mathematical tool to combine the prior expectation and the new observed evidence" title="Direct link to Bayesian inference as a mathematical tool to combine the prior expectation and the new observed evidence">​</a></h2><p><img loading="lazy" alt="screenshot-app" src="/assets/images/bayesian_inference-65c14445df8749f10dd5238cf3771dcf.png" width="980" height="409" class="img_ev3q"></p><div align="center"> Fig. 2: Bayesian inference</div><br><p>With the insights provided by domain experts and extracted from the historical data, we can estimate an expectation of the sales of the current issue, prior to its sale period. However, even though the experts have a deep understanding of the product as well as the market, and the historical data may show some characteristics of the sales curves, there is always a grey zone of uncertainty and error. The market evolves; customers change their buying habits; some internal factors were poorly estimated; some external factors were not considered; some unexpected events happen during the sale period, etc. All those factors make the prediction which is purely based on historical experiences less reliable. </p><p>After the sale of the current issue has taken place, we can cumulate the recorded sales to get the beginning part of its sales curve. The more time passes, the more observations are built up. Those observations provide the latest information of the reality and indicate how the real sales actually evolve. However, the observations themselves contain noise. For example, a point of sale may forget to record a sale or record it a few days late, preventing us from using purely the observations to make a precise prediction.</p><p>Since we do not have enough subjective data, we can leverage Bayesian inference to fold in the prior knowledge that we have already had (thanks to the inputs of the experts and the historical sales) to draw stronger and sharper predictions. Mathematically, we model the sales by a random variable X that follows a distribution p(X|θ) parameterised by a set of parameters θ: X ~ p(X|θ). The prediction problem is reduced to finding the “correct” θ. Given the observations X<sub>obs</sub> (the sales records at the beginning of the sale period of the current issue in our case), frequentist approaches such as the Maximum Likelihood Estimation (MLE) method find an optimal θ that best fits Xobs: θ<sub>MLE</sub> = argmax<sub>θ</sub>(p(X<sub>obs</sub>|θ)), then plug it in to make the sales prediction: X<sup>pred</sup><sub>MLE</sub> = argmax<sub>X</sub>(p(X|θ<sub>MLE</sub>)). However, X<sub>obs</sub> usually contain noise,  results in a bad estimation of θ. With Bayesian inference, we can integrate our prior knowledge to get better predictions. Specifically, from the sales of historical issues and the inputs of the experts, we have an idea of how θ should be, modelled as the prior distribution p(θ). Incorporating it with the information provided by the observations Xobs, we get the posterior distribution p(θ|X<sub>obs</sub>), which is proportional to the prior p(θ) and the likelihood p(X<sub>obs</sub>|θ) (see Fig. 2): </p><div align="center"> p(θ|X<sub>obs</sub>) ∝ p(X<sub>obs</sub>|θ)*p(θ)</div><br><p>In Bayesian inference, rather than predicting a single value of the sales, we predict its distribution, called the posterior predictive distribution:</p><div align="center">p(X<sup>pred</sup><sub>Bayesian</sub>|X<sub>obs</sub>) = ∫p(X<sup>pred</sup><sub>Bayesian</sub>|θ)*p(θ|X<sub>obs</sub>)dθ</div><br><p>By doing so, we take into account the uncertainty in the prior, as well as the fact that the observations are noisy. </p><p> A pipeline—implemented in Spark and PyMC3—of the whole process, is depicted in Fig. 3. At the beginning, based on an analysis of the the characteristics of the issues and the market, the historical sales and the inputs of the experts, we estimate the prior p(θ). During the sale period, this estimation will be regularly revised, updated and adjusted in light of the observations X<sub>obs</sub> to get the posterior  p(θ|X<sub>obs</sub>), which is used to calculate the posterior predictive p(X<sup>θpred</sup><sub>Bayesian</sub>|X<sub>obs</sub>).   The posterior predictive is then formatted and sent to the decision-makers. </p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/pred_pip-5a7d5f28e2df886ed789452953c09f40.png" width="1956" height="790" class="img_ev3q"></p><div align="center"> Fig. 3: Bayesian sales prediction pipeline</div><br><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="results">Results<a href="#results" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results">​</a></h2><p>Fig. 4 shows an example of the prediction at day 30 of the sale period. We compare the performance of the Bayesian model with that of a frequentist baseline (in this example, is a logarithmic curve fitting model). Both the models are fed with the sales curves of historical issues and the sales records of the first 30 days of the current issue’s sale. Additionally, the Bayesian model has the characteristics of the issues (cover, number of holidays during the sale period, etc.) as well as the opinions of the experts on the similarity between the current issue and some issues in the past. The objective is to predict  the sales of the rest 30 days. </p><p>We can observe that the prediction of the frequentist model is too optimistic because of the (false) positive signals during the first half of the sale period. By contrast, the 95% HDI (highest density interval) of the Bayesian prediction well covers the true sales. This is thanks to the prior insights that the Bayesian model has been given.  </p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/sales_pred-a58913d44f4796544d98ff8a182bd8c1.png" width="1516" height="903" class="img_ev3q"></p><div align="center"> Fig. 4: Example of the sales predictions of a magazine at day 30</div><br><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><p>We have walked through an example of how to make better predictions of magazine sales. Data and data science have revolutionized many domains. However, to make the most out of digital assets, a hand-in-hand collaboration between data scientists and business experts is much needed. </p><p>Since 2018, Ekimetrics has been accompanying press publishers in their digital transformation journey. Our fully-industrialized AI-powered solution <a href="https://www.uniqueheritage.fr/fr/emmanuel-mounier-president-unique-heritage-media-et-soline-aubry-senior-manager-ekimetrics-au-salon-big-data-ai/" target="_blank" rel="noopener noreferrer">AthenIA</a> has been providing supply optimization, sales prediction and reporting services that are precise and customized for each client. With the experience of managing more than 90 magazines and 20k active points of sale in France, we are proud to be a reliable partner of press publishers toward sustainable and profitable press. </p></div>]]></content:encoded>
            <category>Bayesian Inference</category>
            <category>Digital Transformation</category>
            <category>Sales Prediction</category>
            <category>AthenIA</category>
        </item>
        <item>
            <title><![CDATA[Newsletter for April 2022]]></title>
            <link>https://ekimetrics.github.io/blog/2022/05/02/newsletter_April-2022</link>
            <guid>https://ekimetrics.github.io/blog/2022/05/02/newsletter_April-2022</guid>
            <pubDate>Mon, 02 May 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[We are now at the start of May and we release our 4th Newsletter! Ranging from podcasts to tutorials, this Newsletter is made for practicioners!]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/couv-1df2abde71ba51d343add1fd4b50949b.jpg" width="3968" height="2976" class="img_ev3q"></p></div><div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="data-science">Data Science<a href="#data-science" class="hash-link" aria-label="Direct link to Data Science" title="Direct link to Data Science">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-is-warner-music-using-ai-to-turn-sound-into-strategic-assets">How is Warner Music using AI to turn sound into strategic assets?<a href="#how-is-warner-music-using-ai-to-turn-sound-into-strategic-assets" class="hash-link" aria-label="Direct link to How is Warner Music using AI to turn sound into strategic assets?" title="Direct link to How is Warner Music using AI to turn sound into strategic assets?">​</a></h3><p><img loading="lazy" src="/assets/images/Warner-4eb3615c5e8f90d09193c5302ffc5734.jpg" width="3888" height="2592" class="img_ev3q"></p><p>Industries relying on creative contents are now moving toward a data centric strategy with dedicated teams and departments. We wanted to share with you the testimony of Kobi Abayomi, vice president of Data Science at Warner Music. The resource is available both as a podcast or as a transcript. The discussion covers broad topics ranging from what’s going on in the industry, what makes a good data science team or what are his views for the future of AI. </p><p><a href="https://sloanreview.mit.edu/audio/turning-sound-into-information-warner-music-groups-kobi-abayomi/" target="_blank" rel="noopener noreferrer">Turning Sound Into Information: Warner Music Group’s Kobi Abayomi (mit.edu)</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="machine-learning">Machine Learning<a href="#machine-learning" class="hash-link" aria-label="Direct link to Machine Learning" title="Direct link to Machine Learning">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="showcasing-and-sharing-your-ml-model-in-the-easiest-way-with-gradio">Showcasing and sharing your ML model in the easiest way with Gradio<a href="#showcasing-and-sharing-your-ml-model-in-the-easiest-way-with-gradio" class="hash-link" aria-label="Direct link to Showcasing and sharing your ML model in the easiest way with Gradio" title="Direct link to Showcasing and sharing your ML model in the easiest way with Gradio">​</a></h3><p><img loading="lazy" src="/assets/images/Gradio-45627b4e4944da21d1c84be082479387.jpg" width="3000" height="2000" class="img_ev3q"></p><p>Gradio is an open-source framework enabling ML Engineer to quickly share their models via a web interface. The cool feature of Gradio is how simple it is while leaving some rooms for changes and modifications.  Gradio can work with any kind of models and data structures (Images, Text, Tabular…). However, Gradio is made to share pre-trained models meaning that it cannot be used as an Active Learning asset where data would be provided to the model iteratively to its training. When having to share quick demos without the time or the need to have bespoke interface, Gradio is a no brainer. </p><p><a href="https://gradio.app/" target="_blank" rel="noopener noreferrer">Gradio</a></p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="learning-to-prompt-for-continual-learning">Learning to prompt for Continual Learning<a href="#learning-to-prompt-for-continual-learning" class="hash-link" aria-label="Direct link to Learning to prompt for Continual Learning" title="Direct link to Learning to prompt for Continual Learning">​</a></h3><p><img loading="lazy" src="/assets/images/Learning-919960e4adde6f8ee62bfa51b51d0d1b.jpg" width="5472" height="3648" class="img_ev3q"></p><p>Even though Pathways Language Models (PaLM) is one of the hottest releases of April 2022, our attention has been focused on another Google Research paper discussing the best way to perform Continual Learning. Continual Learning means to train a single model on various type of tasks iteratively. When being trained at step t, the model does not have access to previous data. One of the main challenge posed by such concept is how to maintain knowledge from past data into the model, avoiding catastrophic forgetting. In this paper, researchers propose an approach by leveraging prompt engineering. Using prompt Is very common in NLP as it tends to better fine-tune pretrained algorithms. The main idea of this paper is to tackle continual learning not as a model weights’ shift but rather as a memory space representing the type of task to be trained on. </p><p><a href="https://arxiv.org/abs/2112.08654" target="_blank" rel="noopener noreferrer">Learning to Prompt for Continual Learning</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="data-engineering--architecture">Data Engineering &amp; Architecture<a href="#data-engineering--architecture" class="hash-link" aria-label="Direct link to Data Engineering &amp; Architecture" title="Direct link to Data Engineering &amp; Architecture">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="putting-elasticsearch-into-production">Putting ElasticSearch into production<a href="#putting-elasticsearch-into-production" class="hash-link" aria-label="Direct link to Putting ElasticSearch into production" title="Direct link to Putting ElasticSearch into production">​</a></h3><p><img loading="lazy" src="/assets/images/ElasticSearch-d3517f406bd4a511dec55cdefffe4f21.jpg" width="3456" height="2304" class="img_ev3q"></p><p>ElasticSearch is a famous distributed search engine build on Apache Lucene. It has now been pretty much the standard for complex use cases where you have to look for data in a large volume and complex database (cf: <a href="https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-5c0c1c7d7eaf" target="_blank" rel="noopener noreferrer">How Netflix Content Engineering makes a federated graph searchable</a>). However, when it comes to put it into production, several challenges and pitfalls can occur. Hence, this blog post is a user sharing story with some best practices to adopt and bad habits to avoid. </p><p><a href="https://medium.com/@mzhaase/in-depth-guide-to-running-elasticsearch-in-production-b2ea7c8fa082" target="_blank" rel="noopener noreferrer">In depth guide to running Elasticsearch in production | by Mattis Haase | Medium</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="app-and-web-development">App and Web Development<a href="#app-and-web-development" class="hash-link" aria-label="Direct link to App and Web Development" title="Direct link to App and Web Development">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-to-build-a-javascript-bundler-from-scratch">How to build a JavaScript Bundler from scratch<a href="#how-to-build-a-javascript-bundler-from-scratch" class="hash-link" aria-label="Direct link to How to build a JavaScript Bundler from scratch" title="Direct link to How to build a JavaScript Bundler from scratch">​</a></h3><p><img loading="lazy" src="/assets/images/Dev-8090e61ca2fa6309bd5a32e608cb121a.jpg" width="5184" height="3456" class="img_ev3q"></p><p>A JavaScript Bundler is a tool combining code files in a unique file making it ready to use and deploy. It keeps tracks of every dependency that you might have into your repository. Those are stored into a graph guaranteeing that all your files are updated accordingly. Even if it starts to be common to use a bundler, nothing worth more than building one by ourselves. Following post shows how to build a bundler by yourself while keeping it relatively simple.  </p><p><a href="https://cpojer.net/posts/building-a-javascript-bundler" target="_blank" rel="noopener noreferrer">Building a JavaScript Bundler | Christoph Nakazawa (cpojer.net)</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="special-section-modeling">Special Section: Modeling<a href="#special-section-modeling" class="hash-link" aria-label="Direct link to Special Section: Modeling" title="Direct link to Special Section: Modeling">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="dealing-with-logs-and-zeros-in-regression-models">Dealing with logs and zeros in regression models<a href="#dealing-with-logs-and-zeros-in-regression-models" class="hash-link" aria-label="Direct link to Dealing with logs and zeros in regression models" title="Direct link to Dealing with logs and zeros in regression models">​</a></h3><p><img loading="lazy" src="/assets/images/Log-7dacba11f3b324db1b2c70bf9d73a49d.jpg" width="5184" height="3456" class="img_ev3q"></p><p>When having data generated by an exponential process, we tend to use log as a function to better fit liner model. However, problems arise when a portion of your data points are equal to zero. Common fix used is adding a constant (often 1) to your data to remove the problem. In this paper, researchers propose a novel family of estimators called iOLS (iterated Ordinary Least Squares). It presents a computational advantage while performing in the best way your fit.</p><p><a href="https://arxiv.org/abs/2203.11820" target="_blank" rel="noopener noreferrer">Dealing with Logs and Zeros in Regression Models (arxiv.org)</a></p></div>]]></content:encoded>
            <category>Machine Learning</category>
            <category>Javascript bundler</category>
            <category>Data Engineering</category>
        </item>
        <item>
            <title><![CDATA[Deploying your Data Science app to the Cloud]]></title>
            <link>https://ekimetrics.github.io/blog/2022/04/21/docker</link>
            <guid>https://ekimetrics.github.io/blog/2022/04/21/docker</guid>
            <pubDate>Thu, 21 Apr 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[Let's explore how we, at Ekimetrics, automate the deployment of our data apps on the cloud. This article will gather elements from our internal & external workshops presented at Datacraft and also the video presenting some docker best practices for industrialization. ]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/annie-spratt-unsplash-45fe35c2ca54f29fb0b4c11941253d90.jpg" width="1920" height="1083" class="img_ev3q"></p></div><div align="justify"><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="tldr">TL;DR<a href="#tldr" class="hash-link" aria-label="Direct link to TL;DR" title="Direct link to TL;DR">​</a></h4><p>Using optimized containers images in your favourite CI/CD pipeline will help you deploy your Data Science app quickly &amp; easily. Check out the replay of our <a href="https://www.youtube.com/watch?v=C7v5lY2G4Os" target="_blank" rel="noopener noreferrer">workshop</a> at Datacraft and our Mastercraft video about<a href="https://www.youtube.com/watch?v=lkL3Ve7sDfc&amp;t=97s&amp;ab_channel=datacraft" target="_blank" rel="noopener noreferrer"> container best practices</a>.  </p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="data-science-storytelling-with-streamlit">Data Science storytelling with Streamlit<a href="#data-science-storytelling-with-streamlit" class="hash-link" aria-label="Direct link to Data Science storytelling with Streamlit" title="Direct link to Data Science storytelling with Streamlit">​</a></h2><p>Over the past year, <a href="https://streamlit.io/" target="_blank" rel="noopener noreferrer">Streamlit</a> has become one of our favourite tools to share data insights through a web app. It's a low-code, data science oriented Python framework that makes your scripts shine in a web app. You can use it to quickly build beautiful visualizations but as data is even better when shared, we would like to help you make it easily accessible to your teammates and colleagues. We will deploy one of the apps in the Streamlit Gallery : <a href="https://github.com/streamlit/demo-uber-nyc-pickups/" target="_blank" rel="noopener noreferrer">NYC Uber Ridesharing Data</a>.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="running-the-app">Running the app<a href="#running-the-app" class="hash-link" aria-label="Direct link to Running the app" title="Direct link to Running the app">​</a></h3><p>Let's keep it simple and follow these steps to get the code and use venv to create a virtual environment with the following steps:</p><div class="language-zsh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-zsh codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">&gt; git clone git@github.com:streamlit/demo-uber-nyc-pickups.git</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&gt; cd demo-uber-nyc-pickups</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&gt; py -3.8 -m venv .venv</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&gt; .venv/scripts/activate</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&gt; pip install -r requirements.txt</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="theme-admonition theme-admonition-info alert alert--info admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_S0QG"><p>We are showcasing <a href="https://docs.python.org/3/library/venv.html" target="_blank" rel="noopener noreferrer">venv</a> here as it is included in Python itself. We invite you to explore other tools such as <a href="https://python-poetry.org/" target="_blank" rel="noopener noreferrer">Poetry</a>, <a href="https://pipenv.pypa.io/en/latest/" target="_blank" rel="noopener noreferrer">pipenv</a> or <a href="https://docs.conda.io/en/latest/" target="_blank" rel="noopener noreferrer">conda</a> depending on your needs and preferences and to always make use of them, especially when collaborating on multiple projects as it will save you from conflicts in your installations.</p></div></div><p>Now, you should be able to run it locally:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">streamlit run index.py </span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><img loading="lazy" alt="screenshot-app" src="/assets/images/screenshot-app-144427ee5ac05be241dfd0d8c9140c69.png" width="1859" height="807" class="img_ev3q"></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="docker-the-cloudy-whale">Docker, the cloudy whale<a href="#docker-the-cloudy-whale" class="hash-link" aria-label="Direct link to Docker, the cloudy whale" title="Direct link to Docker, the cloudy whale">​</a></h2><p>Docker is one of the most popular containerization tools and is widely used in the context of cloud-based solutions. In this article, we will review how to set up your environment so that you can leverage this tool and level up your development workflow! </p><p>The advantage we will explore in this article is the ability to package your environment with all its dependencies and be 99.99% sure that it will be able to run on your target environment. </p><blockquote><p>"If it runs on Docker, it will run everywhere." - one hopeful DataOps</p></blockquote><p>You can safely share your app and be sure that it will run in the same conditions as you worked on and ensure reproducibility of your results. </p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="creating-your-container">Creating your container<a href="#creating-your-container" class="hash-link" aria-label="Direct link to Creating your container" title="Direct link to Creating your container">​</a></h3><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="write-dockerfile">Write Dockerfile<a href="#write-dockerfile" class="hash-link" aria-label="Direct link to Write Dockerfile" title="Direct link to Write Dockerfile">​</a></h4><p>The Dockerfile is the recipe of your container. It contains all the instructions to produce &amp; reproduce it identically everywhere it will be run: on another laptop, maybe with a different OS, in the cloud.</p><p>Some quick definitions:</p><ul><li>The Docker image is the result of the build of the Dockerfile.</li><li>The Docker container is the result of running the image.</li><li>You can pull and push images to a container registry.</li></ul><div align="center"><p><img loading="lazy" alt="Docker definitions schema" src="/assets/images/docker-definitions-schema-742ad3f8ebb9c6e95b4f6b99bcc6601c.gif" width="724" height="317" class="img_ev3q"></p></div><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="choosing-the-base-image">Choosing the base image<a href="#choosing-the-base-image" class="hash-link" aria-label="Direct link to Choosing the base image" title="Direct link to Choosing the base image">​</a></h4><p>It always starts with a <strong>FROM</strong> instruction that defines what is the base image which can be either:</p><ul><li>a linux-based OS: <a href="https://hub.docker.com/layers/ubuntu/library/ubuntu/latest/images/sha256-31cd7bbfd36421dfd338bceb36d803b3663c1bfa87dfe6af7ba764b5bf34de05?context=explore" target="_blank" rel="noopener noreferrer">ubuntu</a>, <a href="https://hub.docker.com/layers/debian/library/debian/stable/images/sha256-7ec7bef742f919f7cc88f41b598ceeb6b74bcb446e9ce1d2d7c31eb26ccba624?context=explore" target="_blank" rel="noopener noreferrer">debian</a>, <a href="https://hub.docker.com/layers/centos/library/centos/latest/images/sha256-a1801b843b1bfaf77c501e7a6d3f709401a1e0c83863037fa3aab063a7fdb9dc?context=explore" target="_blank" rel="noopener noreferrer">centos</a>...</li><li>an OS with preinstalled tools: <a href="https://hub.docker.com/layers/python/library/python/3.8/images/sha256-71d10e809efb1733e2743fb7be3417db18070e10a2276e727216f245f7418592?context=explore" target="_blank" rel="noopener noreferrer">python-3.8</a>, <a href="https://hub.docker.com/layers/mongo/library/mongo/5.0.7/images/sha256-5b5263a7d25d06d5149904eaaacdb359edcd4f26a3d971265f85362dd2406655?context=explore" target="_blank" rel="noopener noreferrer">mongo-5.0.7</a>, <a href="https://hub.docker.com/layers/node/library/node/17.9.0/images/sha256-c1336669570df673e44d5a2152eb4eff99f4c23bb3d19361c69f861b1bd5ffd3?context=explore" target="_blank" rel="noopener noreferrer">node-17.9.0</a></li><li>a ready-to-use image of a project like the <a href="https://hub.docker.com/r/docker/getting-started" target="_blank" rel="noopener noreferrer">Docker Tutorial</a>.</li></ul><p>Choosing your base image is important as it can help you reduce the amount of steps to get ready. If you chose Ubuntu as your base image for a Python project, you will need to write the instructions to install the Python version you want to install and your image will also come with many other packages that are preinstalled in the OS that you may not need for your projet. This is why Python images are available. It comes with Python already installed and some images (like slim images) also remove packages that aren't necessary for Python development. This strongly impacts the size of your image as shown here:</p><div align="center"><p><img loading="lazy" alt="Python Docker images size comparison" src="/assets/images/python-docker-images-size-comparison-8648b2ea9f9ef5fbaace2501f6fee442.png" width="698" height="200" class="img_ev3q"></p></div><p>These are official Docker images and depending on which version you pull, you can see the size difference is quite noticeable.</p><ul><li>python&nbsp;3.8 is almost 1GB.</li><li>python 3.8-slim reduces it by 86%.</li><li>python 3.8-alpine even goes down to &lt;50MB.</li></ul><p>Alpine is not recommended for Python development as standard PyPi wheels don't work on Alpine and you have to compile them for every package, which can lead to additional research to properly achieve it. In some instances, compiling can also be very lengthy, like matplotlib that takes more than 25min to build:</p><p><img loading="lazy" alt="matplotlib-build-alpine" src="/assets/images/python-matplotlib-79daf5b8811d2475b4ebc732d659ec16.png" width="1073" height="395" class="img_ev3q"></p><p>Nobody ain't got time for that. </p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="efficient-build-instructions">Efficient Build Instructions<a href="#efficient-build-instructions" class="hash-link" aria-label="Direct link to Efficient Build Instructions" title="Direct link to Efficient Build Instructions">​</a></h3><p>So let's build our image using a small, ready-to-python image like <code>python:3.8-slim</code>. We first need to install all our app requirements. So we add our <code>requirements.txt</code> file and install just like we would do locally :</p><div class="language-Dockerfile codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-Dockerfile codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">FROM python:3.8-slim</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># We place ourself in a dedicated folder</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">WORKDIR /app</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Installing requirements</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">ADD ./requirements.txt /app/requirements.txt</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">RUN pip3 install -r requirements.txt --no-cache-dir</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Adding the file alone and installing the requirements right after allows you to create a layer that will be cached and you will not have to reuse this step in a future build. It is generally a good practice to use the cache when in development mode as you don't want to spend 10 minutes at every build of your requirements (you know it can take a while). The way Docker cache works is that by default it will reuse the highest unmodified layers.</p><p><img loading="lazy" alt="docker-cache-example" src="/assets/images/docker-cache-example-18ec05ab2a069080c94be26d3cb1aab1.gif" width="1192" height="669" class="img_ev3q"></p><p>Here are the commands that generate a layer and which you want to be able to cache and/or gather in a single instruction.</p><ul><li><strong>FROM</strong> creates a layer from the base image.</li><li><strong>COPY</strong> adds files from your Docker client’s current directory.</li><li><strong>RUN</strong> builds your application with make.</li><li><strong>CMD</strong> specifies what command to run within the container.</li></ul><p><a href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/" target="_blank" rel="noopener noreferrer">Source</a></p><p>Then, we add the different required files with the <em>ADD</em> or <em>COPY</em> instructions. In our example, we need the python main code, the data and one image.</p><div class="language-Dockerfile codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-Dockerfile codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">ADD streamlit_app.py  uber-raw-data-sep14.csv.gz  uber_demo.png /app/</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>To access the app, we need to open port 8051 on the container. Otherwise, even if the app is running, it will not be accessible from outside the container by a browser. Instruction is :</p><div class="language-Dockerfile codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-Dockerfile codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">EXPOSE 8051</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The last instruction is the <em>RUN</em>, which tells Docker which command needs to be run at the start of the container. For us, it means starting the Streamlit app with the command <code>streamlit run /index.py</code>, which in Docker syntax is:</p><div class="language-Dockerfile codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-Dockerfile codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">CMD ["streamlit", "run" ,"/index.py"]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The final Dockerfile should look like this:</p><div class="language-Dockerfile codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockTitle_Ktv7">Dockerfile</div><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-Dockerfile codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">FROM python:3.8-slim</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">WORKDIR /app</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">EXPOSE 8501</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">ADD requirements.txt /app/requirements.txt</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">RUN pip install -r requirements.txt</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">ADD streamlit_app.py  uber-raw-data-sep14.csv.gz  uber_demo.png /app/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">CMD ["streamlit","run","streamlit_app.py"]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="run-your-streamlit-container">Run your Streamlit container<a href="#run-your-streamlit-container" class="hash-link" aria-label="Direct link to Run your Streamlit container" title="Direct link to Run your Streamlit container">​</a></h4><p>To build your image, just run the command:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token function" style="color:rgb(80, 250, 123)">docker</span><span class="token plain"> build --tag ekilab-demo-container </span><span class="token builtin class-name" style="color:rgb(189, 147, 249)">.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Once built, you can run the container with the command:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token function" style="color:rgb(80, 250, 123)">docker</span><span class="token plain"> run -dp </span><span class="token number">8501</span><span class="token plain">:8501 ekilab-demo-container</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Your app should now be running and available at <a href="http://localhost:8501" target="_blank" rel="noopener noreferrer">http://localhost:8501</a>.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="live-reload">Live Reload<a href="#live-reload" class="hash-link" aria-label="Direct link to Live Reload" title="Direct link to Live Reload">​</a></h4><p>The issue with the current setup is that if you make a change to your app, you will need to rebuild and re-run the container to see the changes in your browser. We don't want to rebuild the whole environment, and to do that we will use the <a href="https://docs.docker.com/storage/volumes/" target="_blank" rel="noopener noreferrer">volume</a> feature of Docker, and make it easy with <a href="https://docs.docker.com/compose/" target="_blank" rel="noopener noreferrer">docker-compose</a>.
Docker-compose is a powerful tool to orchestrate multiple containers. But in our case, we will only use it to make our life easier. </p><div class="language-yml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockTitle_Ktv7">docker-compose.yml</div><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yml codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token key atrule">version</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"3.9"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token key atrule">services</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">ekilab-demo-container</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">build</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> .</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">ports</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"8501:8501"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">volumes</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> ./</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">/app </span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Now, you only need to remember one single command: <code>docker compose up</code>. It will build your image, run it with the correct parameters and link it to the correct volume.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="ship-it-to-the-cloud-">Ship it to the Cloud !<a href="#ship-it-to-the-cloud-" class="hash-link" aria-label="Direct link to Ship it to the Cloud !" title="Direct link to Ship it to the Cloud !">​</a></h2><p>If you don't have an Azure account, you can create one for <a href="https://azure.microsoft.com/en-us/free/" target="_blank" rel="noopener noreferrer">free</a> and get 200$ of credits for trying out the platform. </p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="log-into-azure">Log into Azure<a href="#log-into-azure" class="hash-link" aria-label="Direct link to Log into Azure" title="Direct link to Log into Azure">​</a></h3><p>Log into your account using the Azure CLI with the <code>az login</code> command.
Then, we will need a container registry. At Ekimetrics, we usually build our infrastructure using an Infrastructure-As-Code tool such as <a href="https://terraform.io" target="_blank" rel="noopener noreferrer">Terraform</a>. But for the sake of simplicity, let's use simple CLI&nbsp;commands to create our resources.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="container-registry">Container registry<a href="#container-registry" class="hash-link" aria-label="Direct link to Container registry" title="Direct link to Container registry">​</a></h3><p>The container registry will store the different versions of our container. It can also be used to store other images. You can consider it as your private DockerHub.</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">az acr create --name ekilabhub --resource-group ekimetrics-demo --sku basic --admin-enabled </span><span class="token boolean">true</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="linux-app-service-plan">Linux App Service Plan<a href="#linux-app-service-plan" class="hash-link" aria-label="Direct link to Linux App Service Plan" title="Direct link to Linux App Service Plan">​</a></h3><p>To run our app, we need a Linux-based resource. This is what the App Service Plan offers.
B1 is a free instance that you can use for your small apps and Proof Of Concept.</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">az appservice plan create --resource-group ekimetrics-demo -n ekimetrics-lasp -l westeurope --is-linux --sku B1</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Now, we have several options :</p><ul><li>Option 1 - build the image locally and push it to the container registry</li><li>Option 2 - build the image directly from the registry</li><li>Option 3 - build and push the image from a CI/CD pipeline</li></ul><p>Let's try :</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># log into the container registry</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">docker</span><span class="token plain"> login ekilabhub.azurecr.io --username ekilabhub --password its4secr€t</span><span class="token operator">!</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Option 1 - push the new Docker image to the registry</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">docker</span><span class="token plain"> push ekilabhub.azurecr.io/ekilab/ekilab-demo-container:latest</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Option 2 - directly build on Azure Container Registry</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">az acr build --registry ekilabhub --resource-group ekimetrics-demo-rg --image ekilab-demo-container </span><span class="token builtin class-name" style="color:rgb(189, 147, 249)">.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-web-app">The Web App<a href="#the-web-app" class="hash-link" aria-label="Direct link to The Web App" title="Direct link to The Web App">​</a></h3><p>Finally we can create the web app, based on our previously created App Service Plan, and by retrieving the image from the Container Registry.</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">az webapp create --resource-group ekimetrics-demo-rg -p ekimetrics-lasp -n ekilab-demo-app -i ekilabhub.azurecr.io/ekilab/ekilab-demo-container:latest</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="lets-automate">Let's Automate!<a href="#lets-automate" class="hash-link" aria-label="Direct link to Let's Automate!" title="Direct link to Let's Automate!">​</a></h2><p>It's all been fun but we don't want to do this every time we update the code. It would be easy to make a mistake and you may want to collaborate with other Data Scientists on your team and have their updates deployed without your help!</p><div align="center"><p><img loading="lazy" alt="automate-all-the-things" src="/assets/images/automate-b4b2aaa382f4c26530ad030ace0cfb08.png" width="660" height="400" class="img_ev3q"></p></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="bitbucket-pipelines">Bitbucket Pipelines<a href="#bitbucket-pipelines" class="hash-link" aria-label="Direct link to Bitbucket Pipelines" title="Direct link to Bitbucket Pipelines">​</a></h3><p>In this example we are using <a href="https://bitbucket.org/product/fr/features/pipelines" target="_blank" rel="noopener noreferrer">Bitbucket Pipelines</a> to automate these deployment steps but the logic is very similar in other tools such as <a href="https://dev.azure.com/" target="_blank" rel="noopener noreferrer">Azure DevOps</a>, <a href="https://cloud.google.com/build" target="_blank" rel="noopener noreferrer">Cloud Build</a>, <a href="https://docs.gitlab.com/ee/ci/introduction/" target="_blank" rel="noopener noreferrer">Gitlab CI/CD</a>, <a href="https://github.com/features/actions" target="_blank" rel="noopener noreferrer">GitHub Actions</a>, <a href="https://www.jenkins.io/" target="_blank" rel="noopener noreferrer">Jenkins</a>...</p><p>The common concept behind these tools is to store all the instructions in a YAML file and execute them at every commit to a given branch. In our example, the master branch gathers the code that will be delivered to production.</p><div class="language-yml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockTitle_Ktv7">bitbucket-pipelines.yml</div><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yml codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token key atrule">image</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> python</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token number">3.8</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token key atrule">pipelines</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">branches</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">master</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> </span><span class="token key atrule">step</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token key atrule">name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> Build and Push the image         </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token key atrule">services</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> docker</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token key atrule">script</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token comment" style="color:rgb(98, 114, 164)"># build the image</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> docker build </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">t ekilabhub.azurecr.io/ekilab/ekilab</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">demo</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">container</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">$BITBUCKET_BUILD_NUMBER .</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token comment" style="color:rgb(98, 114, 164)"># authenticate with the Azure Container Registry </span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> docker login ekilabhub.azurecr.io </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">username ekilabhub </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">password  $AZURE_CONTAINER_REGISTRY_PASSWORD</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token comment" style="color:rgb(98, 114, 164)"># push the new Docker image to the registry</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> docker push ekilabhub.azurecr.io/ekilab/ekilab</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">demo</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">container</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">$BITBUCKET_BUILD_NUMBER</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> </span><span class="token key atrule">step</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token key atrule">name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> Deploying App to Azure</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token key atrule">script</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token comment" style="color:rgb(98, 114, 164)"># Install the Azure CLI</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> curl </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">sL https</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">//aka.ms/InstallAzureCLIDeb </span><span class="token punctuation" style="color:rgb(248, 248, 242)">|</span><span class="token plain"> bash </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token comment" style="color:rgb(98, 114, 164)"># login with a Service Principal / store the credentials in a secured area</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> az login </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">service</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">principal </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">username $AZURE_APP_ID </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">password $AZURE_PASSWORD </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">tenant $AZURE_TENANT_ID </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token comment" style="color:rgb(98, 114, 164)"># Set the correct image version to the web app </span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> az webapp config container set  </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">resource</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">group $AZURE_RESOURCE_GROUP  </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">name ekilab</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">demo</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">app  </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">docker</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">custom</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">image</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">name ekilab</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">demo</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">container</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">$BITBUCKET_BUILD_NUMBER  </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">docker</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">registry</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">server</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">url ekilabhub.azurecr.io </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">docker</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">registry</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">server</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">user ekilabhub </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">docker</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">registry</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">server</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">password $AZURE_CONTAINER_REGISTRY_PASSWORD</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="whats-next-">What's next ?<a href="#whats-next-" class="hash-link" aria-label="Direct link to What's next ?" title="Direct link to What's next ?">​</a></h2><p><strong>Congratulations !</strong> You should now be able to automate the deployment of your app to the cloud and focus only on updating its content without worrying about how to publish your updates. Now, you might need to have a scalable app to ensure it can handle high loads of visits. You could require tools like <a href="https://kubernetes.io" target="_blank" rel="noopener noreferrer">Kubernetes</a>, <a href="https://docs.docker.com/engine/swarm/" target="_blank" rel="noopener noreferrer">Docker Swarm</a>.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="alternatives">Alternatives<a href="#alternatives" class="hash-link" aria-label="Direct link to Alternatives" title="Direct link to Alternatives">​</a></h3><p>We presented one of the workflow we are using at Ekimetrics, but it's not the only one ! We also work with GCP, AWS, Alibaba, sometimes on-premise infrastructure that will prevent us from using Docker or Azure for example.</p><h5 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="web-app-framework">Web App framework<a href="#web-app-framework" class="hash-link" aria-label="Direct link to Web App framework" title="Direct link to Web App framework">​</a></h5><p>Soe alternavites to streamlit that are also offering low-code, minimalist, <em>straight-to-the-data</em> python framework that you can package in a container.</p><ul><li><a href="https://plotly.com/dash/" target="_blank" rel="noopener noreferrer">Dash Plotly</a></li><li><a href="https://gradio.app/" target="_blank" rel="noopener noreferrer">Gradio.app</a></li></ul><h5 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="containers">Containers<a href="#containers" class="hash-link" aria-label="Direct link to Containers" title="Direct link to Containers">​</a></h5><p>Docker is not the only containerization tool. Here are some alternatives we invite you to check out if Docker doesn't suit you.</p><ul><li><a href="https://podman.io/" target="_blank" rel="noopener noreferrer">podman</a></li><li><a href="https://containerd.io/" target="_blank" rel="noopener noreferrer">containerd</a></li><li><a href="https://coreos.com/rkt/docs/latest/" target="_blank" rel="noopener noreferrer">CoreOS rkt</a></li><li><a href="http://mesos.apache.org/documentation/latest/mesos-containerizer/" target="_blank" rel="noopener noreferrer">Mesos Containerizer</a></li><li><a href="https://linuxcontainers.org/" target="_blank" rel="noopener noreferrer">LXC Linux Containers</a></li><li><a href="https://openvz.org/" target="_blank" rel="noopener noreferrer">OpenVZ</a></li></ul><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="web-app-hosting">Web App Hosting<a href="#web-app-hosting" class="hash-link" aria-label="Direct link to Web App Hosting" title="Direct link to Web App Hosting">​</a></h4><p>There are many others ways to deploy your apps, even free tiers on some providers that can help you get your hands-on with smaller, non-sensitive apps.</p><ul><li><a href="https://streamlit.io/cloud" target="_blank" rel="noopener noreferrer">Streamlit Cloud</a> offers a free tier to host one private app and <strong>unlimited</strong> public apps! Deployment is <a href="https://s3-us-west-2.amazonaws.com/assets.streamlit.io/videos/streamlit_sharing_silent.mp4" target="_blank" rel="noopener noreferrer">very simple with GitHub</a></li><li><a href="https://azure.microsoft.com/en-us/services/container-apps/" target="_blank" rel="noopener noreferrer">Azure Container App</a> a new offer from Microsoft that helps you simply deploy scalable apps.</li><li><a href="https://cloud.google.com/run" target="_blank" rel="noopener noreferrer">GCP Cloud Run</a>, similar to Azure Apps, is a serverless service that helps easily deploy pre-built containerized apps.</li><li><a href="https://www.heroku.com/" target="_blank" rel="noopener noreferrer">Heroku</a> offers a free tier to deploy small apps, Proof Of Concept and also a very simple deployment process.</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="credits">Credits<a href="#credits" class="hash-link" aria-label="Direct link to Credits" title="Direct link to Credits">​</a></h2><ul><li>Cover Photo by <a href="https://unsplash.com/@anniespratt?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank" rel="noopener noreferrer">Annie Spratt</a> on <a href="https://unsplash.com/s/photos/rainbow-cake?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank" rel="noopener noreferrer">Unsplash</a></li><li><a href="https://cloud.google.com/architecture/best-practices-for-building-containers#package_a_single_application_per_container" target="_blank" rel="noopener noreferrer">Google's Best Practices for building Containers</a></li><li><a href="https://pythonspeed.com/articles/base-image-python-docker-images/" target="_blank" rel="noopener noreferrer">The best Docker base image for your Python application (August 2021)</a></li></ul></div>]]></content:encoded>
            <category>DevOps</category>
            <category>DataOps</category>
            <category>Containers</category>
            <category>CI/CD</category>
            <category>Azure</category>
            <category>Docker</category>
        </item>
        <item>
            <title><![CDATA[Newsletter for March 2022]]></title>
            <link>https://ekimetrics.github.io/blog/2022/03/30/newsletter_March-2022</link>
            <guid>https://ekimetrics.github.io/blog/2022/03/30/newsletter_March-2022</guid>
            <pubDate>Wed, 30 Mar 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[Ekimetrics is pleased to release its third Newsletter for the year 2022! A new feature has now been added which is a special section detailing some news that we would like to share but are not specifically linked to one of our main categories. As usual, do not hesitate to check the links added and of course using it live during your projects!]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/forest_1-4e0d36a926dba88b2afd8858ab5abaae.jpg" width="3264" height="2448" class="img_ev3q"></p></div><div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="data-science">Data Science<a href="#data-science" class="hash-link" aria-label="Direct link to Data Science" title="Direct link to Data Science">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="ai-for-executives-decisions-do-we-reach-a-limit">AI for executives’ decisions: do we reach a limit?<a href="#ai-for-executives-decisions-do-we-reach-a-limit" class="hash-link" aria-label="Direct link to AI for executives’ decisions: do we reach a limit?" title="Direct link to AI for executives’ decisions: do we reach a limit?">​</a></h3><p><img loading="lazy" src="/assets/images/Image1-d998ca9a82f1606d345003972510f0a7.jpg" width="1137" height="639" class="img_ev3q"></p><p>The Harvard Business Review published an article where it discusses the use of AI for strategic high-level decisions. While we witness a spread of AI in many operational business areas, it is still not considered as a usable tool for C-executives. The article pictures the current environment to eventually give some key elements to deploy such technologies at this level of management. Deploying ML or even simple data science solutions at this level of responsibility requires us to do a deep check of the possible consequences as a tiny mistake can turn into a major negative outcome. However, while presenting a difficult market to reach for data science, we can also interpret it as an opportunity for data science companies.</p><p><a href="https://hbr.org/2022/03/overcoming-the-c-suites-distrust-of-ai" target="_blank" rel="noopener noreferrer">Overcoming the C-Suite’s Distrust of AI (hbr.org)</a></p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="guide-through-time-series-modeling">Guide through time series modeling<a href="#guide-through-time-series-modeling" class="hash-link" aria-label="Direct link to Guide through time series modeling" title="Direct link to Guide through time series modeling">​</a></h3><div align="center"><p><img loading="lazy" src="/assets/images/Image2-b05708263353c2e253177b1f96b8301d.png" width="497" height="189" class="img_ev3q"></p></div><p>Modeling time series variables is a core competence for any Data Scientist. This exhaustive though pedagogic guide helps you better understand some basic principles behind time-series models making it a must-read for juniors willing to strengthen their knowledge or experts who would like to update their knowledge of this field. The article is made by Neptune, a ML library, so it showcases codes with it. It sill gives a good overview and you can reimplement it in your notebook easily with other libraries if you feel uncomfortable with Neptune.</p><p><a href="https://neptune.ai/blog/select-model-for-time-series-prediction-task?utm_source=mlndev&amp;utm_medium=post&amp;utm_campaign=blog-select-model-for-time-series-prediction-task&amp;ref=mlnews" target="_blank" rel="noopener noreferrer">How to Select a Model For Your Time Series Prediction Task [Guide] - neptune.ai</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="machine-learning">Machine Learning<a href="#machine-learning" class="hash-link" aria-label="Direct link to Machine Learning" title="Direct link to Machine Learning">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="shifting-to-real-time-machine-learning">Shifting to real-time machine learning<a href="#shifting-to-real-time-machine-learning" class="hash-link" aria-label="Direct link to Shifting to real-time machine learning" title="Direct link to Shifting to real-time machine learning">​</a></h3><p><img loading="lazy" src="/assets/images/Image3-1d08b237346eed3f780553401b2b699d.png" width="1123" height="657" class="img_ev3q"></p><p>While ML algorithms provide powerful insights, they need to become “alive”. This need for real-time is explained by two reasons: a need to deliver results quickly and to learn continuously from new data. In theory it looks easy, but deploying the right architecture is far more complicated and many companies fail at tackling this challenge. This post details the global framework to adopt, key challenges to keep in mind and the requirements to succeed at it. </p><p><a href="https://huyenchip.com/2022/01/02/real-time-machine-learning-challenges-and-solutions.html?ref=mlnews#towards-online-prediction" target="_blank" rel="noopener noreferrer">Real-time machine learning: challenges and solutions (huyenchip.com)</a></p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="perceiver-io-the-next-gen-of-large-and-multi--task-models-has-arrived">Perceiver IO, the next gen of large and multi -task models has arrived<a href="#perceiver-io-the-next-gen-of-large-and-multi--task-models-has-arrived" class="hash-link" aria-label="Direct link to Perceiver IO, the next gen of large and multi -task models has arrived" title="Direct link to Perceiver IO, the next gen of large and multi -task models has arrived">​</a></h3><p><img loading="lazy" src="/assets/images/Image4-f524133f4655e8f5ef7182477bbf0c75.png" width="1086" height="471" class="img_ev3q"></p><p>Transformers were introduced in 2017 and created many derivatives (Longformers, Reformers…) while taking the lead in many applications’ fields (NLP, CV…). The Perceiver is like an Optimus Prime for Transformers (or a Megatron based on which side you are). While being technical, the key takeaway of this paper is the level of generality that we get for every domain: Perceivers perform well on texts, sounds, images on a wide variety of tasks with the same underlying mechanism. In addition, it is able to scale well on high dimensional data as it bypasses the quadratic comuting time required by a Transformer. Even though the widespread of Transformers makes it dominant in the field of ML, Perceivers should be added to your competences portfolio as they might be the future for many tasks.</p><p><a href="https://arxiv.org/pdf/2107.14795.pdf" target="_blank" rel="noopener noreferrer">Perceiver IO: A general architecture for structure inputs &amp; outputs</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="data-engineering--architecture">Data Engineering &amp; Architecture<a href="#data-engineering--architecture" class="hash-link" aria-label="Direct link to Data Engineering &amp; Architecture" title="Direct link to Data Engineering &amp; Architecture">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="ml-monitoring-the-promotheus-fails-why-whats-up-ridley">ML Monitoring: The Promotheus fails, why? (what’s up Ridley?)<a href="#ml-monitoring-the-promotheus-fails-why-whats-up-ridley" class="hash-link" aria-label="Direct link to ML Monitoring: The Promotheus fails, why? (what’s up Ridley?)" title="Direct link to ML Monitoring: The Promotheus fails, why? (what’s up Ridley?)">​</a></h3><div align="center"><p><img loading="lazy" src="/assets/images/Image5-cf2491b3cd5dd27bdb60deebcf3cb9a8.png" width="376" height="179" class="img_ev3q"></p></div><p>This article presents the task of ML monitoring while highlighting key liabilities from Prometheus, an open-source software monitoring tool. The author implements a simple example and details where Prometheus fails particularly (query latency, sliding windows…). This post is one among four dealing with ML monitoring and it is worth the reading if you want to deepen the problem!</p><p><a href="https://www.shreya-shankar.com/rethinking-ml-monitoring-3/?ref=mlnews" target="_blank" rel="noopener noreferrer">The Modern ML Monitoring Mess: Failure Modes in Extending Prometheus (3/4) | Shreya Shankar (shreya-shankar.com)</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="app-and-web-development">App and Web Development<a href="#app-and-web-development" class="hash-link" aria-label="Direct link to App and Web Development" title="Direct link to App and Web Development">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="javascript-2021-recap">JavaScript 2021 recap!<a href="#javascript-2021-recap" class="hash-link" aria-label="Direct link to JavaScript 2021 recap!" title="Direct link to JavaScript 2021 recap!">​</a></h3><p><img loading="lazy" src="data:image/png;base64,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" width="208" height="208" class="img_ev3q"></p><p>The 2021 report on the state of JavaScript is now published. From the latest trends, the rising popularity of libraries to most-have tools, this survey gives you a good overview on what is happening in the JavaScript users’ community. One of the many takeaway is the change of perception about the language in itself. “The percentage of people reporting that "JS is moving too fast" is down from 59% to 38%”, showing that JS is now reaching a mature stage. </p><p><a href="https://2021.stateofjs.com/en-US/features/" target="_blank" rel="noopener noreferrer">The State of JS 2021: Features</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="special-section-quantitative-marketing">Special Section: Quantitative Marketing<a href="#special-section-quantitative-marketing" class="hash-link" aria-label="Direct link to Special Section: Quantitative Marketing" title="Direct link to Special Section: Quantitative Marketing">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="tv-elasticity-a-huge-us-surveys-highlights-overinvesting-tendencies">TV elasticity: a huge US surveys highlights overinvesting tendencies<a href="#tv-elasticity-a-huge-us-surveys-highlights-overinvesting-tendencies" class="hash-link" aria-label="Direct link to TV elasticity: a huge US surveys highlights overinvesting tendencies" title="Direct link to TV elasticity: a huge US surveys highlights overinvesting tendencies">​</a></h3><div align="center"><p><img loading="lazy" src="/assets/images/Image7-7e81e2a2b14e3a3b667ce97eee9d83a8.jpg" width="357" height="223" class="img_ev3q"></p></div><p>From now on, we will have a special section where some hot topics that we like, but cannot be categorized in previous sections, will be put. To start it, we will discuss one of the expertise areas of Ekimetrics: Quantitative Marketing. This article from July 2021 estimates the ROI of TV advertising for 288 consumer goods across a large section of products and sectors in the US. The key takeaway is that TV is an over-invested media with saturated returns. The results from this paper do not align with the existent literature promoting a higher return on investment for TV advertising. The writers precise that traditional media market suffers from agencies issues as many professionals prefer maintaining their statement on the TV effectiveness rather than experimenting new marketing combinations. </p><p><a href="https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA17674?casa_token=qFjMlFmqYRgAAAAA:DdA-UGKAxUAm6t8bXRhFcEYO8_M7G7QFEDKh5KQvhx-OBAYJXg90xqgoLiPYaj_TitLTfw9FkjaPjg" target="_blank" rel="noopener noreferrer">TV Advertising Effectiveness and Profitability: Generalizable Results From 288 Brands - Shapiro - 2021 - Econometrica - Wiley Online Library</a></p></div>]]></content:encoded>
            <category>ML Monitoring</category>
            <category>Javascript</category>
            <category>Time Series Modelling</category>
        </item>
        <item>
            <title><![CDATA[Power BI - Improve your development process by using multiple environments]]></title>
            <link>https://ekimetrics.github.io/blog/2022/03/16/powerbi_multiple_environments</link>
            <guid>https://ekimetrics.github.io/blog/2022/03/16/powerbi_multiple_environments</guid>
            <pubDate>Wed, 16 Mar 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[ Learn in this article why and how to use different environments when developing products in Power BI. ]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/windmills-92cf861462ce53a8a2c3bb77743bdb0f.jpg" width="5430" height="3620" class="img_ev3q"></p></div><p>When building a piece of software, you don’t want your users to see every messy part of your application creation process. In order to make sure you control what people see and when they have access to it, development teams use environments to create “stages” of the app which they consider good for releasing.</p><p>Each environment has its own unique purpose. There are different naming standards across the industry, although almost every process starts with a 'development' stage and ends with a 'production' stage. Here is a typical set of environments:</p><ul><li><p><strong>Development environment</strong> : where data scientist/data engineers/software engineers actually develop the product. The end user doesn’t have access to this environment which allows developers to try new features freely. </p></li><li><p><strong>QA environment / Testing environment</strong> : once a product is sufficiently mature to be tested, it is deployed to a new environment in order for testers to work on a stable version, while allowing developers to continue working in the development environment at the same time. The end user doesn’t have access to this environment</p></li><li><p><strong>Production environment</strong> : where the product will be deployed after testing and made accessible to the end client. Every features should work when the product reaches this stage.</p></li></ul><div align="center"><p><img loading="lazy" src="/assets/images/1_pbi-7fba92ca104a1e21a8a0cb954366abca.png" width="691" height="357" class="img_ev3q"></p></div><p>Furthermore, each environment should have its own database because in the same way that products evolve through development, databases also evolve: tables may be modified, added or deleted. Thus, the actual development process actually looks like this:</p><div align="center"><p><img loading="lazy" src="/assets/images/2_pbi-a2fd174f02de181e7229cb902e9e9bd6.png" width="490" height="300" class="img_ev3q"></p></div><p>Now that you know <strong>why</strong> you should use different environments when developing products, this article explains <strong>how</strong> to do that in Power BI! </p><div class="theme-admonition theme-admonition-info alert alert--info admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_S0QG"><p>In Power BI, the product you are developing is a <strong>Report</strong> and the environments are <strong>Workspaces</strong></p></div></div><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>Disclaimer  </div><div class="admonitionContent_S0QG"><ul><li>This guide supposes you have already developed a Power BI report using <a href="https://powerbi.microsoft.com/en-us/desktop/" target="_blank" rel="noopener noreferrer">Power BI Desktop</a> and published this report to a Power BI Workspace, accessible through the <a href="https://powerbi.microsoft.com/fr-fr/" target="_blank" rel="noopener noreferrer">Power BI Web Portal</a>. </li><li>These assumptions will be stated again at the beginning of each Part, in Step 0 - (Baseline).</li></ul></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="in-power-bi-desktop-make-your-data-sources-dynamic">In Power BI Desktop, make your data sources dynamic<a href="#in-power-bi-desktop-make-your-data-sources-dynamic" class="hash-link" aria-label="Direct link to In Power BI Desktop, make your data sources dynamic" title="Direct link to In Power BI Desktop, make your data sources dynamic">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="step-0---baseline-have-a-power-bi-report-connected-to-hard-coded-data-sources">Step 0 - (Baseline) Have a Power BI report connected to hard-coded data sources<a href="#step-0---baseline-have-a-power-bi-report-connected-to-hard-coded-data-sources" class="hash-link" aria-label="Direct link to Step 0 - (Baseline) Have a Power BI report connected to hard-coded data sources" title="Direct link to Step 0 - (Baseline) Have a Power BI report connected to hard-coded data sources">​</a></h3><p>As explained in the disclaimer, you should already have a report connected to data sources. Check out this documentation for more information about <a href="https://docs.microsoft.com/en-us/power-bi/connect-data/desktop-quickstart-connect-to-data" target="_blank" rel="noopener noreferrer">data sources</a> in <a href="https://docs.microsoft.com/en-us/power-bi/connect-data/desktop-data-sources" target="_blank" rel="noopener noreferrer">Power BI</a>. </p><p>A typical report would have several tables in the pane Fields: </p><div align="center"><p><img loading="lazy" src="/assets/images/3_pbi-a79129f5734dead688f2050862358db0.png" width="447" height="394" class="img_ev3q"></p></div><p>As a little foreshadowing and making sure we are on the same page, open the Power Query editor (by clicking <em>Home</em> (Top Ribbon) → <em>Transform Data</em> → <em>Transform Data</em>; then <em>Advanced Editor</em>) for one of your tables, and notice that all your data sourcing informations are hard-coded.</p><p><img loading="lazy" src="/assets/images/4_pbi-c2429c7124c92539f7905d0ccae07274.png" width="1916" height="371" class="img_ev3q"></p><p><img loading="lazy" src="/assets/images/5_pbi-6132b957d5cc458faa7ff7adeac543d8.png" width="1167" height="297" class="img_ev3q"></p><p>In the last picture, we have the following information hard-coded:</p><ul><li><p>The server is: dev-sql-01.database.windows.net</p></li><li><p>The database is: dev-db-01</p></li><li><p>The schema is: dev-rfd-crm</p></li></ul><p>This hard-coded information is problematic as when we publish the report from one workspace to the next, the datasource will not change: we will always get our information from “dev”. What we need are parameters that change automatically as we change workspaces.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="step-1--creating-parameters-inside-your-reports">Step 1- Creating Parameters inside your reports<a href="#step-1--creating-parameters-inside-your-reports" class="hash-link" aria-label="Direct link to Step 1- Creating Parameters inside your reports" title="Direct link to Step 1- Creating Parameters inside your reports">​</a></h3><p>In the Power Query Editor, click the <em>Manage Parameters</em> (top ribbon under the Home tab) → <em>New Parameter</em>. (Or you can click <em>Manage Parameters</em> and select the <em>New</em> button on the top to create a parameter). You can fill in this form and select OK to create a new parameter.</p><div align="center"><p><img loading="lazy" src="/assets/images/6_pbi-cac756688b33cb6d56f5a6b97031819f.png" width="498" height="187" class="img_ev3q"></p></div><p>You should create parameters for every variable that needs to change as you change workspaces. In my case, I needed to create 5 parameters:</p><ul><li><p>One server parameter</p></li><li><p>One database parameter</p></li><li><p>Two schema parameters</p></li><li><p>One parameter called “env” (with simply the options ‘dev’, ‘ppd’ and ‘prd')</p></li></ul><p><img loading="lazy" src="/assets/images/7_pbi-89490893349517986e76afafc9a840f5.png" width="895" height="966" class="img_ev3q"></p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="step-2--using-the-advanced-editor-to-use-the-parameters-when-connecting-to-your-data-source">Step 2- Using the ‘Advanced Editor’ to use the parameters when connecting to your data source<a href="#step-2--using-the-advanced-editor-to-use-the-parameters-when-connecting-to-your-data-source" class="hash-link" aria-label="Direct link to Step 2- Using the ‘Advanced Editor’ to use the parameters when connecting to your data source" title="Direct link to Step 2- Using the ‘Advanced Editor’ to use the parameters when connecting to your data source">​</a></h3><p>Now that you have created your parameters, use the following syntax to refer to your parameters when connecting to your datasource. </p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">#"ParameterName"</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Here is what the difference looks like for me. When the data sources variables are hard coded:</p><p><img loading="lazy" src="/assets/images/8_pbi-6132b957d5cc458faa7ff7adeac543d8.png" width="1167" height="297" class="img_ev3q"></p><p>When the data sources variables use parameters:</p><p><img loading="lazy" src="/assets/images/9_pbi-2e3251dd4849eef049f68ff688e41776.png" width="1005" height="241" class="img_ev3q"></p><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>You made it through Part 1!</div><div class="admonitionContent_S0QG"><p>You now know how to make your data sources dynamic inside your reports, you implemented it and pushed your report to your workspace.  </p><p>And know you ask yourself: “<em>But now, how can I make multiple workspaces and how do I change my report parameters automatically as I publish my report between workspaces?</em>” </p><p>Check out Part 2 for the answer ! </p></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="part-2-in-the-power-bi-web-portal-create-a-deployment-pipeline-and-assign-parameters-to-each-workspace">Part 2: In the Power BI Web Portal, create a deployment pipeline and assign parameters to each workspace<a href="#part-2-in-the-power-bi-web-portal-create-a-deployment-pipeline-and-assign-parameters-to-each-workspace" class="hash-link" aria-label="Direct link to Part 2: In the Power BI Web Portal, create a deployment pipeline and assign parameters to each workspace" title="Direct link to Part 2: In the Power BI Web Portal, create a deployment pipeline and assign parameters to each workspace">​</a></h2><p>In order to use parameters, we will leverage Power BI’s Deployment Pipeline feature. I encourage you to read Microsoft’s official documentation <a href="https://docs.microsoft.com/en-us/power-bi/create-reports/deployment-pipelines-get-started" target="_blank" rel="noopener noreferrer">here</a> if you encounter any issues when working with Deployment pipelines as it is fairly comprehensive. </p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="step-0---baseline-have-your-power-bi-report-already-published-to-a-workspace">Step 0 - (Baseline) Have your Power BI report already published to a Workspace<a href="#step-0---baseline-have-your-power-bi-report-already-published-to-a-workspace" class="hash-link" aria-label="Direct link to Step 0 - (Baseline) Have your Power BI report already published to a Workspace" title="Direct link to Step 0 - (Baseline) Have your Power BI report already published to a Workspace">​</a></h3><p>Check out this <a href="https://www.youtube.com/watch?v=E0L1uXfefms" target="_blank" rel="noopener noreferrer">video</a> if you haven’t already published your report to a Power BI Workspace (don’t hesitate to watch at speed x1.75 😉 ) .</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="step-1--creating-a-deployment-pipeline-from-your-workspace">Step 1- Creating a Deployment Pipeline from your Workspace<a href="#step-1--creating-a-deployment-pipeline-from-your-workspace" class="hash-link" aria-label="Direct link to Step 1- Creating a Deployment Pipeline from your Workspace" title="Direct link to Step 1- Creating a Deployment Pipeline from your Workspace">​</a></h3><div align="center"><p><img loading="lazy" src="/assets/images/10_pbi-9e58c88d0464b8c1bbfd9b8049f5a4d0.png" width="652" height="416" class="img_ev3q"></p></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="step-2--deploy-your-report-from-the-power-bi-development-workspace-to-the-power-bi-test-workspace">Step 2- Deploy your report from the Power BI Development Workspace to the Power BI Test Workspace<a href="#step-2--deploy-your-report-from-the-power-bi-development-workspace-to-the-power-bi-test-workspace" class="hash-link" aria-label="Direct link to Step 2- Deploy your report from the Power BI Development Workspace to the Power BI Test Workspace" title="Direct link to Step 2- Deploy your report from the Power BI Development Workspace to the Power BI Test Workspace">​</a></h3><p><img loading="lazy" src="/assets/images/11_pbi-b76cec09a1ad258ad02ae279b16c20fe.png" width="1674" height="532" class="img_ev3q"></p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="step-3--modify-the-deployment-parameters-to-use-the-correct-parameters">Step 3- Modify the Deployment Parameters to use the correct parameters<a href="#step-3--modify-the-deployment-parameters-to-use-the-correct-parameters" class="hash-link" aria-label="Direct link to Step 3- Modify the Deployment Parameters to use the correct parameters" title="Direct link to Step 3- Modify the Deployment Parameters to use the correct parameters">​</a></h3><ul><li><p>In the pipeline stage you want to create a dataset rule for, select <strong>Deployment settings</strong>.</p></li><li><p>From the Deployment settings pane, select the dataset you want to create a rule for.</p></li><li><p>Select <strong>Parameter rules</strong>, expand the list, and then select <strong>Add rule</strong>.</p></li><li><p>Select a parameter from the list of parameters; the current value is shown. Edit the value to the value you want to take effect after each deployment.</p></li></ul><div align="center"><p><img loading="lazy" src="/assets/images/12_pbi-88b95be953eb9c1dd28e2316ad1eed1f.png" width="1081" height="533" class="img_ev3q"></p><p><img loading="lazy" src="/assets/images/13_pbi-76e9ba92a694789e303b7271df2bf614.png" width="1666" height="588" class="img_ev3q"></p><p><img loading="lazy" src="/assets/images/14_pbi-976587e38b0e5f0a9996df0f089deba6.png" width="592" height="802" class="img_ev3q"></p></div><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_S0QG"><p>Notice on the bottom of the last screenshot the following mention: “<em>Rules will be applied when you deploy to this stage</em>”.</p><p>Thus: once you have set the deployment settings for your report, you need to redeploy your report immediately to this workspace because the rules haven’t been applied yet 😅 .  </p></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><p>So this is how to use different environments in Power BI in a nutshell. We went from one workspace with a hard-coded data source, to three separate workspaces where when a report is pushed from one workspace to an other, its data source changes automatically! </p><p>Definitely a lot of moving pieces here, but understanding theses concepts will make your workflow easier and less error-prone in the long-run ! </p><p>I hope this information can help you on your Power BI journey!</p>]]></content:encoded>
            <category>Power BI</category>
            <category>Environments</category>
        </item>
        <item>
            <title><![CDATA[Newsletter for February 2022]]></title>
            <link>https://ekimetrics.github.io/blog/2022/02/28/newsletter_Feb-2022</link>
            <guid>https://ekimetrics.github.io/blog/2022/02/28/newsletter_Feb-2022</guid>
            <pubDate>Mon, 28 Feb 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[Ekimetrics is pleased to release its second Newsletter of 2022! This cheat sheet will brief you quickly about the latest news in the fields of Data Science, Machine Learning, Data Engineering and Application Development. Warning this one is particularly ruled by DeepMind so be prepared!]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/green_grass_and_sun-17c808386febcf3ecddcabd75c81d223.jpg" width="7360" height="4100" class="img_ev3q"></p></div><div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="data-science">Data Science<a href="#data-science" class="hash-link" aria-label="Direct link to Data Science" title="Direct link to Data Science">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="deepmind-presents-alphacode-your-next-worst-enemy-in-coding-test">DeepMind presents AlphaCode, your next worst enemy in coding test<a href="#deepmind-presents-alphacode-your-next-worst-enemy-in-coding-test" class="hash-link" aria-label="Direct link to DeepMind presents AlphaCode, your next worst enemy in coding test" title="Direct link to DeepMind presents AlphaCode, your next worst enemy in coding test">​</a></h3><p>Google’s AI subsidiary keeps a high publishing pace by presenting a new powerful programming tool. AlphaCode is an algorithm able to understand a required task then generate a relevant and usable code. Beyond being a proof that AI-generated programs are not so far away, their methodology to build the model is particularly interesting. To make it simple, they leveraged a big amount of data from GitHub to pre-train a model able to understand what code is. This pre-trained model is then fine-tuned using CodeContests problems and solutions to precisely show what is a problem and how to solve it with code. Do not hesitate to check this out!</p><p><a href="https://deepmind.com/blog/article/Competitive-programming-with-AlphaCode" target="_blank" rel="noopener noreferrer">Competitive programming with AlphaCode | DeepMind</a></p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="data-starts-to-be-a-vip-for-fashion-weeks">Data starts to be a VIP for fashion weeks<a href="#data-starts-to-be-a-vip-for-fashion-weeks" class="hash-link" aria-label="Direct link to Data starts to be a VIP for fashion weeks" title="Direct link to Data starts to be a VIP for fashion weeks">​</a></h3><p>Fashion, and any creative industries, often neglected data science as their core value is delivered through artistic creation. However, with the rise of Deep-Learning, we are observing a progressive adoption of analytics in some fields, starting by fashion. This link here under shows some types of analytics that some specific companies can extract from Fashion weeks. Even if it’s still relatively simple, several companies start to be specialized in the era of FashionTech. If you’re interested in the topic: go check Heuritech and EvaEngines!</p><p><a href="https://ww.fashionnetwork.com/news/What-were-the-key-trends-during-haute-couture-and-men-s-fashion-weeks-based-on-data-,1378968.html" target="_blank" rel="noopener noreferrer">What were the key trends during Haute Couture and Men's Fashion Weeks based on data?</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="machine-learning">Machine Learning<a href="#machine-learning" class="hash-link" aria-label="Direct link to Machine Learning" title="Direct link to Machine Learning">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="uber-showcases-its-new-technology-for-predicting-deliveries-commands-times-and-routes">Uber showcases its new technology for predicting deliveries, commands, times and routes<a href="#uber-showcases-its-new-technology-for-predicting-deliveries-commands-times-and-routes" class="hash-link" aria-label="Direct link to Uber showcases its new technology for predicting deliveries, commands, times and routes" title="Direct link to Uber showcases its new technology for predicting deliveries, commands, times and routes">​</a></h3><p>The core business of Uber relies on delivering its services in the shortest period possible. The key ingredient in such task is being able to forecast accurately orders’ preparation, availabilities, routes… Usually Uber used Gradient Boosted Trees with advanced improvements making it deeper and more accurate. The drawback of such methodology is that you cannot indefinitely increase the dataset and the model sizes as it becomes technically impossible to use. Uber decided to switch to Deep-Learning as an answer to such problems and delivered a forecasting algorithm relying on Transformers beating their previous ones. While the academic background of such model can be heavy, it still clearly highlights some key components of Machine Learning Projects and should be considered as a whole tutorial!</p><p><a href="https://eng.uber.com/deepeta-how-uber-predicts-arrival-times/" target="_blank" rel="noopener noreferrer">DeepETA: How Uber Predicts Arrival Times Using Deep Learning</a></p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="nuclear-fusion-with-algorithms-deepminds-breakthrough">Nuclear fusion with algorithms, DeepMind’s breakthrough<a href="#nuclear-fusion-with-algorithms-deepminds-breakthrough" class="hash-link" aria-label="Direct link to Nuclear fusion with algorithms, DeepMind’s breakthrough" title="Direct link to Nuclear fusion with algorithms, DeepMind’s breakthrough">​</a></h3><p>This is one of the hottest news being recently released in the field of Machine Learning as DeepMind (sh*t again them) published an article discussing the use of ML within the nuclear industry. Using Reinforcement Learning, the team demonstrated that RL can improve plasma control meaning a big step toward nuclear fusion. Following their previous paper about AlphaFold in 2021, DeepMind keeps breaking sciences’ limits!</p><p><a href="https://deepmind.com/blog/article/Accelerating-fusion-science-through-learned-plasma-control" target="_blank" rel="noopener noreferrer">Accelerating fusion science through learned plasma control | DeepMind</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="data-engineering--architecture">Data Engineering &amp; Architecture<a href="#data-engineering--architecture" class="hash-link" aria-label="Direct link to Data Engineering &amp; Architecture" title="Direct link to Data Engineering &amp; Architecture">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="airflow-starts-being-cut-in-pieces-whats-left">AirFlow starts being cut in pieces, what’s left?<a href="#airflow-starts-being-cut-in-pieces-whats-left" class="hash-link" aria-label="Direct link to AirFlow starts being cut in pieces, what’s left?" title="Direct link to AirFlow starts being cut in pieces, what’s left?">​</a></h3><p>You may have heard about AirFlow, maybe you worked with it or experienced some features. To make it simple AirFlow is a workflow manager programmed in Python and available thanks to Apache (and AirBnB originally). Now that the Data Engineering field is booming, many players are coming on the market tearing in parts the AirFlow business. AirBytes, Census or Arize are among the players in this field. While AirFlow is practical for its non-SQL functioning, there will be some upcoming consolidation in the field as newcomers keep cutting the cake in littler pieces.</p><p><a href="https://blog.fal.ai/the-unbundling-of-airflow-2/" target="_blank" rel="noopener noreferrer">The Unbundling of Airflow (fal.ai)</a></p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="flight-sql-a-new-feature-in-apache-arrow">Flight SQL, a new feature in Apache Arrow<a href="#flight-sql-a-new-feature-in-apache-arrow" class="hash-link" aria-label="Direct link to Flight SQL, a new feature in Apache Arrow" title="Direct link to Flight SQL, a new feature in Apache Arrow">​</a></h3><p>This article discusses Flight SQL, a new client-server protocol developed by the Apache Arrow community for interacting with SQL databases that makes use of the Arrow in-memory columnar format and the Flight RPC framework.</p><p><a href="https://arrow.apache.org/blog/2022/02/16/introducing-arrow-flight-sql/" target="_blank" rel="noopener noreferrer">Introducing Apache Arrow Flight SQL: Accelerating Database Access | Apache Arrow</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="app-and-web-development">App and Web Development<a href="#app-and-web-development" class="hash-link" aria-label="Direct link to App and Web Development" title="Direct link to App and Web Development">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="low-code-vs-traditional-development-a-guide-to-help-you-better-understand-the-stakes">Low code vs Traditional development, a guide to help you better understand the stakes<a href="#low-code-vs-traditional-development-a-guide-to-help-you-better-understand-the-stakes" class="hash-link" aria-label="Direct link to Low code vs Traditional development, a guide to help you better understand the stakes" title="Direct link to Low code vs Traditional development, a guide to help you better understand the stakes">​</a></h3><p>You always dreamed to deeply understand the difference between Low Code and Traditional development? The following article details with very clear examples and exhaustive information what are the stakes!</p><p><a href="https://www.monocubed.com/blog/low-code-vs-traditional-development/#:~:text=Low%20code%20development%20is%20developing,software%20applications%20using%20manual%20coding.&amp;text=Rapid%20application%20development%20tools%20such,low%2Dcode%20web%20app%20development." target="_blank" rel="noopener noreferrer">Which Method to Use – Low Code vs Traditional Development?</a></p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-2022-guide-for-devs">The 2022 guide for devs<a href="#the-2022-guide-for-devs" class="hash-link" aria-label="Direct link to The 2022 guide for devs" title="Direct link to The 2022 guide for devs">​</a></h3><p>Here under is a quick catchup about the best practices for devs for the upcoming year. It may give you some ideas on some tools to use in your mission. If you test some of them do not hesitate to share it to the Eki Community so that we can thrive together! </p><p><a href="https://betterprogramming.pub/5-dev-tools-to-look-out-for-in-2022-713f94c0f3cf" target="_blank" rel="noopener noreferrer">5 Dev Tools To Look Out For In 2022 | by Carlo Morrone | Better Programming</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><p>A promising start of 2022 that set the standards high for the year! If one article has a particular interest for you, we invite you to discuss it with your colleagues or reach out to the Innovation Community!</p></div>]]></content:encoded>
            <category>DeepMind</category>
            <category>AirFlow</category>
            <category>Apache Arrow</category>
            <category>FlightSQL</category>
            <category>Low code</category>
        </item>
        <item>
            <title><![CDATA[Demand Forecasting - Improving Supply Chain operations in uncertain times]]></title>
            <link>https://ekimetrics.github.io/blog/2022/02/22/demand_forecasting_supply</link>
            <guid>https://ekimetrics.github.io/blog/2022/02/22/demand_forecasting_supply</guid>
            <pubDate>Tue, 22 Feb 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[Learn in this article how we industrialized a solution for warehouse demand forecasting to help Operations managers better allocate resources for more than 20.000 referenced products]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/supply_warehouse-2f0d8fd3d2685798e71be2d745374eea.png" width="1200" height="900" class="img_ev3q"></p></div><div align="justify"><p>Demand forecasting has become a trendy topic since the COVID pandemic. It has challenged companies' Supply Chain operations and highlighted that most companies could improve their supply chain management. Therefore, generating demand for enhanced forecasting capabilities.</p><p>In 2021, we had the opportunity to work with a French pioneer in manufacturing on this topic. We had to industrialize a solution for warehouse demand forecasting for more than 20,000 referenced products to improve overall Supply Chain performance. At the time, the existing solution was deprecated and not handling correctly erratic sales. </p><p>Therefore, we had to create a new, more robust algorithm to help Operational Managers better allocate resources. This latter solution enabled a 60% accuracy improvement and was integrated into Operational Managers processes for decision making.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="an-important-asset-for-supply-chain-transformation">An important asset for Supply Chain transformation<a href="#an-important-asset-for-supply-chain-transformation" class="hash-link" aria-label="Direct link to An important asset for Supply Chain transformation" title="Direct link to An important asset for Supply Chain transformation">​</a></h2><p>Demand forecasting has an impact on different levers:</p><ul><li><p><em>Stock Management</em>: forecasting can drastically improve Inventory Management. Underproducing means losing revenue opportunities, whilst overproducing means selling your products cheaper or facing additional storage costs. Furthermore, with the return of activity after the first lockdown, suppliers are now bearing the brunt of building sites recovery, causing delays and creating supply instability.</p></li><li><p><em>Resource allocation</em>: defining the best number of weekly working hours is not an easy task for operational managers, and anticipation is key to achieving optimal throughput.</p></li><li><p><em>Product strategy</em>: demand forecasting also supports better handling of specific trends and product lifecycle. Hence, it helps warehouses anticipate and meet high demands, detect declined products and help marketing teams better understand customers’ needs.</p></li></ul><p>Most importantly, Demand Forecasting has not become a hot topic because of the pandemic. Nevertheless, it brought to light the need of transforming Supply Chains and adapt to unexpected events. Indeed, global warming is increasing the frequency of natural disasters, which will test Supply Chains resilience (<em>have you heard of our AI4S offer?</em>).</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-to-build-demand-forecasting-tool">How to build Demand Forecasting tool<a href="#how-to-build-demand-forecasting-tool" class="hash-link" aria-label="Direct link to How to build Demand Forecasting tool" title="Direct link to How to build Demand Forecasting tool">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-are-we-trying-to-solve">What are we trying to solve?<a href="#what-are-we-trying-to-solve" class="hash-link" aria-label="Direct link to What are we trying to solve?" title="Direct link to What are we trying to solve?">​</a></h3><p>Framing the problem is key to ensuring Data Scientists go in the right direction. When discussing with Supply Chain managers, we understood they wanted to reduce their costs. Thus, improving resource allocation was the best lever to achieve their need. Clearly stating the business need allowed us to understand the problem and impacted the solution implementation.</p><p><img loading="lazy" alt="Business Problem" src="/assets/images/img1_v2-fa995a6196770755daac31f688954430.png" width="1449" height="616" class="img_ev3q"></p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="build-and-assess-model">Build and assess model<a href="#build-and-assess-model" class="hash-link" aria-label="Direct link to Build and assess model" title="Direct link to Build and assess model">​</a></h3><p>Because of the high number of products and their associated nomenclature, we used a decision tree algorithm (LightGBM). The business required a reliable mid-term forecasting tool within a short time lap and with limited team size, justifying the choice of a well-known and high-performance algorithm. Our solution is a 3-week forecasting tool with daily updates for all referenced products.</p><p>The ML pipeline we have built for this project is similar to many existing ones. Therefore, we will focus on how the business problem framed our solution. Still, this project was an opportunity for us to use <a href="https://kedro.readthedocs.io/en/stable/index.html" target="_blank" rel="noopener noreferrer">Kedro</a>, which enabled Data Science teams to set up their ML data pipelines. Kedro had the advantage of visualizing your pipeline and structuring our code to ensure maintainability and reproducibility.</p><p><img loading="lazy" alt="Kedro Pipeline" src="/assets/images/kedro_2-dd195cb1648e4b21ef03025f838dec25.png" width="1843" height="851" class="img_ev3q"></p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="modelling-target">Modelling target<a href="#modelling-target" class="hash-link" aria-label="Direct link to Modelling target" title="Direct link to Modelling target">​</a></h4><p>We want to optimize warehouse working hours to ensure clients satisfaction by delivering stores on time.</p><p>What requires the most operational effort? Our client uses single order picking methods - meaning that pickers work on one order at a time. Thus, it is more time-consuming for a picker to prepare two orders of the same product rather than picking two times the product for the same order. Consequently, we focused on predicting the number of orders rather than the quantity ordered.</p><p>NB: depending on how your supply chain operates, the modelling output would differ.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="modelling-granularity">Modelling granularity<a href="#modelling-granularity" class="hash-link" aria-label="Direct link to Modelling granularity" title="Direct link to Modelling granularity">​</a></h4><p>Our client had a complex product hierarchy, meaning products can be aggregated at different levels. Demand forecasting at a product level gives higher flexibility when used on various business use cases, albeit not being the optimal solution for each one. Therefore, we aggregated ordered products at a higher granularity, to find the best balance between modelling accuracy and business need.</p><p>Working at a product level when having more than 20,000 products can be painful:</p><ul><li><p>it means working with sparse time series (which isn’t trivial when using decision trees)</p></li><li><p>manufacturing companies often deal with products turnover (because products are frequently improved), meaning you may lose the thread of which product is getting improved.</p></li></ul><p>Therefore, to optimize resource allocation, it was relevant to build a forecasting tool at a product category level, to combine similar products and avoid dealing with product’s turnover. Analyzing sparsity at different granularities also helped us define the best modelling level.</p><p><img loading="lazy" alt="Histogram" src="/assets/images/img3-4c20db5d27cd65a1dfd1a1d6e72e841a.png" width="985" height="280" class="img_ev3q"></p><div align="center"> Distribution of products' sparsity (e.g. percentage of zeroes in orders). It is clear that most products have more than 90% of zeroes.</div><br><p>Important learnings here were:</p><ol><li>Sparse time series are hard to predict</li><li><strong>It is best to go for a solution where the unit (time x product x localization) of forecasting matches the unit of decision making</strong></li></ol><p><img loading="lazy" alt="Predictions" src="/assets/images/img4-e049c7e24e00a60d493aa7890847c7c2.png" width="1190" height="295" class="img_ev3q"></p><div align="center"> We see how the coronavirus outbreak impacted the activity of our client during the first containment. Predicting this period is pretentious, but how can it help us reduce uncertainty in the future?</div><br><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="modelling-loss-function">Modelling loss function<a href="#modelling-loss-function" class="hash-link" aria-label="Direct link to Modelling loss function" title="Direct link to Modelling loss function">​</a></h4><p>Modelling errors assess algorithms performance and help choose the model we will industrialize to better impact operations. However, when deploying a solution, we want to measure an error that reflects the current problem. Demand forecasting is not exclusively about being accurate on future demands but also about improving operations. Therefore, customizing errors metrics is a key part of machine learning development, allowing us to define the right balance between product demand (model accuracy) and product average preparation time. (focus on products with high preparation time).</p><p>We implemented the Weighted Root Mean Squared Scaled Error, using product average revenues in the last 6 months and product average preparation time.</p><p><img loading="lazy" alt="Prep Time" src="/assets/images/table_2_v2-a76fe9d47f686003856484e216f304d2.png" width="2759" height="974" class="img_ev3q"></p><div align="center"> Product B weekly preparation time is 400 min vs 200 min for Product A. Thus, higher accuracy on Product B is needed to reflect the operational needs.</div><br><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="evaluate-business-outcome">Evaluate business outcome<a href="#evaluate-business-outcome" class="hash-link" aria-label="Direct link to Evaluate business outcome" title="Direct link to Evaluate business outcome">​</a></h3><p>Because predictions are not always (never) linked to business outcomes, it is important to differentiate outputs (what are the predictions?) from outcomes (how will it impact the business?). This part was not implemented during the project, but we will briefly explain the methodology to give you a flavour on how it could be done.</p><p>Model accuracy is a technical metric, so to quantify our ML model impact on operations we also need to estimate the predicted number of working hours. Comparing the number of predicted working hours with the realized working hours would allow measuring how much the algorithm is reducing costs.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="create-a-sustainable-solution">Create a sustainable solution<a href="#create-a-sustainable-solution" class="hash-link" aria-label="Direct link to Create a sustainable solution" title="Direct link to Create a sustainable solution">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="implement-a-web-app">Implement a web app<a href="#implement-a-web-app" class="hash-link" aria-label="Direct link to Implement a web app" title="Direct link to Implement a web app">​</a></h3><p>A core part of our solution was to implement an easy-to-use application for operations managers. It helped them access predictions in real-time, assess model past performance and define the optimal working hours needed. We used Streamlit for its ease of use and ability to create fast and intuitive applications whenever we are short on time. Consequently, we created a usable solution by communicating predictions, demand, and past errors at different granularities.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="add-a-feedback-loop">Add a feedback loop<a href="#add-a-feedback-loop" class="hash-link" aria-label="Direct link to Add a feedback loop" title="Direct link to Add a feedback loop">​</a></h3><p>Finally, we created a feedback loop to enable the team to maintain, improve and sustain the solution. This concept is well explained in <em>Prediction Machines: The Simple Economics of Artificial Intelligence</em>, a book about what AI means from an economist perspective. They introduce a great concept they have named the Anatomy of a Task.</p><p><img loading="lazy" alt="Anatomy of a Task" src="/assets/images/img5_v3-2cba57b6da74cb303124b1246f9d2b62.png" width="1175" height="757" class="img_ev3q"></p><p>Predictions mainly impact people’s judgement and decisions taken for action. It implies that algorithms are rather helpful for decision-makers than make the actual choice. In our situation, the predictions enable operational managers to take action on the number of pickers needed in the coming weeks. Improving modelling should help these managers be more accurate in their decision process. Giving Data Science teams feedback on how modelling affected outcomes is the added value.</p><p>Consequently, we decided to create a clear feedback process with the operational managers. We made sure we could improve our forecasting model in a business manner rather than a statistical one. Our model not being perfect, the operational team’s feedback on how our predictions are impacting their daily work will help us improve our algorithms and create a sustainable solution.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="concluding-and-food-for-thought">Concluding and food for thought<a href="#concluding-and-food-for-thought" class="hash-link" aria-label="Direct link to Concluding and food for thought" title="Direct link to Concluding and food for thought">​</a></h2><p>New challenges will arise, and data will help supply chains tackle them. Creating a data-driven culture is key to useful and sustainable solutions. Creating state-of-the-art models is great, but a successful data science transformation is about creating impactful projects and delivering value throughout.</p><p>Thanks to this project, we learnt that:</p><ul><li><p>Demand Forecasting is a great entry to improve Supply Chain operations, but the impact will be limited if not embedded in the full Supply Chain environment.</p></li><li><p>Demand forecasting is not about predicting the future, but about how data helps improve communication between teams.</p></li></ul></div>]]></content:encoded>
            <category>Demand Forecasting</category>
            <category>Supply Chain</category>
            <category>LightGBM</category>
            <category>Streamlit</category>
        </item>
        <item>
            <title><![CDATA[Ekimetrics implements Data Mesh. Here is why, and why you should join us]]></title>
            <link>https://ekimetrics.github.io/blog/2022/02/09/ekimetrics_implements_data_mesh</link>
            <guid>https://ekimetrics.github.io/blog/2022/02/09/ekimetrics_implements_data_mesh</guid>
            <pubDate>Wed, 09 Feb 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[This article describes one of the inspirational patterns that drive our projects’ data architectures, the Data Mesh.]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/mesh_building_kennedy-90cf3c430bada4ba07872a6159cd20e4.jpg" width="774" height="581" class="img_ev3q"></p></div><div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h2><p>Ekimetrics’ approach to data management starts from prioritized data use-cases, vision for architecture and vision for organization. Although the vision for architecture is tailored to the culture of every customer, it is inspired by the practices we see as inspiring. This article describes one of these inspirational patterns that drive our projects’ data architectures, the Data Mesh.</p><p>I must introduce Ekimetrics first, we are 320 data scientists, with people and projects in Europe, in the US and in Asia. The goal of Ekimetrics is to become the most trusted partner in data science for business. Every consultant has their preferred topics: Vertical business expertise is the most represented, which is nice because that’s where we derive use-cases from, then statistics / ml / advanced algorithms, then, equally represented, product development and data architecture and engineering. With a couple of colleagues, I lead that data architecture and engineering practice.</p><p>Data Mesh is a new framework that emerged recently, it is so deep and wide that we believe that it has the potential to influence data management for a whole decade.</p><p>The foundational papers, by Zhamak Dehghani (<a href="https://twitter.com/zhamakd" target="_blank" rel="noopener noreferrer">zhamakd</a>) in 2019 are nothing short of fantastic (see articles <a href="https://martinfowler.com/articles/data-monolith-to-mesh.html" target="_blank" rel="noopener noreferrer">How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh</a>  and <a href="https://martinfowler.com/articles/data-mesh-principles.html" target="_blank" rel="noopener noreferrer">Data Mesh Principles and Logical Architecture</a>). They address real-life challenges, they indicate how architectural and organizational aspects can be worked together and they really bring a new energy in data management. So, while I will cherry-pick some of the data mesh ideas that we are implementing today, I strongly encourage you to read both articles thoroughly. Said differently, what I describe below is only what is closest to our mindset and to our maturity on the projects, but there are many more insights in the original papers.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-is-data-mesh">What is data mesh?<a href="#what-is-data-mesh" class="hash-link" aria-label="Direct link to What is data mesh?" title="Direct link to What is data mesh?">​</a></h2><p>Data Mesh is a nice way to organize and grow data assets that serve business purposes. Data Mesh takes a limited number of simple ideas for data management, and all the terms used below are described in detail in the two foundational articles:</p><ul><li><p>Ownership of data should be within the hands of the business teams,</p></li><li><p>Data boundaries should be created along business boundaries for data security, data sharing, cost management, etc., these are called Data Domains,</p></li><li><p>Sharing datasets inside and possibly outside the company should be simple,</p></li><li><p>Focusing on data products is a good approach to create value from data,</p></li><li><p>Federated governance is an approach that will scale where other cannot,</p></li><li><p>There is a harmonious way for IT, Data and Business teams to collaborate around data,</p></li></ul><p>We see Data Mesh as a holistic way to organize data management. And since some cloud vendors have managed services that can help organize data like that, we have been working in that direction successfully with many organizations.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="who-is-data-mesh-for">Who is data mesh for?<a href="#who-is-data-mesh-for" class="hash-link" aria-label="Direct link to Who is data mesh for?" title="Direct link to Who is data mesh for?">​</a></h2><p>We did not identify customer profiles where data mesh would not apply, so we believe it is ubiquitous. Medium and large organizations in financial services, retail, utilities, luxury, etc., both the references we have and the references we learn about prove it applies to all kind of companies.</p><p>Limits are on the capability to challenge existing data assets and existing data culture, and it is not always the right moment to change these. That said, it should be possible to gradually transition from a monolithic data lake to a data mesh, by slowly introducing data domains.</p><p>And a consultancy firm like Ekimetrics is the right place to be to participate in these changes !</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-is-the-relationship-with-gcp-synapse-or-databricks">What is the relationship with GCP, Synapse or Databricks?<a href="#what-is-the-relationship-with-gcp-synapse-or-databricks" class="hash-link" aria-label="Direct link to What is the relationship with GCP, Synapse or Databricks?" title="Direct link to What is the relationship with GCP, Synapse or Databricks?">​</a></h2><p>We see the latest generation of platforms is indeed very aligned with what data mesh requires, and GCP BigQuery, Azure Synapse and Databricks are good candidates for Data Mesh deployments.</p><p>With GCP as an example, it is easy to separate the cost of the storage in one project and the cost of the processing in another project and it’s easy to leverage the data that one part of the company has prepared into a data product that is owned by another part of the company, without copying data, which is a data mesh idea. While this has been in BigQuery for a decade now, the framework now has a name!</p><p>Such solutions can now be built with GCP, Synapse and Databricks.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><p>If you are interested in building Data Mesh with us, please visit our <a href="https://ekimetrics.com/careers/" target="_blank" rel="noopener noreferrer">website (careers page)</a> or our <a href="https://www.welcometothejungle.com/fr/companies/ekimetrics" target="_blank" rel="noopener noreferrer">page on Welcome to the Jungle</a>.</p></div>]]></content:encoded>
            <category>Data Mesh</category>
            <category>Architecture</category>
        </item>
        <item>
            <title><![CDATA[Building a datalake - Part 1 - Usable, Useful, Used, or how to avoid dataswamp and empty shell traps]]></title>
            <link>https://ekimetrics.github.io/blog/2022/02/07/building_datalake_part_1</link>
            <guid>https://ekimetrics.github.io/blog/2022/02/07/building_datalake_part_1</guid>
            <pubDate>Mon, 07 Feb 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[In this article, learn how to create your first datalake following best practices in order to make it robust, evolutive and central to your company's information architecture, as well as take advantage of the opportunities it presents.]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/image_overview_article_datalake_1-5cfbda6ff15b79dc1a23bd1f1acf2d3d.jpg" width="1920" height="1082" class="img_ev3q"></p></div><div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="article-scope">Article Scope<a href="#article-scope" class="hash-link" aria-label="Direct link to Article Scope" title="Direct link to Article Scope">​</a></h2><p>Scalable and distributed, datalakes have become the new go-to for centralizing storage and insights. The main challenge is not to create one, but rather to do it right and avoid falling into the so-called <strong>data swamp and empty shell traps</strong>. In this article, we’ll go through how you can create your first datalake following best practices in order to make it robust, evolutive and central to your company’s information architecture, as well as take advantage of the opportunities it presents.</p><p>In this post, we’ll be technology agnostic. The focus here is a smart architecture that will help solve functional and technical challenges, whether you’re using Azure, Amazon Web Services, Google Cloud Platform, any other provider or even building &amp; managing the infrastructure yourself.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h2><p>Before talking about how to design a datalake, let’s review what it is, why it’s useful and the main aspects that will ensure its quality.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-is-a-datalake">What is a datalake?<a href="#what-is-a-datalake" class="hash-link" aria-label="Direct link to What is a datalake?" title="Direct link to What is a datalake?">​</a></h3><p>A datalake is a global storage space allowing to interact with all data type: unstructured (e.g. text documents, images, videos), semi-structured (e.g. XML / JSON files), structured (e.g. SQL tables, CSV files). Data is extracted from all relevant sources, loaded into the datalake, then transformed into their use case specific target.</p><p>A well-designed datalake is flexible and scalable, allowing it to handle the increase in data volume, velocity, and variety. Because a datalake stores raw data before transforming it, it is extremely useful for extracting potential value from the data for the future. By centralizing data sources into a global storage space, it helps data scientists and analysts explore data to generate new use cases and business insights.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="is-building-a-datalake-the-right-solution">Is building a datalake the right solution?<a href="#is-building-a-datalake-the-right-solution" class="hash-link" aria-label="Direct link to Is building a datalake the right solution?" title="Direct link to Is building a datalake the right solution?">​</a></h3><p>The goal shouldn’t be to build a datalake for the sake of building one. A datalake is only the foundation, corner pieces in your larger data analytics and data architecture puzzle. Your goal should be to find a use case from which you can build a robust and evolutive foundation to your technical and functional architecture. Having a clear vision as to how your datalake can grow in a 2-year horizon, with new data sources and analytical use cases, is essential.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-data-swamp-and-empty-shell-traps">The data swamp and empty shell traps<a href="#the-data-swamp-and-empty-shell-traps" class="hash-link" aria-label="Direct link to The data swamp and empty shell traps" title="Direct link to The data swamp and empty shell traps">​</a></h3><p>Chasing the dream of a datalake that meets all current and future analytics needs or that unites all of your organization’s stakeholders around a single vision is an excellent way to fall into the data swamp or empty shell traps.</p><p>By trying to collect all current – and future – data sources from the start, there is a risk that your datalake will become a swamp. When that’s the case, due to unorganized, unvalidated or simply hard to exploit data, it can be incredibly challenging to make your datalake useable.</p><p>Your first goal should be to analyze which high impact use cases will be used by your organization’s stakeholders and target data for these specific use cases. Starting with the business needs instead of chasing a technical achievement, you can be sure that your datalake will be used and useful, instead of an empty shell: a good foundation but unable to attract your organization’s stakeholders. Read more about how we can avoid missing out on business impact in <a href="https://ekimetrics.com/white-paper/data-science-industrialization-business-impact/" target="_blank" rel="noopener noreferrer">this whitepaper</a>.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-foundation-architecture">The foundation architecture<a href="#the-foundation-architecture" class="hash-link" aria-label="Direct link to The foundation architecture" title="Direct link to The foundation architecture">​</a></h2><p>Let us first establish the building blocks of your datalake. This foundation is the basis for logical separation of your data and use cases in the platform you'll be building.</p><p>A datalake can ingest any type of data: from structured tables to unstructured files, whether ingested by batch or through streaming processes. Data will transit through four zones in sequence, from its raw form to fully processed for use cases, and from a technical to a business orientation.</p><ul><li><p>Landing LAN : temporary zone for all sources. Once data is extracted, it transits through this zone and is dispatched into the next zone by validation mechanisms.</p></li><li><p>RAW (also known as bronze): data extraction pipelines will fetch data in the landing zone and validate it technically as well as functionally to make sure it is usable. This zone is the main storage component for all your data, and contains every extracted source, raw and untransformed, only with a concern for what is technically and functionally usable.</p></li></ul><p>In landing and raw zone, data is organized logically by source: CRM, Point of Sales, WEB, etc.</p><ul><li><p>Trusted TRD (also known as silver): in this zone, you'll find the output of your data pipelines. Here, data is deduplicated, cleaned, transformed, and aggregated for your target use cases. Data is organized logically by data domain (Marketing, Finance, Sales, Operations…) and use case (Reporting, API exposition, AI training…).</p></li><li><p>Refined RFD (also known as gold): this is the final zone for your data. It is the exposition layer for your use cases and is organized logically by use case.</p></li></ul><p>These four zones are a representation of your data's journey through the datalake. From the refined zone, a serving layer can be added, taking shape in data marts, APIs, file exposition, etc. Your apps and users can then fetch or be sent data through this serving layer and make decisions that will give your data value.</p><p><img loading="lazy" src="/assets/images/data_journey_through_datalake_full-61f1e356cfda2c8d3c455e00f78f1c00.png" width="2639" height="940" class="img_ev3q"></p><div align="center"> Data’s journey through a datalake, from source to usage</div><br><p>Your data’s journey from sources to serving layers is managed by pipelines, which are themselves managed by orchestrators. Pipelines describe how data is moved and transformed, be it with highly customized code or low code interfaces: in terms of concept, pipelines are close to ELTs (Extract, Load, Transform) mechanisms. Orchestrators are an ensemble of pipelines, defining parameters for each of them and specifying when and how they should be run: periodically, when an event triggers, using larger or smaller computing power, etc.</p><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_S0QG"><p>A sandbox storage zone and computing environment can be made available with access to some of your storage zones. That sandbox allows for exploration of data by data scientists and data analysts, motivating new use cases.</p></div></div><p>Based on this architecture, it all seems too simple: find a high value use case to start off the building process, then add new use cases and sources later. Following the design principles in the next section, it could in fact be that simple, all the while avoiding falling into the data swamp and empty shell traps.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="usable-and-useful-through-architecture-and-design">Usable and Useful through architecture and design<a href="#usable-and-useful-through-architecture-and-design" class="hash-link" aria-label="Direct link to Usable and Useful through architecture and design" title="Direct link to Usable and Useful through architecture and design">​</a></h2><p>We’ve gathered some best practices and design principles that work well to counteract the possibility of your datalake becoming messy or unused. We like to think of each of these best practices as serving a goal that is threefold: building a datalake that is <strong>usable, useful, and used</strong> - what we refer to as our three “U”s. The first “U” is targeted towards the data swamp trap, the latter two towards the empty shell trap.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="usable-through-clear-storage-and-pipelines-architecture">Usable through clear storage and pipelines architecture<a href="#usable-through-clear-storage-and-pipelines-architecture" class="hash-link" aria-label="Direct link to Usable through clear storage and pipelines architecture" title="Direct link to Usable through clear storage and pipelines architecture">​</a></h3><p>The first step for building a usable datalake is to <strong>make your storage and pipelines easy to comprehend</strong> for users as well as automated processes. Technical and functional separation for pipelines and storage is your first step in making your datalake understandable.</p><ul><li><strong>Make it clear who does what in the datalake</strong>. A use case is built on top of data, which needs to be collected: we’ve found it best to separate data collection from its transformation from the start. Using at least different pipelines, and, better still, different scheduler resources will make understanding who (or which pipeline) is doing what much easier. One orchestrator for data collection pipelines, and other orchestrators for each data domain and use case theme – e.g. finance reporting, marketing CRM, sales exposition... This also sets up your datalake’s functional architecture in a “data mesh”-like manner, where <strong>each data domain has its own identifiable perimeter</strong>. </li></ul><p><img loading="lazy" src="/assets/images/pipeline_separation_full-9d5a8850e22af998c0407c312fef1eb7.png" width="2991" height="1207" class="img_ev3q"></p><div align="center"> Separating pipelines among data domain orchestrators allows for a better understanding of who does what</div><br><ul><li><p><strong>Make it obvious where to find data in your storage</strong>. We’ve talked a little about Landing, Raw, Trusted and Refined data storage layers. Separating your data by source in the Raw/Bronze layer will help your users and transformation pipelines identify each dataset. Separating transformed data by use case in the Trusted and Refined layers will give a better understanding of the functional architecture behind the storage.</p></li><li><p><strong>Make it apparent which transformation or collection pipelines is responsible for which data</strong>.</p><ul><li><p>Use identifiers such as date and time “YYYYMMDD_HHMM” for your pipelines and processes. Date and time is a great identifier for <strong>understanding when data was transformed</strong> or collected. Sticking to them for your logging as well, you can make sure that monitoring your pipelines and exploring their results is as easy as possible.</p></li><li><p>Inside your transformation pipelines, we’ve found that using break points is highly useful. Writing a dataset’s state after it’s been cleaned, pre-transformed, etc… up until its final refined state will make debugging and understanding your pipelines much easier. Using names like <em>0_deduplicated</em>, <em>1_cleaned</em>, <em>2_calculated</em> to show breakpoints in your transformations allow for a better understanding of the data lineage of each use case, also helping exploration in the sandbox.</p></li></ul></li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="usable-through-data-validation">Usable through data validation<a href="#usable-through-data-validation" class="hash-link" aria-label="Direct link to Usable through data validation" title="Direct link to Usable through data validation">​</a></h3><p>Now that your storage and pipelines architecture is understandable, your second goal is to <strong>make your data usable</strong> so that the datalake doesn’t immediately become a swamp with faulty and unchecked data.</p><p>One mistake to avoid is to consider that since a datalake can store any type of data, all data is good to collect, and you can figure out if it’s usable and useful later. On the contrary: validating data from the get-go is essential. By having your data collection pipelines look at the structure and content of the data, you can make sure that your transformation pipelines will have a smooth time reading it down the line.</p><p>However, this does not mean that you instantly delete any data that is deemed unfit for your use cases. We’ve found that dispatching data into <strong>rejected and validated subzones</strong> in the RAW/bronze zone is a good option:</p><ul><li><p>A rejected subzone for any <strong>data that will not technically fit</strong> your use case pipelines: faulty files with bad lines, wrong column count for structured data, corrupt zip files, bad encoding, the list goes on. Deciding which data to move to the rejected subzone ultimately depends on how it will be read by later pipelines.</p></li><li><p>We’ve also seen cases where data needs to be <strong>rejected for functional reasons</strong>: data from a source can be sent to you even though you’re not allowed to use it, or the data is technically readable but does not match with the sources you accept. In this instance, you can divide your rejected subzone into technical rejects and functional rejects.</p></li><li><p>Finally, any data that is technically and functionally usable can transit to the validated subzone. When a use case pipeline reads data, it will usually only read that data’s validated zone. This not only ensures that your pipelines are robust and will not fail due to faulty or unexpected data, but it also allows for a stable data model to be built.</p></li></ul><p><img loading="lazy" src="/assets/images/technical_functional_data_validation_full-7ef228be08cc7bf082afc04a0a556328.png" width="2580" height="896" class="img_ev3q"></p><div align="center"> Technical and functional data validation guarantee a first level of data quality</div><br><p>Why keep rejected data, you’ll ask?</p><p>Data that is technically rejected is useful for several reasons, the first one being that it is a good indicator of your sources' quality, and allows you to communicate efficiently with your data providers. Secondly, technically invalid data at one point in time may be usable in the future, be it by adjusting your later pipelines to be able to read them or simply correcting the data as a one-shot fix.</p><p>Data that is functionally rejected is also useful, particularly in cases where it’s from a source you’re not sure you’re allowed to collect. An additional subzone can then be a quarantine zone, where you would store data for which you need to check whether you can accept it in your use cases.</p><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_S0QG"><p>By managing functional and technical rejections, your collection pipelines can be a powerful tool for your Data Steward and apply robust and monitorable governance at the datalake’s entry point.</p></div></div><p><img loading="lazy" src="/assets/images/datasources_to_breakpoint_full-86c1df7c11dfea49df5efdae49e0bb48.png" width="3155" height="1212" class="img_ev3q"></p><div align="center"> From validated sources to a specific breakpoint in a run ID of a Use Case in a domain</div><br><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="useful-through-a-business-driven-design">Useful through a business driven design<a href="#useful-through-a-business-driven-design" class="hash-link" aria-label="Direct link to Useful through a business driven design" title="Direct link to Useful through a business driven design">​</a></h3><p>What we mean by “useful” - the second of our three “U”s, is that your datalake has to <strong>serve a business use, while not limiting itself</strong> to the first identified use cases. This in turn means several things.</p><p>The first thing being the importance of starting from a business use case to then identify which data must be collected and transformed. Working from the business needs is always the winning strategy, especially for data science and datalake projects where it is easy to get lost in all the possibilities. Collecting data into your datalake is not an effortless task, especially following the aforementioned tips that help validate the data’s usability, functionally and technically. Which is why the first step is to make sure that the data you collect is indeed useful to your target use case. Our article <a href="https://ekimetrics.com/article-insights/customer-data-platform/" target="_blank" rel="noopener noreferrer">“Customer Data Platform: Thinking Backwards is the Way to Go”</a> summarizes this business driven approach well.</p><p>Now, in order to make the most of that data and not limit a source to the one and only use case, your collection pipeline should only be considering it as that: a source. This means that it is not the role of your collection pipeline to determine which data will be useful and remove all other data: delta calculation, replacing referentials, etc. is not up to the collection, but to the use case pipelines. Making the most of a datalake’s scalability in storage and letting your use cases define what is useful to them in their own pipelines, you’ll allow for more opportunities to use the underlying source down the line.</p><p>One last element to make your datalake useful is to anticipate changing functional parameters. Historical depth for your transformations, functional validation for your sources, ways to trigger custom runs… If modifying these parameters has a business or data management use, they should be included in the first iteration of your pipeline developments. In our projects, they have allowed us to be extremely flexible if a pipeline needs functional change and focus on adding new use cases and sources rather than tweaking the same pipeline over and over.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="used-through-scalability-of-business-impact">Used through scalability of business impact<a href="#used-through-scalability-of-business-impact" class="hash-link" aria-label="Direct link to Used through scalability of business impact" title="Direct link to Used through scalability of business impact">​</a></h2><p>Start small, allow it to grow : identify a single use case to automate and build your foundation architecture to host it. Using the best practices presented above you’ll then be able to build on top of this robust architecture.</p><p>The target for your datalake’s first use case should be for it to be <strong>high impact, low time to value</strong>. It must be the proof of value your businesses divisions need to invest more in the platform and should be an inspiration for other use cases. Once that use case is identified, along with the data it needs and a design for its transformation pipelines and exposition, you can start building it following the design principles presented above.</p><p>The steps below are a summary of how that first use case is built, but are also highly reproducible to build new use cases once your foundation architecture is in place.</p><ul><li><p>Ingest a new source</p><ul><li><p>Technical validation: what are the rules that define whether this data is usable? Set up data structure validation and organize the source in an understandable manner.</p></li><li><p>If the data source is highly likely to evolve over time, make sure that the collection and technical validation pipeline for this source is evolutive as well to anticipate future changes.</p></li></ul></li><li><p>Build a new use case</p><ul><li><p>Functional validation: for this use case, what are the rules that define whether this data is useful? Once again, it is best to leave functional validation to your use case pipelines, as different use cases will have different requirements for what data they want (useful) and what they can legally use (usable).</p></li><li><p>Same goes for how the data is used: delta calculation, targeting referentials, cleaning, etc… these steps should be part of your use case pipeline.</p></li><li><p>Build transformations with break points inside the trusted data storage zone, up to the exposition layer (refined zone / gold), where it is fully ready to serve use cases. Which of these transformations is likely to change, and can you anticipate these changes by using simple configurable parameters?</p></li></ul></li></ul><p><img loading="lazy" src="/assets/images/data_storage_architecture_full-c50a8426b67694b08480e3ed0fad3a52.png" width="2588" height="1300" class="img_ev3q"></p><div align="center"> This data storage architecture makes it easy to add new sources and use cases</div><br><p>These first two steps will show your datalake’s versatility. It is able to ingest any type of data, from unstructured to structured, streaming or batch. It is also able to transform it efficiently to feed use cases, due to the scalability of its computing power.</p><ul><li><p>Build a new serving layer</p><ul><li><p>This final step will vary the most between use cases, as depending on your user endpoint, there might not be a unified way to serve refined data.</p></li><li><p>Here you may need additional resources on top of your foundation architecture: an API management service, a database to efficiently serve structured data, a web service or container to display interfaces…</p></li></ul></li></ul><p>That’s it, your use case is fully functional on your datalake. Using the best practices we’ve presented, the use case is robust, evolutive, scalable, understandable, and useful.</p><p>Once the foundation architecture is laid out and more use cases and sources are added, an argument can be made for a <strong>core data model</strong>. This model would be built by a single transformation pipeline that unifies deduplication / delta calculation, data cleaning, GDPR compliance transformations and preparation of highly used KPIs for your use cases. The core transformation should be considered as a use case pipeline: its job is to prepare data, saving on computing resources and providing a single source of truth for other use cases. It is also a great way to ease data exploration for data savvy users in the sandbox, allowing them to be inspired through use case agnostic and clean datasets. To maintain relevance, the core data model should be updated with new data sources as they are added, and new use cases using that new source should feed on the cleaned and prepared datasets.</p><p><img loading="lazy" src="/assets/images/core_data_model_full-9eee3df156f98cfedb2204804e73ba06.png" width="2774" height="932" class="img_ev3q"></p><div align="center"> A Core data model centralizes all cleaning and pre-calculations, further simplifying the storage architecture</div><br><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><p>Starting with the identification of a first high impact use case, you’ve been able to build a robust and evolutive datalake. Following the design principles we’ve reviewed, data and use case pipelines in your datalake are usable and useful. You are now able to show a proof of value for your datalake and grow it into a central data platform for your company.</p><p>The final U we could dive deeper into is “Used” : your goal is now to maintain, grow and manage the datalake as a unified data platform, centralizing data and solutions for your business. An efficient and relevant datalake project comes with technical design challenges, some of which we’ve talked about here, but also organizational challenges. By finding solutions for technical challenges, your datalake has shown its value and it is now essential to ensure its growth. As you coordinate with business divisions, you’ll be able to draw a roadmap for the datalake and improve processes at an organizational level, using “data mesh”-like functional architectures that allow for the best flexibility in use cases, growing your platform’s relevance and enhancing data capabilities for your business.</p></div>]]></content:encoded>
            <category>Datalake</category>
            <category>Data Engineering</category>
            <category>Architecture</category>
            <category>Data Governance</category>
            <category>Data Mesh</category>
        </item>
        <item>
            <title><![CDATA[Which AutoML platform to choose to start your Data Science project?]]></title>
            <link>https://ekimetrics.github.io/blog/2022/01/27/automl_benchmark</link>
            <guid>https://ekimetrics.github.io/blog/2022/01/27/automl_benchmark</guid>
            <pubDate>Thu, 27 Jan 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[How do Automatic Machine Learning solutions work? What are the most popular AutoML solutions out there, and what do you need to know about them to make the right choice for your use case? Find answers to these questions and many others in this article.]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/Nyhavn_3-ca6dc7204dc3f387c23b5dc549a6f9fb.jpg" width="1470" height="980" class="img_ev3q"></p></div><div align="justify"><p>Applying traditional Machine Learning methods to real-word business problems can be time-consuming, resource-intensive and expensive. With Automated Machine Learning (Auto ML) however, it can take days at most for business professionals &amp; Data Scientists alike to develop and compare dozens of models, find insights and solve business problems quickly. But what is AutoML, how does it work and what are the most popular AutoML solutions out there? In this article, we will introduce the field of Automated Machine Learning by exploring some popular AutoML frameworks and trying to answer those questions around how to make the right choice for your use case.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-is-automl-and-why-it-is-interesting">What is AutoML and why it is interesting?<a href="#what-is-automl-and-why-it-is-interesting" class="hash-link" aria-label="Direct link to What is AutoML and why it is interesting?" title="Direct link to What is AutoML and why it is interesting?">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="definition">Definition<a href="#definition" class="hash-link" aria-label="Direct link to Definition" title="Direct link to Definition">​</a></h3><p>Sebastian Raschka, a well regarded American statistics professor, states: ‘’Computer programming is about automation, and Machine Learning is all about automating automation’’. If that’s true then we can say that Automated Machine Learning is the automation of automating automation..! AutoML is a new optimization technique which aims at automating some of the core - but highly iterative - parts of the traditional modelling process in ML, in particular feature selection and model selection (including hyperparameter tuning and stacking). Let’s take the example of a Decision Tree Algorithm. This algorithm has many hyperparameters (leaf, depth, split etc.) and browsing through all those hyperparameters can take days. An AutoML algorithm can intelligently explore all the possible parameters in order to find the best possible ML algorithm solving a given task. Some advanced AutoML algorithms can even choose between different features, create new ones and try different data cleansing scenarios!</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-automl-can-help-us">How AutoML can help us?<a href="#how-automl-can-help-us" class="hash-link" aria-label="Direct link to How AutoML can help us?" title="Direct link to How AutoML can help us?">​</a></h3><p><img loading="lazy" src="/assets/images/Steps_DS_automl_3-88f832e3508f3fd93be110edc0c4cf6e.png" width="1977" height="510" class="img_ev3q"></p><p>There are many steps in a Machine Learning project, with Model Training typically being one of the most time consuming. A standard project will focus on a Business Problem and will pass through Data Collection, Cleaning and Processing before the training phase. Model Training can then be cumbersome as we have to select an appropriate model family and to fine tune many times differents hyperparameters before building the analysis and deploying the model. This process can take weeks or months, but it seems that several steps can be automated using optimization algorithms and that is the goal of auto ML.</p><p>In a sense, AutoML is seen as a way to increase productivity, to allow the Data Scientist to focus more on defining the problem and the analyses rather than the models finetuning, to help avoiding errors, and on top of that, to democratize Machine Learning so that everyone can leverage its power. Its integration within ML Engineering environment allows to streamline pipeline and integrate specific training jobs, that allows to parametrize a model search completely from a simple configuration file.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="overview-of-the-main-automl-frameworks-in-the-market">Overview of the main AutoML frameworks in the market<a href="#overview-of-the-main-automl-frameworks-in-the-market" class="hash-link" aria-label="Direct link to Overview of the main AutoML frameworks in the market" title="Direct link to Overview of the main AutoML frameworks in the market">​</a></h3><p>For this article, we have selected 7 well known AutoML frameworks, but many more exist in the market. Below is an overview of some of the types of Data Science problems these AutoML solutions can address. Among them two are open source: AutoKeras &amp; H2o and the others are not: with DataRobot, Dataiku, Azure, GCP, and AWS SageMaker. (2 other open-source solutions AutoKeras and AutoGluon were studied, but these frameworks are not available on Windows.)</p><p><img loading="lazy" src="/assets/images/automl_platforms_3-ab585502e47d2a9e8e976a028edeb3dc.png" width="1236" height="539" class="img_ev3q"></p><p>Many of the AutoML solutions address different types of Data Science problems such as Sentiment Analysis or Computer Vision, however what is going to interest us for the rest of is article are the Classification and Regression ML problems.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="exploring-the-capabilities-of-several-automl-solutions">Exploring the capabilities of several AutoML solutions<a href="#exploring-the-capabilities-of-several-automl-solutions" class="hash-link" aria-label="Direct link to Exploring the capabilities of several AutoML solutions" title="Direct link to Exploring the capabilities of several AutoML solutions">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="our-methodology-classificationregression-datasets-kaggle">Our methodology (Classification/Regression, datasets Kaggle)<a href="#our-methodology-classificationregression-datasets-kaggle" class="hash-link" aria-label="Direct link to Our methodology (Classification/Regression, datasets Kaggle)" title="Direct link to Our methodology (Classification/Regression, datasets Kaggle)">​</a></h3><p>The purpose here is to bring some knowledge in the field of <strong>Automated Machine Learning</strong> by <strong>exploring some popular AutoML frameworks</strong>. In order to establish a performance benchmark, we decided to test those frameworks on <strong>common ML topics</strong> like <strong>Regression</strong> and <strong>Classification</strong>. The idea is also to share our personal experience (and difficulties encountered) with those tools so that everyone can have an overview of each one these AutoML solutions.</p><p>These packages were benchmarked on two classical datasets available on Kaggle. We chose the <strong>Titanic Challenge</strong> (<a href="https://www.kaggle.com/c/titanic/overview" target="_blank" rel="noopener noreferrer">Titanic - Machine Learning from Disaster | Kaggle</a>) for the Classification problem and the <strong>House Prices Challenge</strong> (<a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/rules" target="_blank" rel="noopener noreferrer">House Prices - Advanced Regression Techniques | Kaggle</a>) for the Regression problem. For each framework, we submitted predictions on the test set on Kaggle so that we can compare the results to the general leaderboard. What is interesting with this approach is that we were able to compare performance not only between each AutoML solution but also with individual performance.</p><p><img loading="lazy" src="/assets/images/methodology_automl_3-cd9ec5d23c010842c461dbb9b35ef5df.png" width="1463" height="556" class="img_ev3q"></p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="our-results">Our results<a href="#our-results" class="hash-link" aria-label="Direct link to Our results" title="Direct link to Our results">​</a></h3><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="classification-problem---titanic-challenge">Classification Problem - Titanic Challenge<a href="#classification-problem---titanic-challenge" class="hash-link" aria-label="Direct link to Classification Problem - Titanic Challenge" title="Direct link to Classification Problem - Titanic Challenge">​</a></h4><p><img loading="lazy" src="/assets/images/titanic_challenge_details_3-61787d540f40955c9486851fb33d4f0d.png" width="1416" height="606" class="img_ev3q"></p><p>The Titanic Challenge is one of the most famous on Kaggle with more than 30,000 challengers. The objective is to have a model which will predict if a Titanic passenger survives or not on a small data set with 9 features and around 900 rows in the training set. The metric used for this challenge is the accuracy and the leaderboard we have downloaded from Kaggle is provided below.</p><p><img loading="lazy" src="/assets/images/benchmark_titanic_challenge_3-bbe560d7ac7ae1ff3223505f40746f63.png" width="1273" height="745" class="img_ev3q"></p><p>Each point represents a challenger with the performance of all tested frameworks noted, with ranking &amp; accuracy plotted on the x and y axis. We found that Azure has the best accuracy followed by H2O and that the accuracy for all frameworks ranges within 73% - 80%.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="regression-problem---house-prices-challenge">Regression Problem - House Prices Challenge<a href="#regression-problem---house-prices-challenge" class="hash-link" aria-label="Direct link to Regression Problem - House Prices Challenge" title="Direct link to Regression Problem - House Prices Challenge">​</a></h4><p><img loading="lazy" src="/assets/images/house_price_challenge_details_3-31c76b327cffe802e74a7ba81fdc17df.png" width="1575" height="566" class="img_ev3q"></p><p>For the House Prices Challenge, the goal is to predict the final price of residential homes in the United States. This time the metric is the root mean squared error, which is a metric that tells us the average distance between the predicted values from the model and the actual values in the dataset. We have around 8000 challengers (less challengers than the Titanic Challenge) and the dataset here is bigger with more than 250 features and around 1400 rows in the training set.</p><p><img loading="lazy" src="/assets/images/benchmark_houseprice_challenge_3-f91493ad4f705725bc41e4d4d46685a7.png" width="1249" height="739" class="img_ev3q"></p><p>We can see that all frameworks did better than the baseline RMSE. Each of the tested models fall between 0.24 - 0.12 in terms of the RSME and H2O again performs the best among other tested frameworks.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="which-automl-solutions-to-choose">Which AutoML solutions to choose?<a href="#which-automl-solutions-to-choose" class="hash-link" aria-label="Direct link to Which AutoML solutions to choose?" title="Direct link to Which AutoML solutions to choose?">​</a></h3><p>In conclusion, which AutoML to choose? When making our decision, the framework performance is important, but not all frameworks were in fact easy to use. Conversely, we spent a lot of time on some frameworks, because some of them were not as intuitive as expected.</p><p>User friendliness must also therefore be a major criterion to consider when you are wondering which AutoML to use. That’s why, in order to consider the frameworks performance on both the Kaggle challenge but also the user friendliness of each solution (if it is easy to install it, to get familiar with the tool, to set up the experiment without errors), we built two further metrics to evaluate each AutoML framework and to compare them.</p><ul><li><p>On the x-axis: we give the ease of use for each AutoML framework from 1 to 10 (10 being the best)</p></li><li><p>On the y-axis: we took the average ranking on the both Kaggle challenge (the closer to 0, the better)</p></li></ul><p><img loading="lazy" src="/assets/images/final_benchmark_3-adf3db6d8095c9e8d6559367855e55e2.png" width="1276" height="743" class="img_ev3q"></p><p>The metric 'ease of use' is somewhat subjective as it refers to our own personal experience. However, we would add that one of the goals of AutoML is to make Machine Learning accessible to everyone, so for someone with very little knowledge, it is interesting to see how accessible the differents tools are.</p><p>We found that:</p><ul><li><p>The cloud solutions are more difficult to master especially if you’ve never used cloud computing before. In fact, some time is needed to get familiar with all the different functionalities of cloud solutions like Azure, AWS or GCP as there are powerful and comprehensive tools.</p></li><li><p>Dataiku and DataRobot are very easy to use and very visual, with many icons. These interfaces are very clear and simple.  If you want to use an extremely easy-to-use AutoML tool, which can perform automated machine learning very quickly, DataRobot and Dataiku seem to be good solutions.</p></li><li><p>H2O works as a notebook so if you are familiar working on a Jupyter notebook for example, it can definitely be a good option and not too painful to use. H2O seems to be a very good choice in terms of performance and ease of use. Moreover, this framework has the advantage of being completely open source.</p></li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="final-thoughts-our-key-takeaways">Final thoughts, our key takeaways<a href="#final-thoughts-our-key-takeaways" class="hash-link" aria-label="Direct link to Final thoughts, our key takeaways" title="Direct link to Final thoughts, our key takeaways">​</a></h3><p>This article tries to compare 7 majors AutoML frameworks. To do so, we introduced a methodology that considers the performance of each solution, but also their ease of use. What appears clearl is that AutoML can definitely be a good starting point to an ML project. In fact, for both the Classification and Regression problem, all the frameworks performed almost as well, even better sometimes than the baseline, just by using the AutoML functionality.</p><p>However, it is important to remember that automatically generated pipelines are still very basic and are not able to beat human experts yet. As we could see on the different benchmarks from Kaggle, Data Scientists still perform better. A Data Scientist analyzes the hidden information inside data, extracts useful correlations, gives useful insights about the business that has created data itself and all these tasks cannot be fully automated.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">​</a></h2><p>To read more about AutoML:</p><p><a href="https://arxiv.org/pdf/1908.00709.pdf" target="_blank" rel="noopener noreferrer">2019 | AutoML: A Survey of the State-of-the-Art | Xin He, et al.</a> </p><p><a href="https://arxiv.org/pdf/1904.12054.pdf" target="_blank" rel="noopener noreferrer">2019 | Survey on Automated Machine Learning | Marc Zoeller, Marco F. Huber</a> </p><p><a href="https://arxiv.org/pdf/2008.08516.pdf" target="_blank" rel="noopener noreferrer">2020 | Automated Machine Learning--a brief review at the end of the early years | Escalante, H. J. 2008.08516.pdf</a></p></div>]]></content:encoded>
            <category>AutoML</category>
            <category>Benchmark</category>
            <category>Automatic Machine Learning</category>
        </item>
        <item>
            <title><![CDATA[Hackathon Stories - Ensuring access to affordable and clean energy]]></title>
            <link>https://ekimetrics.github.io/blog/2022/01/06/hackathon</link>
            <guid>https://ekimetrics.github.io/blog/2022/01/06/hackathon</guid>
            <pubDate>Thu, 06 Jan 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[Read this article to see how and why Ekimetrics organize hackathons to foster creativity, innovation and team work in our Data teams. Also learn important tips for organizing your own hackathons.]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/Solar_2-1cccadf61fbb3774931d84adb1d7e64b.jpg" width="1744" height="1161" class="img_ev3q"></p></div><p>At Ekimetrics, we organize internal hackathons several times a year to continue to improve our data science skills, get people working together and develop new solutions to difficult problems.</p><p>In July 2021, we organized a new hackathon on the issue of access to clean energy for African populations, inspired by a competition organized by <a href="https://zindi.africa/competitions" target="_blank" rel="noopener noreferrer">Zindi</a> and open to everyone (you can participate at this <a href="https://zindi.africa/competitions/sfc-paygo-solar-credit-repayment-competition" target="_blank" rel="noopener noreferrer">link</a>).</p><p>In this article, we wanted to come back on the reasons why we organize hackathons, our tips for organizing them in your data teams, some insights on that particular competition and some code snippets behind the most interesting achievements of the participants.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="ensuring-access-to-affordable-and-clean-energy">Ensuring access to affordable and clean energy<a href="#ensuring-access-to-affordable-and-clean-energy" class="hash-link" aria-label="Direct link to Ensuring access to affordable and clean energy" title="Direct link to Ensuring access to affordable and clean energy">​</a></h2><p>The name <a href="https://sdgs.un.org/goals" target="_blank" rel="noopener noreferrer">Sustainable Development Goals</a> (or SDGs) is commonly used to refer to the seventeen goals established by the member states of the United Nations which are gathered in the 2030 Agenda. </p><div align="center"><p><img loading="lazy" src="/assets/images/sdgs-ed567aba7bcd06770e172e42a606f881.png" width="728" height="451" class="img_ev3q"></p></div><p>The goal 7 - “Ensure access to reliable, sustainable and modern energy services at an affordable cost for all" - is a particularly important issue for the African continent, where 596 million people do not have access to electricity.
Most of these people live outside of urban centers, and therefore out of reach of the continent’s electricity grid. Some existing systems also struggle to supply enough energy to the homes and businesses that are on the grid. </p><p>It is estimated that 592 million people in Africa are living without access to electricity. Most of these people live outside of urban centers and therefore out of reach of the continent’s electricity grid. Furthermore, the existing systems in many African countries even struggle to supply enough energy to the homes and businesses that are on the grid. </p><p>Pay-as-you-go (PAYGo) solar technology has become Africa’s most promising approach to handling the continent’s growing energy problems. PAYGo users pay a small downpayment for a solar kit that provides up to eight hours of emission-free lighting every day, as well as enough energy to charge mobile phones and other devices. </p><p>With PayGo solar, residents are able to reduce their energy spending by up to 50%.
The objective of this challenge is to help predict the next six months of payments for different customers. This will allow PAYGo distributors to provide appropriate services and customer support, ensuring that they can continue to provide these important devices affordably and efficiently to the benefit of people all over Africa. </p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="why-do-we-organize-hackathons">Why do we organize hackathons?<a href="#why-do-we-organize-hackathons" class="hash-link" aria-label="Direct link to Why do we organize hackathons?" title="Direct link to Why do we organize hackathons?">​</a></h2><p>After a long period of COVID19 marked by an almost generalized working from home, we wanted to allow our teams to meet in real life with the coworkers they only saw in back-to-back Teams calls, and work together on an important cause.  </p><p>We love hackathons at Ekimetrics as they enable: </p><ul><li>To federate teams;</li><li>To make people that don't usually work together collaborate to drive innovation;</li><li>To get some practical experience at tackling new problems far from our usual business topics.</li></ul><p>That is why we organize two big hackathons every year and bi-monthly coding challenge sessions, some of which you can find in open source on our <a href="https://ekimetrics.github.io/hacks/" target="_blank" rel="noopener noreferrer">hackathons homepage</a>.
For this event, we were lucky enough to be able to carry it out in our office while respecting all sanitary recommendations.</p><p><img loading="lazy" src="/assets/images/Team-b2f6672278d4a9ac6b03855e3d5a74fe.png" width="1357" height="1017" class="img_ev3q"></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-to-organize-a-hackathon-for-your-data-teams">How to organize a hackathon for your data teams?<a href="#how-to-organize-a-hackathon-for-your-data-teams" class="hash-link" aria-label="Direct link to How to organize a hackathon for your data teams?" title="Direct link to How to organize a hackathon for your data teams?">​</a></h2><p>Having organized a dozen hackathons internally and for our clients, and participated in many of them, we have consolidated our learnings into a set of best practices which we are happy to share with you to help you organize your own hackathon for your data teams:</p><ul><li><p><strong>Finding a platform</strong> - There are also platforms integrating many public hackathons like <a href="https://www.kaggle.com/" target="_blank" rel="noopener noreferrer">Kaggle</a> or <a href="https://zindi.africa/" target="_blank" rel="noopener noreferrer">Zindi</a> mentioned above. This time, we chose to contribute to an existing hackathon - see section below about developing a hackathon platform. </p></li><li><p><strong>Choosing a meaningful topic</strong> - Get involved in a project or a project that matters to you and your colleagues. It's important to work on a different topic than your daily work, but you need to have passion. </p></li><li><p><strong>Preparing the minimum starting pack</strong> - Focus on the answer to the problem and spend time preparing notebooks and training tools to facilitate the handling of the subject by the teams and allow them to increase their skills on new technologies. Maybe also find a few data points and some interesting resources and references to.</p></li><li><p><strong>Making balanced teams</strong> - It is essential to balance the teams taking into account their varying levels of expertise and seniority;</p></li><li><p><strong>Timing, Timing, Timing</strong> - ⏱ It is necessary to pace the flow of the event, to allow participants to get into the topic and also to propose dedicated sessions allowing for discussions with the organizers and help throughout the event;</p></li><li><p><strong>Setting the adequate duration</strong> - 🏁 Ideally, hackathons last between 6 and 8 hours over an evening and part of the night in order to condense the challenge as much as possible while allowing the teams to take control of the issues at stake - to 2 days to have time to build a demo.</p></li><li><p><strong>Proposing a baseline</strong> - 💻 Depending on the topic it can be very useful to propose one or more examples of models allowing to build a base line from which to iterate;</p></li><li><p><strong>Communication</strong> - 💬 It is necessary to provide a common discussion thread for all participants as well as private channels by teams;</p></li><li><p><strong>Fun</strong> - 🍾 Of course pizzas, beers, and other refreshments to keep a good energy level and especially a nice atmosphere throughout the competition;</p></li><li><p><strong>Letting the stage</strong> - 😎 Finally, to showcase the night’s work, we always plan a pitch session the next day to present the projects in front of Ekimetrics partners and founders.</p></li><li><p><strong>Rewards &amp; gifts</strong> - 🎁 A nice option is also to reserve gifts for the winners if you can decide of a winner </p></li></ul><p><img loading="lazy" src="/assets/images/Image32-05c05ede3312e15664307f882b60e5cf.png" width="1438" height="1080" class="img_ev3q"></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="do-you-need-a-hackathon-platform">Do you need a hackathon platform?<a href="#do-you-need-a-hackathon-platform" class="hash-link" aria-label="Direct link to Do you need a hackathon platform?" title="Direct link to Do you need a hackathon platform?">​</a></h2><p>If it is not an open innovation hackathon, and depending on the topic and how the competition works, it can be interesting to have a main hackathon platform for participants to upload their results, test their solutions and compare with other teams. In this case there are multiple solutions to consider : </p><ul><li>Using existing platform like <a href="https://zindi.africa/" target="_blank" rel="noopener noreferrer">Zindi Africa</a>, <a href="https://www.kaggle.com/" target="_blank" rel="noopener noreferrer">Kaggle</a> or <a href="https://www.drivendata.org/" target="_blank" rel="noopener noreferrer">Driven Data</a> </li><li>Build your platform adapted to your needs, in particular now you can easily use <a href="https://streamlit.io/" target="_blank" rel="noopener noreferrer">Streamlit</a> to build your own. See some examples below;</li><li>Not using a platform, avoid building a new one if you can focus on human relations, and the platform does not add value to the competition or the innovation </li></ul><p>For our different <a href="https://ekimetrics.github.io/hacks/" target="_blank" rel="noopener noreferrer">hackathons</a>, we developed the platforms we needed : </p><h5 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="our-main-platform-for-custom-hackathons-where-we-can-compute-a-score-between-teams">Our main platform for custom hackathons where we can compute a score between teams<a href="#our-main-platform-for-custom-hackathons-where-we-can-compute-a-score-between-teams" class="hash-link" aria-label="Direct link to Our main platform for custom hackathons where we can compute a score between teams" title="Direct link to Our main platform for custom hackathons where we can compute a score between teams">​</a></h5><p><img loading="lazy" src="/assets/images/platform-5b816453b878f149f9da9ec8c63d9a8e.png" width="1889" height="759" class="img_ev3q"></p><h5 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-platform-we-developed-on-streamlit-for-our-data-science-escape-game">The platform we developed on Streamlit for our Data Science Escape Game<a href="#the-platform-we-developed-on-streamlit-for-our-data-science-escape-game" class="hash-link" aria-label="Direct link to The platform we developed on Streamlit for our Data Science Escape Game" title="Direct link to The platform we developed on Streamlit for our Data Science Escape Game">​</a></h5><p><img loading="lazy" src="/assets/images/platform2-d637e027dd4fc955368fc603216e310c.png" width="1119" height="590" class="img_ev3q"></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-is-our-hackathon-philosophy-at-ekimetrics">What is our hackathon philosophy at Ekimetrics?<a href="#what-is-our-hackathon-philosophy-at-ekimetrics" class="hash-link" aria-label="Direct link to What is our hackathon philosophy at Ekimetrics?" title="Direct link to What is our hackathon philosophy at Ekimetrics?">​</a></h2><p>Our goal is not to become "Kaggle masters" to reach the highest score.
We are always keen to put forward the double hat (technical and business) of Ekimetrics consultants in order to give meaning to the data.
That's why during this challenge we evaluate the teams around three dimensions:</p><ul><li><p>The performance of the models;</p></li><li><p>The quality of the data analysis, the insights, and the quality of the visualizations;</p></li><li><p>The overall strategy to tackle the problem, going back to the bigger picture - i.e. broader issuers such as sustainable development.</p></li></ul><p>Offering different types of sub-challenges within the same hackathon created great opportunities for <em>all</em> Eki employees (including members of the marketing, HR or finance teams) to integrate within a team, which helped create links between people who do not always meet on a daily basis - let alone work together.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="if-you-want-to-do-it-yourself">If you want to do it yourself<a href="#if-you-want-to-do-it-yourself" class="hash-link" aria-label="Direct link to If you want to do it yourself" title="Direct link to If you want to do it yourself">​</a></h2><p>This hackathon is open, do not hesitate to participate <a href="https://zindi.africa/competitions/sfc-paygo-solar-credit-repayment-competition" target="_blank" rel="noopener noreferrer">here</a>.</p><p><strong>Thanks to all the participants !</strong></p>]]></content:encoded>
            <category>Hackathon</category>
            <category>Teamwork</category>
            <category>Sustainable AI</category>
        </item>
        <item>
            <title><![CDATA[Customer Journey Clustering - How to use sequential modeling techniques to handle customer journey data]]></title>
            <link>https://ekimetrics.github.io/blog/2021/12/22/cjc</link>
            <guid>https://ekimetrics.github.io/blog/2021/12/22/cjc</guid>
            <pubDate>Wed, 22 Dec 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Enrich your understanding of customer journeys through clustering using  image embedding, autoencoders and unsupervised learning.]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/customer_journey-6dafdaf917880f96d89f1e7349911266.jfif" width="1121" height="793" class="img_ev3q"></p></div><div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="enriching-understanding-of-customer-journeys-through-clustering">Enriching understanding of customer journeys through clustering<a href="#enriching-understanding-of-customer-journeys-through-clustering" class="hash-link" aria-label="Direct link to Enriching understanding of customer journeys through clustering" title="Direct link to Enriching understanding of customer journeys through clustering">​</a></h2><p>Analyzing customer website behaviour is an essential step towards understanding the ways in which customers perceive and interact with a brand’s products and services. Methods for analyzing these online interactions have improved in sophistication and depth of insight offered in recent years meaning many traditional methods, such as journey mapping now fall short. Exploring Customer Journeys, whilst more complex, offers brands the opportunity to dive deeper and obtain more granular insights that can then be used to optimise and improve the experience of future customers.</p><p>Employing advanced methodologies is not only beneficial, but it has also become a necessary action for any business that wants to explore the increasingly complex customer journeys possible on modern websites. To provide some context to this challenge - when investigating this possibility for one of our clients, a global leader in the telecom industry, we quickly realized that given the number of possible actions on their website (about 1,760), performing standard analysis would be overwhelming. It would, in fact, raise the number of potential journeys to something of the order of 10<sup>4900</sup>. To put this number into perspective, the number of atoms in the universe is of the order of 10<sup>100</sup>. A simple method of analysing customer journeys is mapping. Essentially creating a visual representation of every touchpoint your customers have with you, creating a fluid timeline from how you initially engaged them to their first purchase and tracking the ongoing relationship.  </p><blockquote><p>… it would raise the number of possible journeys to something of the order of 10<sup>4900</sup>. To put this number into perspective, the number of atoms in the universe is of the order of 10<sup>100</sup>.</p></blockquote><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="advancing-methodologies---creating-groups-of-similar-journeys-through-image-embedding-autoencoders-and-unsupervised-learning">Advancing methodologies - Creating groups of similar journeys through image embedding, autoencoders and unsupervised learning<a href="#advancing-methodologies---creating-groups-of-similar-journeys-through-image-embedding-autoencoders-and-unsupervised-learning" class="hash-link" aria-label="Direct link to Advancing methodologies - Creating groups of similar journeys through image embedding, autoencoders and unsupervised learning" title="Direct link to Advancing methodologies - Creating groups of similar journeys through image embedding, autoencoders and unsupervised learning">​</a></h2><p>Many businesses are adapting strategic models to emphasise customer-centricity and build up their online presence. Websites, therefore, become a powerful asset in understanding customer behaviour through the data that they generate. And analyzing this data becomes a potential source of insight to improve future interactions with the customer base, driving loyalty and higher conversion. Journey mapping and other traditional methods can provide fast and high-level customer insight, which can be an important first step in making use of this asset. However, they often fall short as the numbers of potential journeys increase exponentially. </p><p>Journey clustering is a more advanced approach that uses algorithms to analyse all journeys by identifying groups of customers who interact with the website in similar ways. In doing so, we identify patterns in customer behaviour and begin to understand the motivations behind their actions.  This type of analysis can be used by businesses to personalise their websites and improve customer experience. Ekimetrics' innovative approach specifically uses advanced encoding and machine learning methods, as well as clustering algorithms to make sense of the complex digital world.</p><p>To address this challenge our approach requires three key steps: </p><ol><li><p>Reducing the dimensions of our customer journey dataset</p></li><li><p>Creating image-like representations of customer journeys; one image represents one customer journey</p></li><li><p>Applying clustering algorithms on images to create groups of users with similar behaviours. For each cluster, we are then able to define a “standard journey”.</p></li></ol><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="step-1-reducing-the-dimensions-of-our-dataset">Step 1: Reducing the dimensions of our dataset<a href="#step-1-reducing-the-dimensions-of-our-dataset" class="hash-link" aria-label="Direct link to Step 1: Reducing the dimensions of our dataset" title="Direct link to Step 1: Reducing the dimensions of our dataset">​</a></h3><p>Analyzing customer journeys is a difficult task that can lead to memory allocation errors, primarily due to high data diversity. Even for less sophisticated brands, the number of parameters feeding into a customer journey analysis model can quickly become unmanageable due to the sheer size of data needing to be analysed. </p><p>For example, the following features can be tracked: the URL of the pages visited, time spent on one page, overall duration on the website, user clicks, and many more.  The temptation is to add as much detail as possible, but more variables create more noise and lengthening computation time. </p><p>A solution to this problem is to identify the most relevant variables - with respect to the business question we want to answer - to take forward through feature selection, these metrics can then be aggregated into a single data structure. This process serves two purposes: firstly reducing the size and complexity of our inputs and ensuring the outputs are directed towards the business objectives and free of unnecessary noise.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="step-2-encoding-and-creating-images-to-represent-journeys">Step 2: Encoding and creating images to represent journeys<a href="#step-2-encoding-and-creating-images-to-represent-journeys" class="hash-link" aria-label="Direct link to Step 2: Encoding and creating images to represent journeys" title="Direct link to Step 2: Encoding and creating images to represent journeys">​</a></h3><p>Customer Journeys are path-dependent processes that describe customers’ interactions with different touchpoints, for example, all actions on a website leading up to purchase. To analyse these relationships we need to manipulate datasets to represent online journeys but also translate them into something that can be easily interpreted and modelled.</p><p>To do this and ensure no information is lost we create a 3D embedding ‘image’ that merges all this data into a single structure through the following processes:</p><ul><li><p>Touchpoints that a customer has interacted with are represented in columns</p></li><li><p>The order in which interactions happen is represented in rows</p></li><li><p>A vector describing the quality of the customer interaction (e.g. time spent/number of clicks…)</p></li></ul><p><img loading="lazy" src="/assets/images/Image_1-f51932e54a6f74dbb70d7de7c389ac0f.png" width="1177" height="485" class="img_ev3q"></p><div align="center"> Building our 3D embedding through an iterative process</div><br><p>The 3 parts for building our 3D embedding highlighted in the figure above are:</p><ul><li><p>Given n, the number of possible interactions on the platform, it is possible to build a vector V=(1,n) representing all actions that a customer can perform. At a particular stage t, it is possible to fill V with “0“ or “1” depending on the customer’s action: if the i<sup>th</sup> action has been performed at stage t, set V(1,i)=1 , otherwise V(1,i)=0</p></li><li><p>It is then possible to replicate this process at every stage of the customer’s journey in order to build a matrix M(l,n) where every line l represents the lth action performed by the customer</p></li><li><p>Finally, adding an indicator of the journey’s quality consists of replacing the 0 &amp; 1 in the previous matrix, with a vector of values describing the interactions.</p></li></ul><p><img loading="lazy" src="/assets/images/Image_2-163fab723ca3e6737cbf9a587ffb2a88.png" width="921" height="334" class="img_ev3q"></p><div align="center"> Example of an image decomposition through RGB channels</div><br><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_S0QG"><p>Colourized pictures can be decomposed into Red, Green and Blue monochromatic sub-pictures.
In our case, we can draw a parallel between the different indicators describing the interactions and the colourized levels of an image.</p></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="step-3-clustering-customer-journey-images-to-identify-standard-paths">Step 3: Clustering customer journey “images” to identify standard paths<a href="#step-3-clustering-customer-journey-images-to-identify-standard-paths" class="hash-link" aria-label="Direct link to Step 3: Clustering customer journey “images” to identify standard paths" title="Direct link to Step 3: Clustering customer journey “images” to identify standard paths">​</a></h3><p>In order to apply a clustering algorithm first, we had to clean and apply pre-processing to our data. We then had to control the dimensions in order to keep the number of variables below a defined threshold (we chose ~log #observations). To achieve such control we used autoencoder neural networks.</p><p>The goal of such algorithm is to learn a semantic representation by extracting the relevant signals from a given set of observations. This is a dimension reduction method that enables us to perform non-linear transformations. In other words autoencoders enable us to turn our inputs into meaningful and compressed data by getting rid of the non relevant information.</p><div class="theme-admonition theme-admonition-info alert alert--info admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_S0QG"><p>PCA is another such dimension reduction method that performs linear transformations.</p></div></div><p><img loading="lazy" src="/assets/images/Image_3-142efcbabc575d028c9301768810ce8f.png" width="1615" height="536" class="img_ev3q"></p><div align="center"> Using a convolutional autoencoder to compress customer journeys </div><br><p> The above figure shows how an autoencoder performs on a dimension reduction problem. First, the input signal is compressed over the blue layers to reach a compressed representation (represented by the Dense layer). The signal is reconstructed using the green layers to then give the output. <strong>The goal is to have similar inputs and outputs while controlling the size of the Dense layer.</strong></p><p>The above architecture is different from the classic convolutional autoencoders (classic autoencoders involve a succession of convolution and pooling layers for the encoding part, and upsampling layers for the decoding part). The architecture we used relies on convolution layers, and on deconvolution layers rather than upsampling ones. Since deconvolution layers are trainable (meaning that they depend on learnable parameters), and upsampling ones are not, this architecture can learn more complex transformations than a classic convolutional autoencoder of the same depth.</p><p> <em>This architecture is inspired from the one described in this <a href="https://xifengguo.github.io/papers/ICONIP17-DCEC.pdf" target="_blank" rel="noopener noreferrer">article about Deep Clustering</a></em>.</p><p>Once we had completed the dimension reduction, we need to apply a clustering solution on the compressed customer journeys. We then assigned each group of similar customer journeys to a representative standard journey.</p><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span><mdxadmonitiontitle><strong>A word about clustering</strong></mdxadmonitiontitle></div><div class="admonitionContent_S0QG"><p>A clustering process consists of building groups of observations (e.g. customer journeys) based on their characteristics. The key components of the clustering process are:</p><ul><li>Building groups containing similar observations: we talk about intra-class variance minimization.</li><li>Building groups that are different from each other: we talk about inter-class variance maximization.</li></ul><p>Clustering tasks rely on unlabelled data (such learning tasks are known as ‘unsupervised learning’).</p></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="making-the-customer-journey-analysis-easier-to-perform---taking-advantage-of-a-reduced-number-of-journeys">Making the customer journey analysis easier to perform - taking advantage of a reduced number of journeys<a href="#making-the-customer-journey-analysis-easier-to-perform---taking-advantage-of-a-reduced-number-of-journeys" class="hash-link" aria-label="Direct link to Making the customer journey analysis easier to perform - taking advantage of a reduced number of journeys" title="Direct link to Making the customer journey analysis easier to perform - taking advantage of a reduced number of journeys">​</a></h2><p>Performing a clustering algorithm on Customer Journeys aims to reduce the complexity of the analysis through the creation of groups with similar journeys. The analysis is easier to perform as it becomes possible to focus only on the representations of each group, instead of analyzing every single path. This method drastically reduces the number of journeys that require our attention. It also becomes possible to highlight complex similarities or patterns when creating these clusters.</p><div class="theme-admonition theme-admonition-info alert alert--info admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_S0QG"><p>Once the clusters are created, each of them is assigned to a standard journey (e.g. when using k-means, these standard journey are called centroids).</p></div></div><p><img loading="lazy" src="/assets/images/Image_4-87162fb7366cd46a150c4d3f96db2334.png" width="1199" height="446" class="img_ev3q"></p><div align="center"> Example of standard journeys </div><br><p>The example above shows three of these standard journeys. We can already see some interesting information and differences between the clusters:</p><ul><li><p>Customers in the Standard journey #1 group complete more actions compared to #2 and #3.</p></li><li><p>All three standard journeys have different starting and ending pages that correspond to the first and last rows in the pictures). By looking at these standard journeys, at a glance we can already spot some initial differences between the clusters. </p></li></ul><p>Note that deeper analyses can be performed on these journeys by looking at the non-embedded data.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="leveraging-the-customer-journey-clustering---three-ways-of-improving-customer-centric-strategy">Leveraging the Customer Journey Clustering - three ways of improving customer-centric strategy<a href="#leveraging-the-customer-journey-clustering---three-ways-of-improving-customer-centric-strategy" class="hash-link" aria-label="Direct link to Leveraging the Customer Journey Clustering - three ways of improving customer-centric strategy" title="Direct link to Leveraging the Customer Journey Clustering - three ways of improving customer-centric strategy">​</a></h2><p>We believe that this solution has several uses to improving the way businesses operate by better understanding customer behaviour, forming a stronger view of the customer lifetime value, and improving the user experience and conversion rate.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="improving-websites">Improving websites<a href="#improving-websites" class="hash-link" aria-label="Direct link to Improving websites" title="Direct link to Improving websites">​</a></h3><p>We can use customer journey clustering to build a representative journey (using the centroid of each cluster), establishing the usual actions a user performs between touchpoint A (e.g. website’s homepage) and touchpoint B (e.g. website’s purchasing page). These typical journeys help gain a better understanding of the customers’ experiences, allowing us to identify ways to improve the website. </p><p>For example, it will help brands optimise the groups of products shown at the top of a landing page.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="building-audiences">Building audiences<a href="#building-audiences" class="hash-link" aria-label="Direct link to Building audiences" title="Direct link to Building audiences">​</a></h3><p>By understanding the size and similarities between groups that interact with our website Customer Journey Clustering is also a way to create specific audiences. It then allows brands to tailor their activity to engage with these specific audiences, improving the overall experience. </p><p>For example, if clusters 1 and 2 appear similar, but cluster 1 has a higher conversion rate than cluster 2, then analyzing the behaviours from cluster 1 may help us understand how to better retarget cluster 2. </p><p>These insights also make it possible to redirect customers through an optimal action that will drive more conversions.</p><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_S0QG"><p>We could go a step further by creating customer conversion scores based on their journey. These scores could then be used to build an optimized communication strategy, leading to the next optimal action (i.e. increase or reduce media spend, depending on the score). </p></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="improve-churn-prediction">Improve churn prediction<a href="#improve-churn-prediction" class="hash-link" aria-label="Direct link to Improve churn prediction" title="Direct link to Improve churn prediction">​</a></h3><p>Another use case to Customer Journey clustering is evaluating churn and when customers are likely to shop with a competitor or stop ordering from us. When journeys are associated to a churn score, it becomes possible to create a predictive model that can be used to identify the groups that are more inclined to churn.</p><p>Following the training of the model, the learnings could be applied to the standard journeys associated with each cluster, outputting a global churn score per cluster. These scores can then be evaluated and compared to other groups to better manage expected orders and improve communications with at-risk customers. </p><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_S0QG"><p>Since a churn predictive model such as this is trained on the 3D embedded customer journeys, one can expect it to be capable of spotting complex churn patterns involving huge sequences of actions, or time spent per action.</p></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><p>At Ekimetrics we believe that customer journey clustering can help enhance the customer experience and increase customer lifetime value. We also know that with customer behaviours changing rapidly, an approach such as this provides a flexible solution that can translate into actionable insights for any business. </p><p>However, complexity is not always the answer. A good model should tell a compelling and comprehensive story, but increased complexity must be accompanied by an effort to improve interpretability and should showcase enhanced benefits for business application.</p><p>The customer journey is still a challenge for every business globally. We do believe that there is no « plug and play » answer and each solution needs to be built for the problem at hand. For us, intelligent data science means developing integrated approaches and leveraging different methods to help make brighter business decisions.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="appendix">Appendix<a href="#appendix" class="hash-link" aria-label="Direct link to Appendix" title="Direct link to Appendix">​</a></h2><p>To give you more details, if we take an example where a user can perform up to 10 different actions on a website, the number of possible journeys is 9,864,100.</p><div class="theme-admonition theme-admonition-info alert alert--info admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_S0QG"><p>Given n the number of possible interactions, the number of possible journeys is given by the sum from 1 to n of the number of permutation of n.</p></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">​</a></h2><ul><li>Xifeng Guo, Xinwang Liu, En Zhu, and Jianping Yin. <a href="https://xifengguo.github.io/papers/ICONIP17-DCEC.pdf" target="_blank" rel="noopener noreferrer">Deep Clustering with Convolutional
Autoencoders</a></li></ul></div>]]></content:encoded>
            <category>Clustering</category>
            <category>Autoencoders</category>
            <category>Churn</category>
            <category>Interpretability</category>
        </item>
        <item>
            <title><![CDATA[Introduction to Pyepidemics - epidemiological modeling in Python]]></title>
            <link>https://ekimetrics.github.io/blog/introduction-pyepidemics</link>
            <guid>https://ekimetrics.github.io/blog/introduction-pyepidemics</guid>
            <pubDate>Mon, 13 Dec 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[We created Pyepidemics, an open-source library to simulate epidemics (SIR, SEIHDR, COVID19). We detail in this article what you can build with the library.]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/intro-pyepidemics-570112f749aac7c85f218663ef85daed.jpg" width="1189" height="669" class="img_ev3q"></p></div><div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="pyepidemics">Pyepidemics<a href="#pyepidemics" class="hash-link" aria-label="Direct link to Pyepidemics" title="Direct link to Pyepidemics">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="context">Context<a href="#context" class="hash-link" aria-label="Direct link to Context" title="Direct link to Context">​</a></h3><p>During the first wave of COVID19 in 2020, Ekimetrics joined the CoData movement, a coalition of data and artificial intelligence specialists whose goal was to pool their skills to provide answers and solutions on the evolution of the pandemics. We had the chance to work with many epidemiological experts, and as we went along we built a toolbox to facilitate our modeling of the current pandemics. We then put this toolbox in open source under the name <strong>pyepidemics</strong> to contribute to the community on this scientific discipline difficult to apprehend for Data Scientists but with obvious bridges facilitating innovation.
Today, with the resurgence of the epidemic in Europe, it seemed important to present this library more widely to democratize these analyses on a larger scale. </p><ul><li>The library is available on Github at <a href="https://github.com/ekimetrics/pyepidemics" target="_blank" rel="noopener noreferrer">this link</a></li><li>The documentation is available at <a href="https://ekimetrics.github.io/pyepidemics" target="_blank" rel="noopener noreferrer">this link</a></li></ul><p>This article will serve as a synthetic presentation of what can be done with the library, please refer to the documentation for more details. Do not hesitate to post issues on Github and to contribute with new proposals, the development is still in experimental version. </p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="introduction-to-pyepidemics">Introduction to Pyepidemics<a href="#introduction-to-pyepidemics" class="hash-link" aria-label="Direct link to Introduction to Pyepidemics" title="Direct link to Introduction to Pyepidemics">​</a></h3><p>Pyepidemics allows to simply create compartmental epidemiological models (also used in system dynamics) and to solve the differential equations that model the phenomenon. The different features implemented today are: </p><ul><li>Creation of classical compartmental models (SIR, SEIR, SEIDR, etc...)</li><li>Creation of COVID19 related model (with ICU and different levels of symptoms)</li><li>Creation of custom compartmental model</li><li>Implementation of policies (lockdown, tracing, testing, etc...)</li><li>Calibration of epidemiological parameters on real-world data using Bayesian optimization</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="installation">Installation<a href="#installation" class="hash-link" aria-label="Direct link to Installation" title="Direct link to Installation">​</a></h3><p>You can simply install pyepidemics using the command </p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">pip install pyepidemics</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="introduction-to-epidemiological-modeling-in-python">Introduction to epidemiological modeling in Python<a href="#introduction-to-epidemiological-modeling-in-python" class="hash-link" aria-label="Direct link to Introduction to epidemiological modeling in Python" title="Direct link to Introduction to epidemiological modeling in Python">​</a></h2><p>The images, the reasoning and the construction of the first bricks of the library are largely inspired by the exceptional work of Henri Froese with his series of articles on epidemiology, in particular the first article <a href="https://towardsdatascience.com/infectious-disease-modelling-beyond-the-basic-sir-model-216369c584c4" target="_blank" rel="noopener noreferrer">Infectious Disease Modelling: Beyond the Basic SIR Model</a>. </p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="compartmental-models">Compartmental models<a href="#compartmental-models" class="hash-link" aria-label="Direct link to Compartmental models" title="Direct link to Compartmental models">​</a></h3><p>An epidemic is modeled with the different possible states for the population, for example: unaffected, immunized, vaccinated, symptomatic patient, asymptomatic patient, hospitalized, in intensive care ... Each state will be modeled with a compartment, and the population will transition between the different compartments according to parameters of probability and transition duration. In concrete terms, this means solving a system of differential equations as a function of time. </p><p>For example the simplest compartmental model is the SIR model (for the 3 states Susceptible - Infected - Removed).
It is possible to write the following compartments with their transition:</p><div align="center"><p><img loading="lazy" src="data:image/png;base64,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" width="875" height="102" class="img_ev3q"></p></div>The transition parameters being given by :<div align="center"><p><img loading="lazy" src="/assets/images/sir2-8ea66548e761663f2852bafada77bb09.png" width="460" height="81" class="img_ev3q"></p></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="building-a-sir-model-with-pyepidemics">Building a SIR model with pyepidemics<a href="#building-a-sir-model-with-pyepidemics" class="hash-link" aria-label="Direct link to Building a SIR model with pyepidemics" title="Direct link to Building a SIR model with pyepidemics">​</a></h3><p>This section is detailed in this <a href="https://ekimetrics.github.io/pyepidemics/tutorials/quickstart/" target="_blank" rel="noopener noreferrer">tutorial</a> which is also available directly on <a href="https://colab.research.google.com/github/ekimetrics/pyepidemics/blob/master/docs/tutorials/quickstart.ipynb" target="_blank" rel="noopener noreferrer">Colab</a>. </p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="using-the-bank-of-models">Using the bank of models<a href="#using-the-bank-of-models" class="hash-link" aria-label="Direct link to Using the bank of models" title="Direct link to Using the bank of models">​</a></h4><p>As the SIR model is a standard model, you can find an already coded version in the model bank. We will learn in the next section how this abstraction is built to allow you to add details in the modeling. </p><p>With pyepidemics:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> pyepidemics</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">models </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> SIR</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">N </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">1000</span><span class="token plain"> </span><span class="token comment" style="color:rgb(98, 114, 164)"># Thousand persons</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">beta </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">3</span><span class="token operator">/</span><span class="token number">4</span><span class="token plain"> </span><span class="token comment" style="color:rgb(98, 114, 164)"># One person contaminates 3/4 person per day</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">gamma </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">1</span><span class="token operator">/</span><span class="token number">4</span><span class="token plain"> </span><span class="token comment" style="color:rgb(98, 114, 164)"># One person stay infected for 4 days</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">sir </span><span class="token operator">=</span><span class="token plain"> SIR</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">N</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">beta</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">gamma</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>It is then possible to solve the system of differential equations simply with the method <code>.solve()</code> (more parameters are available, see the tutorial quoted above : </p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">states </span><span class="token operator">=</span><span class="token plain"> sir</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">solve</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">states</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">show</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">plotly </span><span class="token operator">=</span><span class="token plain"> </span><span class="token boolean">False</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>What is commonly called an epidemic "wave" is observed. </p><div align="center"><p><img loading="lazy" src="/assets/images/sir3-484b2d98bfdbc017ca838656ab9f087e.png" width="1104" height="310" class="img_ev3q"></p></div><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="reimplementing-the-sir-model">Reimplementing the SIR model<a href="#reimplementing-the-sir-model" class="hash-link" aria-label="Direct link to Reimplementing the SIR model" title="Direct link to Reimplementing the SIR model">​</a></h4><p>Let's now go into the internal workings of this abstraction to reimplement its operation in a few lines of code. Concretely we build a graph between the different states by detailing the transitions. Pyepidemics then translates this graph into a system of differential equations in order to solve it. </p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> pyepidemics</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">models </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> CompartmentalModel</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">class</span><span class="token plain"> </span><span class="token class-name">SIR</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">CompartmentalModel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">__init__</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">N</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">beta</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">gamma</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token comment" style="color:rgb(98, 114, 164)"># Define compartments name and number</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        compartments </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"S"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">"I"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">"R"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token builtin" style="color:rgb(189, 147, 249)">super</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">__init__</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">compartments</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token comment" style="color:rgb(98, 114, 164)"># Parameters</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">N </span><span class="token operator">=</span><span class="token plain"> N </span><span class="token comment" style="color:rgb(98, 114, 164)"># Total population</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">beta </span><span class="token operator">=</span><span class="token plain"> beta </span><span class="token comment" style="color:rgb(98, 114, 164)"># How many person each person infects per day</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">gamma </span><span class="token operator">=</span><span class="token plain"> gamma </span><span class="token comment" style="color:rgb(98, 114, 164)"># Rate of infection, duration = 1/gamma</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token comment" style="color:rgb(98, 114, 164)"># Add transition</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">add_transition</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"S"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">"I"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">lambda</span><span class="token plain"> y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">t</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">beta </span><span class="token operator">*</span><span class="token plain"> y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"S"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"I"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">/</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">N</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">add_transition</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"I"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">"R"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">lambda</span><span class="token plain"> y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">t</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">gamma </span><span class="token operator">*</span><span class="token plain"> y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"I"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>So it is possible to build any kind of compartmental model with this operation. You can see more complex examples in this <a href="https://ekimetrics.github.io/pyepidemics/tutorials/custom_model/" target="_blank" rel="noopener noreferrer">tutorial</a>.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="create-a-realistic-epidemiological-model-for-covid19">Create a realistic epidemiological model for COVID19<a href="#create-a-realistic-epidemiological-model-for-covid19" class="hash-link" aria-label="Direct link to Create a realistic epidemiological model for COVID19" title="Direct link to Create a realistic epidemiological model for COVID19">​</a></h2><p>In the first section we saw how to build a SIR model, however we are far from being able to use this model to simulate an epidemic like COVID19 and for governments to be able to take actions using those analyses. For this we would need to:</p><ul><li>Have a more complete description taking into account the specifics of the disease states</li><li>Take into account the mitigation strategies provided by governments</li><li>Calibrate the model to describe the evolution of the epidemic as closely as possible</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="build-the-right-compartmental-model">Build the right compartmental model<a href="#build-the-right-compartmental-model" class="hash-link" aria-label="Direct link to Build the right compartmental model" title="Direct link to Build the right compartmental model">​</a></h3><p>Indeed, the COVID19 is more specific than simply the 3 SIR states described above. During the year 2020, we used the following compartmental model inspired by the work of the Pasteur Institute and the INSERM (taking into account the different levels of symptoms, the incubation phase, and the passages in hospital or in intensive care)</p><p><img loading="lazy" src="/assets/images/model-fc4e9034f32bad97e1e9c6dbb213cbbd.png" width="1068" height="475" class="img_ev3q"></p><p>You can look in the library at the implementation of the <code>COVID19</code> model to see the implementation (very close to what has been described above for the SIR model, one of the strengths of pyepidemics). </p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="describe-the-mitigation-solutions-implemented-containment-testing">Describe the mitigation solutions implemented (containment, testing)<a href="#describe-the-mitigation-solutions-implemented-containment-testing" class="hash-link" aria-label="Direct link to Describe the mitigation solutions implemented (containment, testing)" title="Direct link to Describe the mitigation solutions implemented (containment, testing)">​</a></h3><p>Some parameters, such as the famous "R0", have varied enormously during the epidemic depending on behaviors (distancing, masks), policies (testing, lockdowns, vaccination), or variants of the epidemic. Thus, to create a realistic model, we need to take these variations into account.
We will take the example of the first lockdowns to understand how to implement these variations with pyepidemics.   </p><p>Let's create an evolution of the parameter <code>R(t)</code> according to the different lockdowns (during the first wave for example)</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> pyepidemics</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">policies</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">utils </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> make_dynamic_fn</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">policies </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token number">3.3</span><span class="token operator">/</span><span class="token number">4</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1</span><span class="token operator">/</span><span class="token number">4</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">53</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">2</span><span class="token operator">/</span><span class="token number">4</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">80</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">fn </span><span class="token operator">=</span><span class="token plain"> make_dynamic_fn</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">policies</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">sigmoid </span><span class="token operator">=</span><span class="token plain"> </span><span class="token boolean">False</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Create our time-dependent parameter</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">beta </span><span class="token operator">=</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">lambda</span><span class="token plain"> y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">t </span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> fn</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">t</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Visualize policies</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">x </span><span class="token operator">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">linspace</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">100</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">y </span><span class="token operator">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">vectorize</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">fn</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">x</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">plt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">figure</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">figsize </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">15</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">4</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">plt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">plot</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">x</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">y</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">plt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">show</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><img loading="lazy" src="/assets/images/policies-525fb1e91b67b9b2e61dbd330609b252.png" width="1092" height="315" class="img_ev3q"></p><p>And that's it! We can use this time-dependent parameter like we used a constant before. To see more complete examples and results, you can redirect to this <a href="https://ekimetrics.github.io/pyepidemics/tutorials/beyond-sir/" target="_blank" rel="noopener noreferrer">tutorial</a>.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="find-the-most-appropriate-parameters-for-calibration">Find the most appropriate parameters for calibration<a href="#find-the-most-appropriate-parameters-for-calibration" class="hash-link" aria-label="Direct link to Find the most appropriate parameters for calibration" title="Direct link to Find the most appropriate parameters for calibration">​</a></h3><p>Finally, in the previous example, as in those before, we chose arbitrarily the values of the different probabilities and transition times. But these are precisely what epidemiologists want to estimate. How does this work? Simply by testing different parameters until the solution of the compartmental model matches as much as possible the real evolution of the epidemic. Of course, we are not going to test all the combinations but rather : </p><ul><li>Start with an a priori on the different parameter values from the most recent epidemiological studies </li><li>Use a Bayesian optimization algorithm to go through the parameter space efficiently without having to calculate everything. </li></ul><p>These two methods are easy to implement with pyepidemics if we have a pyepimemics model with a <code>.reset()</code> method that recreates a compartmental model with the new set of parameters. First we create a parameter space to explore:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">space </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">"beta_low"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">0.01</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">0.4</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">"beta_high"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">0.6</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">1.5</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">"I0"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">100</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">"recovery_rate_asymptomatic"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1</span><span class="token operator">/</span><span class="token number">8</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">1</span><span class="token operator">/</span><span class="token number">2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">"recovery_rate_mild"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1</span><span class="token operator">/</span><span class="token number">8</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">1</span><span class="token operator">/</span><span class="token number">2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">"death_rate_hospital"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">0.006</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">0.02</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">"death_rate_icu"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">0.006</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">0.02</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">"recovery_rate_icu"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">0.02</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">0.08</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">"recovery_rate_hospital"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">0.02</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">0.08</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Then we will simply use the method <code>.fit()</code> by giving as input a <em>dataframe</em> with the real values of the epidemic for the compartments for which we would have succeeded in obtaining data (in general the deaths and the hospital stays which are the most reliable). </p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">fit</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    cases</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"D"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">"H"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">"ICU"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    space</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    n </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">200</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    early_stopping </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">100</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>This step is not easy, takes time and often also requires iterations to change the definition of the compartmental model. But once solved it is what allows epidemiologists to follow the dynamics of the pandemic. </p><p>More details on calibration are available in this <a href="https://ekimetrics.github.io/pyepidemics/tutorials/calibration/" target="_blank" rel="noopener noreferrer">tutorial</a>. </p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><p>With this article we could introduce you to our epidemiological modeling library pyepidemics - simple modeling, flexible compartmental models, calibration, dynamic parameters, etc... The details of the different features are available in the library documentation. </p><p>As previously mentioned, this library is still under experimental development, we are still using it to model the 5th wave of the epidemic, in particular by adding compartments for vaccination - we will write an article on the modeling in the coming months. Do not hesitate to contribute to democratize this discipline to the Data Science community. </p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">​</a></h2><p>These few references helped us greatly during the development of the library</p><ul><li><a href="https://towardsdatascience.com/infectious-disease-modelling-beyond-the-basic-sir-model-216369c584c4" target="_blank" rel="noopener noreferrer">Infectious Disease Modelling: Beyond the Basic SIR Model</a></li><li><a href="https://www.medrxiv.org/content/10.1101/2020.05.08.20095521v1" target="_blank" rel="noopener noreferrer">Expected impact of lockdown in Île-de-France and possible exit strategies</a> - by INSERM</li><li><a href="https://hal-pasteur.archives-ouvertes.fr/pasteur-02548181/document" target="_blank" rel="noopener noreferrer">Estimating the burden of SARS-CoV-2 in France</a> - by Institut Pasteur</li></ul></div>]]></content:encoded>
            <category>Open-Source</category>
            <category>Epidemiology</category>
            <category>Bayesian</category>
        </item>
        <item>
            <title><![CDATA[Solving the Traveling Salesman Problem with Reinforcement Learning]]></title>
            <link>https://ekimetrics.github.io/blog/2021/11/03/tsp</link>
            <guid>https://ekimetrics.github.io/blog/2021/11/03/tsp</guid>
            <pubDate>Wed, 03 Nov 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[A practical use of Reinforcement Learning and the Q-Learning algorithm to solve the Traveling Salesman Problem]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/tsp-6a6c73c13104c2c80f047a36eb607d39.jfif" width="1484" height="840" class="img_ev3q"></p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="summary">Summary<a href="#summary" class="hash-link" aria-label="Direct link to Summary" title="Direct link to Summary">​</a></h2><div align="justify"><p>Reinforcement Learning (RL) is usually applied for state of the art AI research and often make the headlines. Yet it still fails to deliver on concrete business topics. At Ekimetrics we strive to transfer AI innovations into the business world and Reinforcement Learning is a unbelievable playground to find disruptive solutions to complex real-world problems. In particular, there are many optimization problems that could be solved using RL.</p><p>The Traveling Salesman Problem (TSP) has been solved for many years and used for tons of real-life situations including <strong>optimizing deliveries</strong> or <strong>network routing</strong>. This article will show a simple framework to <strong>apply Q-Learning to solving the TSP</strong>, and discuss the pros &amp; cons with other optimization techniques. It's a perfect introduction for beginners in Reinforcement Learning and does not require heavy computational capabilities.</p><blockquote><p>You can find all the code open sourced <a href="https://github.com/TheoLvs/reinforcement-learning/tree/master/5.%20Delivery%20Optimization" target="_blank" rel="noopener noreferrer">here on Github</a></p></blockquote><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-traveling-salesman-problem">The Traveling Salesman Problem<a href="#the-traveling-salesman-problem" class="hash-link" aria-label="Direct link to The Traveling Salesman Problem" title="Direct link to The Traveling Salesman Problem">​</a></h2><div align="center"><p><img loading="lazy" src="/assets/images/tsp_example-a3af33433166558228c6bd765c1aeb5e.png" width="900" height="482" class="img_ev3q"></p></div><p>The Traveling Salesman Problem (or TSP) is a typical optimization problem, where one has to find the shortest route to visit different cities. There are many different ways to solve this problem using discrete optimization techniques. </p><p>Like many optimization problems, it's a NP-hard problem, which in short means that it's easy (in terms of speed) to solve for 5 cities, already impossible to brute force for 50. And almost impossible for most algorithms for  5,000 cities. Even for 15 cities, it's already 1 trillion permutations to compute (15!), there are optimization techniques that are more adequate : dynamic programming, branch and bound algorithms, nearest neighbors approximations, or ant colonies optimizations.</p><p>Furthermore, the problem described here is too simple to describe a real-life delivery situation. It does not take into account multiple vehicle fleet, electric vehicle charging, time window constraints, capacity constraints, aleatory perturbations, etc... Hence, each variant of the TSP has its own optimization frameworks (in terms of variables and constriants), and the more you complexify the problem, the more difficult it is of course. That's why in practice delivery companies use combinations of those variants coupled with heuristics. The most advanced companies today add Machine Learning techniques on top of those algorithms, in particular to replace manual heuristics and better estimate in-context durations.    </p><p>In Python, the easiest way to get started with TSP and its variants is probably the great open source library <a href="https://developers.google.com/optimization/routing" target="_blank" rel="noopener noreferrer">OR-Tools by Google</a>. And if you want to learn more about discrete optimization, I can only recommend the great MOOC on <em>Discrete Optimization by the University of Melbourne</em> you can find on <a href="https://www.coursera.org/learn/discrete-optimization" target="_blank" rel="noopener noreferrer">Coursera</a>. </p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="applying-reinforcement-learning-to-the-tsp">Applying Reinforcement Learning to the TSP<a href="#applying-reinforcement-learning-to-the-tsp" class="hash-link" aria-label="Direct link to Applying Reinforcement Learning to the TSP" title="Direct link to Applying Reinforcement Learning to the TSP">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="why-bother-using-reinforcement-learning">Why bother using Reinforcement Learning?<a href="#why-bother-using-reinforcement-learning" class="hash-link" aria-label="Direct link to Why bother using Reinforcement Learning?" title="Direct link to Why bother using Reinforcement Learning?">​</a></h3><p>If it's already solved by classical optimization techniques, why bother using Reinforcement Learning? Well several answers: </p><ul><li>It's fun and I personally love practicing RL. Curiosity is a great driver for innovation, and here I was really wondering if it could be applied for such a problem.</li><li>RL is rarely used in real-life problems. Playing games or manipulating robot hands is awesome, but when it comes to business problems, RL often fails compared to simple heuristics or algorithms. At Ekimetrics, we always look for applying state-of-the-art AI research to the business problems we encounter in the real-world. </li></ul><p>However, Reinforcement Learning (in theory) would hold many advantages compared to classical optimization techniques : </p><ul><li><strong>Offering a general framework for all problems</strong>, indeed instead of tweaking the constraints and defining extra variables, you can change the reward, and defining a multi agent problem if needed for fleet optimization. Adding extra information like delivery time estimation is also eased if you can integrate the prediction algorithm with a similar ML techniques (e.g. Neural Networks)</li><li><strong>Having a "live" decision making algorithm</strong>. Because you would train a RL alorithm by making the next delivery decision at each stop, compared to "offline" optimization algorithms that study the problem with no unknowns, you inherently would be able to take different decisions if something happened during the experience, whereas you would need to recalculate with classical techniques</li><li><strong>Being robust to unknowns and aleatory perturbations</strong></li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="transforming-the-tsp-to-a-rl-problem">Transforming the TSP to a RL problem<a href="#transforming-the-tsp-to-a-rl-problem" class="hash-link" aria-label="Direct link to Transforming the TSP to a RL problem" title="Direct link to Transforming the TSP to a RL problem">​</a></h3><p>Before jumping into the TSP variants, let's take the most simplest version: <em>a delivery man who has to deliver 50 packages</em>. Between each stop can be defined a distance or a duration (or a more complex metric). </p><p>To transform it to a RL problem, we need to define: </p><ul><li><strong>Agent</strong>: the delivery man</li><li><strong>Environment</strong>: the different packages to deliver (and their destination) and the city where to navigate</li><li><strong>States</strong>: the location where the delivery guy currently stops </li><li><strong>Actions</strong>: at each location, the decisions to make (modeled as a Markov process) are: "where to go next" (or "which location do I chose next")</li><li><strong>Reward</strong>: Between two locations (states), how long (or how far) it is</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="creating-the-routing-environment">Creating the routing environment<a href="#creating-the-routing-environment" class="hash-link" aria-label="Direct link to Creating the routing environment" title="Direct link to Creating the routing environment">​</a></h3><p>Creating a simple version of the environment is quite simple in pure Python. Indeed, you can store the position of the stops in a 2D virtual world as a numpy array or a Pandas DataFrame. Distances between stops can be calculated with an euclidean distance, but it could be complexified to account for durations or represent any distance/duration metric you would see in a routing network. </p><div align="center"><p><img loading="lazy" src="/assets/images/env2-8887c8a80fd93eab07028f70b2a38dc3.png" width="404" height="408" class="img_ev3q"></p></div><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> scipy</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">spatial</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">distance </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> cdist</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">ENV_SIZE </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">10</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">N_STOPS </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">100</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Creating the stops using numpy random points generator</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">xy </span><span class="token operator">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">random</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">rand</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">N_STOPS</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token operator">*</span><span class="token plain">ENV_SIZE</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Computing the distances between each points</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Here use euclidean distances, but any metric would do</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># This distance matrix can actually represent a time, a distance or something else</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">distance_matrix </span><span class="token operator">=</span><span class="token plain"> cdist</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">xy</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">xy</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Then we want to define the starting position, the route, and the end position. Meaning that if we have to traverse A, B, C, D, E, possible routes would be for example <code>A-&gt;C-&gt;B-&gt;E-&gt;D</code> or <code>B-&gt;D-&gt;E-&gt;C-&gt;A</code>. </p><p>By the way with 5 stops you have 5! possible routes or 120 possible routes. For 10 stops you already have 3.6 million possible routes. For 100 stops it's 10 to the 158th power routes. It's called the combinatory explosion, and that explains why this simple problem is impossible to brute force for a high number of stops. </p><p>We can visualize routes by drawing lines between each stop</p><div align="center"><p><img loading="lazy" src="/assets/images/env1-ce256f64cdffdbd7e5143d792d7867ad.png" width="404" height="408" class="img_ev3q"></p></div><p>We can also already imagine more complex and realistic situations. For example, one experiment I wanted to do was to stress test the RL algorithm to aleatory events. For example a known environment with unknown perturbations, like traffic. To model this interaction, we add a "traffic zone" inside our environment, where routes are much slower if taken. To account for the extra duration, we calculate the intersection points of the route within the "traffic zone" and measure the distance of the formed segment, we then use that distance weighed with a traffic intensity factor. </p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">i1,i2 = intersections</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">distance_traffic = np.sqrt((i2[1]-i1[1])**2 + (i2[0]-i1[0])**2)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">additional_reward = distance_traffic * traffic_intensity * np.random.rand()</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div align="center"><p><img loading="lazy" src="data:image/png;base64,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" width="404" height="408" class="img_ev3q"></p></div>Now that we have the overal idea, we have to design an environment object in Python to be fed to a Reinforcement Learning agent. We use the typical design framework inspired from OpenAI Gym:<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">class</span><span class="token plain"> </span><span class="token class-name">DeliveryEnvironment</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">reset</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">"""Restart the environment for experience replay</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">        Returns the first state</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">        """</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">pass</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">step</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">"""Takes an action in a given state</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">        Returns:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">            s_next: the next state</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">            reward: the reward for such action</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">            done: if the simulation is done</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">        """</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">pass</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">render</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">"""Visualize the environment state</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">        """</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">pass</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="designing-the-q-learning-algorithm">Designing the Q-Learning algorithm<a href="#designing-the-q-learning-algorithm" class="hash-link" aria-label="Direct link to Designing the Q-Learning algorithm" title="Direct link to Designing the Q-Learning algorithm">​</a></h3><p>Before jumping into complex Deep Learning, I wondered if a simple <strong>Q-Learning framework would work</strong>. Basically with Q-Learning you want to evaluate a matrix mapping states, actions and rewards. Indeed to make a decision in a given state about the best actions to do, you would love to have an estimate if the decision was the best in the long term. This is represented by the Q values. </p><p>In our case, the rows are the different states (all the stops) and the columns the possible actions to take in this state, hence the next stop to go. The values are the estimated long-term reward you would get by taking this action. So, if you are found in a state A, you would like to take the action with the maximum Q value. </p><p>For our TSP problem for example, we would have a Q-Matrix of 50 by 50 if we have 50 stops. </p><p>Because we want to inform the routing algorithm with as much unbiased data we can find, we can actually initalize the Q matrix (also called Value function because it maps out states and actions to rewards - the values) with the distance matrix between all stops. Indeed, if you would not consider a long-term decision making strategy, you would apply a greedy one where you would chose the closest stop as a next destination. Yet it would definetely be better than random. </p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> scipy</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">spatial</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">distance </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> cdist</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Q </span><span class="token operator">=</span><span class="token plain"> cdist</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">xy</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">xy</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Ok, but how to update those values and incorporate long-term planning? This is exactly the goal of the Q-Learning algorithm.
Imagine you are delivering pizzas every day all across the city, you have an old GPS to help you decide the shortest route between your stops. But your city is much more complicated than what you and your GPS know. What would happen? </p><ul><li>The first day, you would chose the closest stop first and then jump to the next closest one. You will quickly realize that it's not optimized. You should maybe have gone first to another neighborhood and deliver everything in the area before jumping to another one. </li><li>So the next day, you commit to another strategy, armed with yesterday's experience. Definitely, looking ahead and avoiding single deliveries in remote areas even if they are closest is a better idea. But while you are exploring the best routes, you pass through the city center and get blocked into traffic that terribly slows you down. </li><li>The 3rd day, you will try to apply the same general strategy, but definitely if possible you will never go through the city center, and try circling around the city to save some precious time. </li></ul><p>This <strong>trial-and-error behavior is basically how experience replay works in the Reinforcement Learning framework</strong>. </p><p>If we code an agent abstraction, it could look like this:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">class</span><span class="token plain"> </span><span class="token class-name">QAgent</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">Agent</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">__init__</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">states_size</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">actions_size</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">epsilon </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">1.0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    epsilon_min </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">0.01</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">epsilon_decay </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">0.999</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">gamma </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">0.95</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">lr </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">0.8</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">states_size </span><span class="token operator">=</span><span class="token plain"> states_size</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">actions_size </span><span class="token operator">=</span><span class="token plain"> actions_size</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">epsilon </span><span class="token operator">=</span><span class="token plain"> epsilon</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">epsilon_min </span><span class="token operator">=</span><span class="token plain"> epsilon_min</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">epsilon_decay </span><span class="token operator">=</span><span class="token plain"> epsilon_decay</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">gamma </span><span class="token operator">=</span><span class="token plain"> gamma</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">lr </span><span class="token operator">=</span><span class="token plain"> lr</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Q </span><span class="token operator">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">build_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">states_size</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">actions_size</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">build_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">states_size</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">actions_size</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        Q </span><span class="token operator">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">zeros</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">states_size</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">actions_size</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">return</span><span class="token plain"> Q</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">train</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">s</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">r</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">s_next</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Q</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">s</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Q</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">s</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">+</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">lr </span><span class="token operator">*</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">r </span><span class="token operator">+</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">gamma</span><span class="token operator">*</span><span class="token plain">np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token builtin" style="color:rgb(189, 147, 249)">max</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Q</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">s_next</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token operator">-</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Q</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">s</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">if</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">epsilon </span><span class="token operator">&gt;</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">epsilon_min</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">epsilon </span><span class="token operator">*=</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">epsilon_decay</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">act</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">s</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        q </span><span class="token operator">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Q</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">s</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">if</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">random</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">rand</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token operator">&gt;</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">epsilon</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            a </span><span class="token operator">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">argmax</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">q</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">else</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            a </span><span class="token operator">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">random</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">randint</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">actions_size</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">return</span><span class="token plain"> a</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The most important line is this one:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Q</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">s</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Q</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">s</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">+</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">lr </span><span class="token operator">*</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">r </span><span class="token operator">+</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">gamma</span><span class="token operator">*</span><span class="token plain">np</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token builtin" style="color:rgb(189, 147, 249)">max</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Q</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">s_next</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token operator">-</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Q</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">s</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>This is the update rule in basic Q-Learning, where you increment the Q-value for each action in each experience replay (simulating a day of delivery hundreds of time) by the current reward + the reward for the best possible action you would take in the future. This recursive equation is a variation of the famous Bellman equation for Q value functions. </p><p>Notice two factors: </p><ul><li>The learning rate <code>lr</code> control the learning speed (like in Deep Learning)</li><li>The gamma factor <code>gamma</code> is the discount factor, and control long-term planning. Indeed if gamma = 0, you will only get the next action reward (your agent is "short sighted" only seeking current rewards), if gamma = 1 you will be more oriented towards the future rewards (above 1 it may diverge)</li></ul><p>Those two factors are our most important hyperparameters to tune during training (there are others like the epsilon variables defined in the initialization, if you are curious take a look at epsilon-greedy methods, a super simple way of tackling the exploration-exploitation dilemma in Reinforcement Learning)</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="writing-the-training-loop">Writing the training loop<a href="#writing-the-training-loop" class="hash-link" aria-label="Direct link to Writing the training loop" title="Direct link to Writing the training loop">​</a></h3><p>Once we have define the environment, the Q-Agent and the update rules for the value function, we only need the final step: the training loop. We will apply <strong>experience replay</strong> to our poor delivery guy stucked in an infinite time loop. </p><div align="center"><p><img loading="lazy" src="/assets/images/drstrange-9a522bfeff0b9cdcc50a5ba4c85f8c2f.gif" width="500" height="200" class="img_ev3q"> </p></div><p>Every day, over and over, he will try to deliver our 50 packages, find the best routes and do it all over the next day. In Reinforcement Learning we call each day an <strong>episode</strong>, where we simply:</p><ul><li>Reset the environment</li><li>Make a decision of the next state to go to</li><li>Remember the reward gained by this decision (minimum duration or distance elapsed)</li><li>Train our agent with this knowledge</li><li>Make the next decision until all stops are traversed</li></ul><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">run_episode</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">env</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">agent</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">verbose </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    s </span><span class="token operator">=</span><span class="token plain"> env</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">reset</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    agent</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">reset_memory</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    max_step </span><span class="token operator">=</span><span class="token plain"> env</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">n_stops</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    episode_reward </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    i </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">while</span><span class="token plain"> i </span><span class="token operator">&lt;</span><span class="token plain"> max_step</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token comment" style="color:rgb(98, 114, 164)"># Remember the states</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        agent</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">remember_state</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">s</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token comment" style="color:rgb(98, 114, 164)"># Choose an action</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        a </span><span class="token operator">=</span><span class="token plain"> agent</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">act</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">s</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token comment" style="color:rgb(98, 114, 164)"># Take the action, and get the reward from environment</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        s_next</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">r</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">done </span><span class="token operator">=</span><span class="token plain"> env</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">step</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token comment" style="color:rgb(98, 114, 164)"># Tweak the reward</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        r </span><span class="token operator">=</span><span class="token plain"> </span><span class="token operator">-</span><span class="token number">1</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> r</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">if</span><span class="token plain"> verbose</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">print</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">s_next</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">r</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">done</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token comment" style="color:rgb(98, 114, 164)"># Update our knowledge in the Q-table</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        agent</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">train</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">s</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">a</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">r</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">s_next</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token comment" style="color:rgb(98, 114, 164)"># Update the caches</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        episode_reward </span><span class="token operator">+=</span><span class="token plain"> r</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        s </span><span class="token operator">=</span><span class="token plain"> s_next</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token comment" style="color:rgb(98, 114, 164)"># If the episode is terminated</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        i </span><span class="token operator">+=</span><span class="token plain"> </span><span class="token number">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">if</span><span class="token plain"> done</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">break</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">return</span><span class="token plain"> env</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">agent</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">episode_reward</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="results">Results<a href="#results" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="q-learning-for-a-simple-tsp">Q-Learning for a simple TSP<a href="#q-learning-for-a-simple-tsp" class="hash-link" aria-label="Direct link to Q-Learning for a simple TSP" title="Direct link to Q-Learning for a simple TSP">​</a></h3><p>When you start training and experiencing the same problem overtime, your agent learns about the environment, this is shown by the episode rewards values for each experience replay. </p><div align="center"><p><img loading="lazy" src="/assets/images/training-31d61ed9f6a9a6693dd40a6c4dfe151e.png" width="882" height="204" class="img_ev3q"></p></div><ul><li>In the first phase, until the 400th episode, you are still exploring the different routes. Indeed with the epsilon decay method (with <code>epsilon_decay=0.999</code>), you are taking random actions at each step. </li><li>In the second phase, epsilon is lowering, and you start exploiting what you have learnt, and take less and less random actions to be more driven by Q values. </li></ul><p>What's tricky with epsilon-greedy methods, is that it kind of forces of the convergence. So did it work? Let's see. </p><div align="center"><h5 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="50-stops-experiment">50 stops experiment<a href="#50-stops-experiment" class="hash-link" aria-label="Direct link to 50 stops experiment" title="Direct link to 50 stops experiment">​</a></h5><p><img loading="lazy" src="/assets/images/training_50_stops-f0073a10e627a6f50a17b55a2c32eeac.gif" width="504" height="504" class="img_ev3q"></p><h5 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="100-stops-experiment">100 stops experiment<a href="#100-stops-experiment" class="hash-link" aria-label="Direct link to 100 stops experiment" title="Direct link to 100 stops experiment">​</a></h5><p><img loading="lazy" src="/assets/images/training_100_stops-af172053854703e0125a1b6bf4fe81f8.gif" width="504" height="504" class="img_ev3q"></p><h5 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="500-stops-experiment">500 stops experiment<a href="#500-stops-experiment" class="hash-link" aria-label="Direct link to 500 stops experiment" title="Direct link to 500 stops experiment">​</a></h5><p><img loading="lazy" src="/assets/images/training_500_stops-3529ca589990add491164cb5925bfd81.gif" width="504" height="504" class="img_ev3q"></p></div><p>In each experiment, the algorithm converges quite fast to a seamingly acceptable route. After exploring a lot of options it not only gives one route but variations of the accepted strategy, which can already be interesting to find alternatives. </p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="q-learning-for-a-tsp-with-traffic-zones">Q-Learning for a TSP with traffic zones<a href="#q-learning-for-a-tsp-with-traffic-zones" class="hash-link" aria-label="Direct link to Q-Learning for a TSP with traffic zones" title="Direct link to Q-Learning for a TSP with traffic zones">​</a></h3><p>Now that we have our environment, agent and framework defined, what's great with RL is that we don't have to change anything but the reward to model a different situation. Indeed because we tweaked the reward when you drove through a traffic zone, the agent will learn the same way to optimize his holistic route. </p><div align="center"><h5 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="100-stops-experiment-with-traffic-zones">100 stops experiment with traffic zones<a href="#100-stops-experiment-with-traffic-zones" class="hash-link" aria-label="Direct link to 100 stops experiment with traffic zones" title="Direct link to 100 stops experiment with traffic zones">​</a></h5><p><img loading="lazy" src="/assets/images/training_100_stops_traffic-2f1b2933055b3023670ec5f1e577afad.gif" width="504" height="504" class="img_ev3q"></p></div><p>Eureka, the agent will avoid as much as possible the traffic zones</p><div align="center"><h5 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="500-stops-experiment-with-traffic-zones">500 stops experiment with traffic zones<a href="#500-stops-experiment-with-traffic-zones" class="hash-link" aria-label="Direct link to 500 stops experiment with traffic zones" title="Direct link to 500 stops experiment with traffic zones">​</a></h5><p><img loading="lazy" src="/assets/images/training_500_stops_traffic-0467ef757724ee8f0377f56c6f8b7b14.gif" width="504" height="504" class="img_ev3q"></p></div><p>With more points, it's even more interesting, the agent will really circle around the traffic zone and prefer longer but faster routes. </p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="next-steps">Next steps<a href="#next-steps" class="hash-link" aria-label="Direct link to Next steps" title="Direct link to Next steps">​</a></h2><p>I hope this simple experiment has highlighted how to apply (non-Deep Learning) Reinforcement Learning techniques to real-life problems. I haven't had time to benchmark the resolution against other optimization techniques (which I should have done I confess), but let's try to draw some pros and cons for the approach. </p><h5 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="-cons">⛔ Cons:<a href="#-cons" class="hash-link" aria-label="Direct link to ⛔ Cons:" title="Direct link to ⛔ Cons:">​</a></h5><ul><li>Probably slower</li><li>Definitely less accurate than a discrete optimization technique</li></ul><h5 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="-pros">✅ Pros:<a href="#-pros" class="hash-link" aria-label="Direct link to ✅ Pros:" title="Direct link to ✅ Pros:">​</a></h5><ul><li>General framework to be updated in real-life situations (eg: the traffic) and extended to more complex problems</li><li>Alternative routes are proposed</li><li>"Online" decision making (meaning that you have an algorithm armed with a next-best decision recommendation system)</li></ul><p>Next steps is to extend the work to an even more global framework to account for multiple vehicle fleets, charging stations and more. The latter idea will require to use Deep Reinforcement Learning because states could not be represented as a matrix, and will probably be more difficult (impossible?) to train, but that's a topic for a next article!  </p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">​</a></h2><ul><li>All the code is open sourced <a href="https://github.com/TheoLvs/reinforcement-learning/tree/master/5.%20Delivery%20Optimization" target="_blank" rel="noopener noreferrer">here</a> on Github</li><li>OR-Tools <a href="https://developers.google.com/optimization/routing" target="_blank" rel="noopener noreferrer">open source library</a> by Google</li><li>MOOC on <a href="https://www.coursera.org/learn/discrete-optimization" target="_blank" rel="noopener noreferrer">discrete optimization</a> by the University of Melbourne on Coursera</li><li>The timeless <a href="https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;ab_channel=DeepMind" target="_blank" rel="noopener noreferrer">MOOC on Reinforcement Learning</a> by David Silver at Deepmind</li></ul></div>]]></content:encoded>
            <category>Reinforcement Learning</category>
            <category>Logistics</category>
        </item>
        <item>
            <title><![CDATA[Deploying a Python Dash application for beginners]]></title>
            <link>https://ekimetrics.github.io/blog/dash-deployment</link>
            <guid>https://ekimetrics.github.io/blog/dash-deployment</guid>
            <pubDate>Sat, 16 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Learn the basics of application deployment from simple examples on Heroku to creating Docker containers]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/dash-deployment-9915c614706ee5305450f6d12f8bce23.jpg" width="1050" height="700" class="img_ev3q"></p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="creating-a-dash-application">Creating a Dash application<a href="#creating-a-dash-application" class="hash-link" aria-label="Direct link to Creating a Dash application" title="Direct link to Creating a Dash application">​</a></h2><p>Before deployment, the first step if of course to create your own application. <br>
You can follow the guidelines in Dash official documentation <a href="https://dash.plotly.com/installation" target="_blank" rel="noopener noreferrer">https://dash.plotly.com/installation</a></p><p><img loading="lazy" src="https://dash-gallery.plotly.host/Manager/apps_data/dash-oil-and-gas/thumbnail_0a718df0-9ce7-11e9-8982-0242ac11004a.png" class="img_ev3q"></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="deploying-your-dash-application">Deploying your Dash application<a href="#deploying-your-dash-application" class="hash-link" aria-label="Direct link to Deploying your Dash application" title="Direct link to Deploying your Dash application">​</a></h2><p><a href="https://dash.plotly.com/deployment" target="_blank" rel="noopener noreferrer">https://dash.plotly.com/deployment</a>
Dash/Plotly offers a paid service to super easily deploy and manager your applications. Yet as most of it is open source, and you may want a simple thing for a prototype, you can simply deploy it in your own server.</p><p>Then you have several options: </p><ul><li>Beginners - Deploy it on a simple Heroku server</li><li>Advanced - Deploy it on a cloud server (AWS, GCP, Azure) with docker containers</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="deploying-on-heroku-from-github">Deploying on Heroku from GitHub<a href="#deploying-on-heroku-from-github" class="hash-link" aria-label="Direct link to Deploying on Heroku from GitHub" title="Direct link to Deploying on Heroku from GitHub">​</a></h3><p>Heroku is the most simple server provider. It's even more simpleYou can create and deploy apps for free in just a few minutes. That's what we are going to do here : </p><ul><li>Create your dash app, eg <code>app.py</code> (from Dash tutorial)</li></ul><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># -*- coding: utf-8 -*-</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Run this app with `python app.py` and</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># visit http://127.0.0.1:8050/ in your web browser.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> dash</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> dash_core_components </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token plain"> dcc</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> dash_html_components </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token plain"> html</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> plotly</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">express </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token plain"> px</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> pandas </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token plain"> pd</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">external_stylesheets </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">'https://codepen.io/chriddyp/pen/bWLwgP.css'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">app </span><span class="token operator">=</span><span class="token plain"> dash</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Dash</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">__name__</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> external_stylesheets</span><span class="token operator">=</span><span class="token plain">external_stylesheets</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">server </span><span class="token operator">=</span><span class="token plain"> app</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">server</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># assume you have a "long-form" data frame</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># see https://plotly.com/python/px-arguments/ for more options</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">df </span><span class="token operator">=</span><span class="token plain"> pd</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">DataFrame</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">"Fruit"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"Apples"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"Oranges"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"Bananas"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"Apples"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"Oranges"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"Bananas"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">"Amount"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">4</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">4</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">5</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">"City"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"SF"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"SF"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"SF"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"Montreal"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"Montreal"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"Montreal"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">fig </span><span class="token operator">=</span><span class="token plain"> px</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">bar</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">df</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> x</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">"Fruit"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> y</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">"Amount"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> color</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">"City"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> barmode</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">"group"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">app</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">layout </span><span class="token operator">=</span><span class="token plain"> html</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Div</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">children</span><span class="token operator">=</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    html</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">H1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">children</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">'Hello Dash'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    html</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Div</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">children</span><span class="token operator">=</span><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">'''</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">        Dash: A web application framework for Python.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token triple-quoted-string string" style="color:rgb(255, 121, 198)">    '''</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    dcc</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Graph</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token builtin" style="color:rgb(189, 147, 249)">id</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">'example-graph'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        figure</span><span class="token operator">=</span><span class="token plain">fig</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">if</span><span class="token plain"> __name__ </span><span class="token operator">==</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'__main__'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    app</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">run_server</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">debug</span><span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ul><li>Create a requirements file, eg <code>requirements.txt</code>. You can use tools such as <code>pipreqs</code>, <code>pipenv</code> or other environment managers to help you create the right file. </li><li>Don't forget to add if not present the requirements to <code>gunicorn</code> in your requirements file</li><li>Create a Procfile - it's a text file to help Heroku understand what file to be launched on the server. Write in it the following command. </li></ul><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">web: gunicorn app:server</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ul><li>As of <code>2020-12-01</code>, Dash documentation is not totally correct when it comes to deployment on Heroku, you should correctly link the server variable in Python and the declaration in the <code>Procfile</code> :</li></ul><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># Add following line in your app.py script</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">server </span><span class="token operator">=</span><span class="token plain"> app</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">server</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Write the Procfile</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># - app refer to the file name app.py</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># - server refer to the variable name for the Flask Server </span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">web</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> gunicorn app</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">server</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>At the end your repo on Github should look like this:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">app.py</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Procfile</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">requirements.txt</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ul><li><p>Deploy directly on Heroku from GitHub, you can follow the instructions below</p><ul><li><p>Create a new application
<img loading="lazy" src="/assets/images/deploy1-3ff5f56d09525d8a6eaa1a7d28c03d2d.jpg" width="1739" height="160" class="img_ev3q"></p></li><li><p>Find the right name and server region
<img loading="lazy" src="/assets/images/deploy2-27b1b4e8a48326566a151173de137787.jpg" width="969" height="588" class="img_ev3q"></p></li><li><p>Link via github, search for your repo and click on connect
<img loading="lazy" src="/assets/images/deploy3-7baddac8cf539257b25de03c805ac72b.jpg" width="1574" height="801" class="img_ev3q"></p></li><li><p>Deploy manually by clicking on deploy and choosing the right github branch
<img loading="lazy" src="/assets/images/deploy4-9ce5fb1726645caa869d8457f9ac1076.jpg" width="1353" height="222" class="img_ev3q"></p></li><li><p>You can even set up a CI/CD process with auto-deploys by playing with the auto-deploy section
<img loading="lazy" src="/assets/images/deploy5-39060190e5604d2315b16e3faf461ca5.jpg" width="904" height="356" class="img_ev3q"></p></li><li><p>You are all set ! Your app should be live !
<img loading="lazy" src="/assets/images/deploy7-196c9b1582e3de2f3cb9835fd093a566.jpg" width="1148" height="515" class="img_ev3q">
</p></li></ul></li></ul><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="problems-you-can-encounter">Problems you can encounter<a href="#problems-you-can-encounter" class="hash-link" aria-label="Direct link to Problems you can encounter" title="Direct link to Problems you can encounter">​</a></h4><ul><li>Having your app not at the root of the repo, you can use subdir buildpack</li><li>Not linking correctly your server in the Procfile</li><li>Having difficulties to link with a database </li></ul><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>If your app does not work, you can check in the logs why it failed :
<img loading="lazy" src="/assets/images/deploy6-1866d64b76ae518722783bdb6ae673f5.jpg" width="1574" height="590" class="img_ev3q"></p></div></div><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-heroku-is-doing-under-the-hood">What Heroku is doing under the hood<a href="#what-heroku-is-doing-under-the-hood" class="hash-link" aria-label="Direct link to What Heroku is doing under the hood" title="Direct link to What Heroku is doing under the hood">​</a></h4><p>Heroku does a lot for us actually. It detects the technology behind the web server pushed on Heroku (Python, Node, etc...). Looks for a Procfile with instructions on how to launch the server. And knows many things on how to set it up. </p><p>For example for Python servers, it will look first to find a <code>requirements.txt</code> file or <code>pipenv.lock</code> file. For a Node.js server it will look at the <code>package.json</code> and the lock file as well.  </p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="deploying-with-docker">Deploying with Docker<a href="#deploying-with-docker" class="hash-link" aria-label="Direct link to Deploying with Docker" title="Direct link to Deploying with Docker">​</a></h3><p>If you want to better master what you are deploying. You may want to use Docker. It's actually universal and you'll be able to deploy it almost anywhere.<br>
I just google searched "Dockerfile Dash" and found a suitable example as a template <a href="https://github.com/jucyai/docker-dash/blob/master/Dockerfile" target="_blank" rel="noopener noreferrer">https://github.com/jucyai/docker-dash/blob/master/Dockerfile</a><br>
How does it work? You will setup a virtually empty server with nothing but python 3.9 and the application, i.e a Container. </p><div class="language-docker codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-docker codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">FROM python:3.9</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">ENV DASH_DEBUG_MODE True</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">COPY ./app /app</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">WORKDIR /app</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">RUN set -ex &amp;&amp; \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    pip install -r requirements.txt</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">EXPOSE 8050</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">CMD ["python", "app.py"]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Using this Dockerfile you could be able to deploy anywhere from GCP to Azure or even Heroku. </p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="going-further">Going further<a href="#going-further" class="hash-link" aria-label="Direct link to Going further" title="Direct link to Going further">​</a></h2><p>To better deploy, it's always interesting to learn more about what you are manipulating. Here, you have to know that Dash is a wrapper for other technologies put together, and in particular:</p><ul><li>Flask as backend and server</li><li>React as frontend</li><li>Plotly (the python library) for most graphs</li></ul><p>If you want to be a Deployment ninja 🐱‍👤, you may find useful to learn more about Flask and webservers in general. And eventually learn about React. </p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="adding-databases">Adding databases<a href="#adding-databases" class="hash-link" aria-label="Direct link to Adding databases" title="Direct link to Adding databases">​</a></h3><p>What is recommended is to avoid storing your database in the same server. You should approach the problem with a "microservice" mindset, meaning that you should put your database on another server. </p><ul><li>Indeed, it means you have to do another deployment, maybe using Docker again to expose your database correctly</li><li>But, it also decouple your app in production with the database, allowing you for more modularity (requesting the database in other platforms)</li></ul>]]></content:encoded>
            <category>Industrialization</category>
        </item>
        <item>
            <title><![CDATA[Welcome to the new Ekimetrics Tech website!]]></title>
            <link>https://ekimetrics.github.io/blog/welcome</link>
            <guid>https://ekimetrics.github.io/blog/welcome</guid>
            <pubDate>Fri, 15 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Welcome to EkiLab website - the Ekimetrics technology & innovation blog and website!]]></description>
            <content:encoded><![CDATA[<div align="center"><p>  <img loading="lazy" src="https://miro.medium.com/max/700/1*yCcP_ZGS2mhlA9XnFFZDEw.jpeg" alt="screenshot-app " class="img_ev3q"></p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="welcome-to-our-technology-website">Welcome to our technology website!<a href="#welcome-to-our-technology-website" class="hash-link" aria-label="Direct link to Welcome to our technology website!" title="Direct link to Welcome to our technology website!">​</a></h2><div align="justify"><p>We have been working in the Data Science industry for 15 years and are now the biggest pure player in Europe with 250+ data profiles. We have been benefiting from the open source community for a while, and we want to give back to the community by sharing insights on what we've learned over the years:</p><ul><li><a href="/blog">Blog</a> - read articles on various Data topics: from industrialization on cloud platforms to exotic Deep Learning algorithms</li><li><a href="/resources/trainings">Trainings</a> - access to our internal trainings and tutorials we can provide access</li><li><a href="/about">Best practices &amp; convictions</a> - discover our programming best practices, our tech convictions and preferred technologies</li><li><a href="/resources">Hackathons &amp; challenges</a> - immerse yourself in our Data Science hackathons and challenges to test and improve your skills   </li></ul><p>💌 After reading behind the scenes of the Data Science Company, feel free to <a href="mailto:inno@ekimetrics.com" target="_blank" rel="noopener noreferrer">send us a email</a> for any questions or feedbacks! </p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="about-ekimetrics">About Ekimetrics<a href="#about-ekimetrics" class="hash-link" aria-label="Direct link to About Ekimetrics" title="Direct link to About Ekimetrics">​</a></h2><p>Ekimetrics is the first pure player in Data Science in Europe. We operate from Paris, London, New York and Hong Kong with 250+ Data Scientists, Data Engineers, Full Stack Developers, strategy consultants and UX designers. </p><p>We help companies steer their data opportunity, build data capabilities, and deploy actionable solutions, to power up marketing and operational performance, as well as (re)energizing business models. Our primary focus is to deliver immediate business gains, while guaranteeing sustainable data capital for our clients.</p></div>]]></content:encoded>
            <category>Ekimetrics</category>
        </item>
    </channel>
</rss>