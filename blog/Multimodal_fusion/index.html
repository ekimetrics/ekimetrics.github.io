<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Multimodal Deep Learning | Eki.Lab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://ekimetrics.github.io/blog/Multimodal_fusion"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Multimodal Deep Learning | Eki.Lab"><meta data-rh="true" name="description" content="Understand why multimodal deep learning models are more accurate than assembled unimodal models."><meta data-rh="true" property="og:description" content="Understand why multimodal deep learning models are more accurate than assembled unimodal models."><meta data-rh="true" name="keywords" content="data science,deep learning,multimodal,transfer learning,fine tuning,fusion encoder,dual encoder,natural language processing,computer vision,classification"><meta data-rh="true" property="og:image" content="https://ekimetrics.github.io/img/blog/multimodal_fusion.jpg"><meta data-rh="true" name="twitter:image" content="https://ekimetrics.github.io/img/blog/multimodal_fusion.jpg"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2024-04-29T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://www.linkedin.com/in/fguerillon/"><meta data-rh="true" property="article:tag" content="data science,deep learning,multimodal"><link data-rh="true" rel="icon" href="/img/favicon.png"><link data-rh="true" rel="canonical" href="https://ekimetrics.github.io/blog/Multimodal_fusion"><link data-rh="true" rel="alternate" href="https://ekimetrics.github.io/blog/Multimodal_fusion" hreflang="en"><link data-rh="true" rel="alternate" href="https://ekimetrics.github.io/blog/Multimodal_fusion" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Eki.Lab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Eki.Lab Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-124520099-9","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>




<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-124520099-9"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-124520099-9",{anonymize_ip:!0}),gtag("config","G-MQNYE0E8GE",{anonymize_ip:!0})</script><link rel="stylesheet" href="/assets/css/styles.97279a6c.css">
<link rel="preload" href="/assets/js/runtime~main.ea919b1e.js" as="script">
<link rel="preload" href="/assets/js/main.604bb521.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"dark")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><b class="navbar__title text--truncate">Eki.Lab</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">About us</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/about">Ekilab</a></li><li><a class="dropdown__link" href="/about/ekimetrics">Ekimetrics</a></li><li><a class="dropdown__link" href="/about/stack">Technology stack</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Resources</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/resources/">Hackathons</a></li><li><a class="dropdown__link" href="/resources/trainings">Trainings</a></li></ul></div><a href="https://ekimetrics.com/fr/carrieres/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Careers</a></div><div class="navbar__items navbar__items--right"><a href="https://ekimetrics.com/fr/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Ekimetrics website</a><a href="mailto:inno@ekimetrics.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Contact us!<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="searchBox_ZlJk"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input type="search" id="search_input_react" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><main class="col col--9 col--offset-1" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="image" content="https://ekimetrics.github.io/img/blog/multimodal_fusion.jpg"><header><h1 class="title_f1Hy" itemprop="headline">Multimodal Deep Learning</h1><div class="container_mt6G margin-vert--md"><time datetime="2024-04-29T00:00:00.000Z" itemprop="datePublished">April 29, 2024</time> · <!-- -->9 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/fguerillon/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="/img/authors/francois_guerillon.jpg" alt="François GUERILLON"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/fguerillon/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">François GUERILLON</span></a></div><small class="avatar__subtitle" itemprop="description">Multimodal Specialist</small></div></div></div></div></header><div id="__blog-post-container" class="markdown" itemprop="articleBody"><div align="justify"><h1>Part 1: Why we use a fusion weapon</h1></div><div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/Introduction-0c5bea6b0953aca320c988247d219f80.png" width="767" height="607" class="img_ev3q"></p></div><br><div align="justify"><p>Data science (and specifically machine learning) is often seen as made up of different fields of expertise and research: « classical » machine learning (on tabular data), time series analysis &amp; forecasting, computer vision, natural language processing… Indeed, each kind of (input or output) data, which we call <strong>modality</strong> – e.g. image, text, time series –, has specific properties, raises specific challenges, and therefore requires dedicated solutions and evaluation methods. A pastry chef may ignore how to cook a black truffle risotto; a renowned ophthalmologist may deny any knowledge on kidney function; as well, in a siloed view of data science, a computer vision expert might pay no attention to the latest advances in gradient boosting or large language models. </p><p>At <strong>Ekimetrics</strong>, we have abandoned that siloed view: useful and meaningful data often is multimodal, and leveraging the predictive or generative power of several modalities cannot just be a matter of stacking or superficially connecting each field’s solutions. Let us see why this is important. </p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="multimodal-data-is-everywhere">Multimodal data is everywhere<a href="#multimodal-data-is-everywhere" class="hash-link" aria-label="Direct link to Multimodal data is everywhere" title="Direct link to Multimodal data is everywhere">​</a></h2><div align="justify"><p>In many fields of economy and science, data is multimodal, and each modality may contain useful information. Multimodality is not just a theoretic use case! Let us provide three examples: </p><p><img loading="lazy" alt="screenshot-app " src="/assets/images/Ad_Picture-bea3732d610bfa9db711ee0282bbeb13.png" width="650" height="746" class="img_ev3q"></p><ul><li>Product databases (manufacturing, retail, tourism, real estate…) may contain tabular data, product descriptions or other textual information, product photos, time series for price or sales history… Each product has multimodal data. Each modality contains potential predictive information, for instance to forecast future demand or sales.</li><li>Medical records include textual documents, tabular data, images, regular or irregular time series. Here again, for each patient, every modality may contain precious information to help diagnosis or prognosis.</li><li>Advertisements, e-mails, newspaper covers and articles and social media posts are usually bimodal: texts combined with one or several images (and sometimes, texts within images). Both modalities surely have an influence on the impact (clicks, “likes”, sales…) of these signals, and should be addressed when predicting this impact.</li></ul><p>This is why, in our job as data scientists, we must design and implement the best solutions to harness combined data modalities.</p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="a-multimodal-brain-is-better-than-several-unimodal-brains">A multimodal brain is better than several unimodal brains<a href="#a-multimodal-brain-is-better-than-several-unimodal-brains" class="hash-link" aria-label="Direct link to A multimodal brain is better than several unimodal brains" title="Direct link to A multimodal brain is better than several unimodal brains">​</a></h2><div align="justify"><p>Every minute or even second, our human brain reacts, evaluates, and makes decisions after combining and processing several signals of different kinds, gathered from several senses. Each sense (or each modality) is correctly interpreted <strong>thanks to the knowledge of all other signals</strong>:</p><ul><li>To know if the noise I heard on my right reveals a danger, I must concentrate my vision sense to the same direction (maybe turn my head) and look at the source of the noise.</li><li>Imagine that someone shows me a document and gives oral explanations. Something sounds weird in these explanations. To know if my interlocutor is serious or joking, I must stop looking at the document, and look at his/her facial expression.</li><li>To select what to pay attention to in this <a href="https://www.youtube.com/watch?v=KB_lTKZm1Ts" target="_blank" rel="noopener noreferrer">famous awareness test</a>, I must know and understand the written instructions (or game rule).</li><li>As a specialist physician, to avoid missing weak signals in exam results (imagery, blood test history…) and symptoms, I must understand why the patient was sent to me, which means reading the cover letter or medical report, so as to know what to focus on.</li><li>In many bimodal documents, for instance scientifical articles, understanding the images (diagrams, charts) help understanding the text, and reading some texts (e.g. captions) can be necessary to understand the images or to deduce what is important to be watched in the image.</li></ul><p>If all these examples, suppose each modality is observed by a different person (a different unimodal brain), without any communication with the other persons, and afterwards, only afterwards, these persons can communicate and decide, without any access to the modalities. What will happen? They will probably miss something, because they did not look in the right direction, or did not focus enough on some detail, or misinterpreted a signal.</p></div><div align="center"><p><img loading="lazy" alt="screenshot-app " src="/assets/images/Monkeys-572eccaee87d0f6a45c2c5291997e81d.png" width="395" height="270" class="img_ev3q"></p><p>What if the neurons are artificial ones?</p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="unimodal-embeddings-are-not-all-you-need">Unimodal embeddings are not all you need<a href="#unimodal-embeddings-are-not-all-you-need" class="hash-link" aria-label="Direct link to Unimodal embeddings are not all you need" title="Direct link to Unimodal embeddings are not all you need">​</a></h2><div align="justify"><p>In simple (or maybe naive) multimodal deep learning architectures, unimodal state-of-the-art (SoTA) neural networks work independently. Each unimodal brain classically produces a <strong>unimodal embedding</strong>, which is usually a vector (1D-array) or a sequence of vectors (2D-array) of numerical values, which is supposed to concentrate all useful information on the modality, in a reduced mathematical space. Then, a prediction is built from all these gathered embeddings, thanks to the addition of a few more trained neural layers (a “head” block), or with numerical machine learning solutions such as tree boosting. It looks SoTA, but it often induces poor performance.</p><p>The first reason is that each unimodal embedding is given in a different latent space: it means that it uses a language which is completely different from the other unimodal embeddings. Thus, the image embedding does not help guessing what is important to check in the text embedding, and vice versa. In the human scenario (unimodal brains), it would be as if the involved persons could not communicate correctly together: making a good decision would be quite unlikely.</p><p>Fortunately, public pre-trained models and architectures called <strong>dual encoders</strong>, like CLIP<sup id="fnref-1-2e81d8"><a href="#fn-1-2e81d8" class="footnote-ref">1</a></sup> , can produce text embeddings and image embeddings in the same latent space (same language), which significantly helps bridging the gap between modalities, and improve predictive performance.</p><p>But this is not enough. You already know the second reason: even with dual encoders, each unimodal brain (network) does not communicate with the other brains when analyzing its modality, and therefore, does not focus enough on what is important in it. This is why a new family of multimodal models has been designed: <strong>fusion encoders</strong>. They allow, in each modality analysis, to progressively introduced useful information from other modalities. Thus, useful information is jointly extracted from all modalities of a same data sample.</p><p>Pretrained image &amp; text (bimodal) fusion encoders, ingeniously assembling SoTA expertise of both computer vision and natural language processing, are now publicly available: ViLT<sup id="fnref-2-2e81d8"><a href="#fn-2-2e81d8" class="footnote-ref">2</a></sup> , FLAVA<sup id="fnref-3-2e81d8"><a href="#fn-3-2e81d8" class="footnote-ref">3</a></sup> , BLIP<sup id="fnref-4-2e81d8"><a href="#fn-4-2e81d8" class="footnote-ref">4</a></sup> , BLIP-2<sup id="fnref-5-2e81d8"><a href="#fn-5-2e81d8" class="footnote-ref">5</a></sup> … They can output <strong>bimodal embeddings</strong>, which contain rich relevant information from a pair {image, text}, analyzed together. They are excellent candidates for “transfer learning” strategies: adding a trained “head” network on top of a fine-tuned fusion encoder, to make the desired prediction or forecast.</p></div><div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/BLIP-a4096d65e4dda89ff8514e24d609aeb2.png" width="480" height="217" class="img_ev3q"></p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="a-demonstration-by-ekimetrics">A demonstration by Ekimetrics<a href="#a-demonstration-by-ekimetrics" class="hash-link" aria-label="Direct link to A demonstration by Ekimetrics" title="Direct link to A demonstration by Ekimetrics">​</a></h2><div align="justify"><p>Huge product databases sometimes suffer from insufficient or irregular quality, due to historical or heterogeneous (sometimes manual) feeding processes. For instance, an irrelevant picture (product photo) may have been associated to an unmatching product. Manual thorough detection of errors, on millions of products, would be extremely costly. At Ekimetrics Innovation Lab (Eki.Lab), we have designed, built, evaluated, and validated a deep learning solution to efficiently semi-automate that task: it can detect most mismatches between an image and the known text fields (name, properties, description, summary…) of the same product.</p><p>Our most efficient solution mainly relies on a fine-tuned fusion encoder and trained head. Even with artificially generated hard-to-find image permutations (e.g. replacing an image with a similar but slightly different one) in test database, our solution can detect 87% (recall) of existing mismatches, with a tolerated precision of 50% (i.e. allowing that that only 1 suspicious image out of 2 is actually incorrect). On top of that, we have designed and validated a method to adapt the model to any unlabeled product database (self-supervision): costly human labelling is unnecessary.</p><p>Our experiments confirm that without a fusion encoder, and even using CLIP as a dual encoder, results are much worse: recall remains under 30% with the same constraint on precision.</p></div><div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/Fusion_encoder-761c366ece8019ab9debcfc288ef235d.png" width="499" height="263" class="img_ev3q"></p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-about-tabular-data-and-time-series">What about tabular data and time series?<a href="#what-about-tabular-data-and-time-series" class="hash-link" aria-label="Direct link to What about tabular data and time series?" title="Direct link to What about tabular data and time series?">​</a></h2><div align="justify"><p>Now, what would be the best solution for a predictive task in which multimodal input includes tabular data and/or time series? In most cases, there is no relevant pre-trained fusion encoder on these modalities, since there is no universal knowledge to deduce from pre-training on time series or tabular data. However, a few recent research papers, such as the Perceiver family (Perceiver<sup id="fnref-6-2e81d8"><a href="#fn-6-2e81d8" class="footnote-ref">6</a></sup> , Perceiver IO<sup id="fnref-7-2e81d8"><a href="#fn-7-2e81d8" class="footnote-ref">7</a></sup> , Hierarchical Perceiver<sup id="fnref-8-2e81d8"><a href="#fn-8-2e81d8" class="footnote-ref">8</a></sup> ), show promising ways to build and train an accurate unique (fusion) deep learning model based on any combination of various modalities.</p><p>Combining such a “multimodal-by-design” architecture with pre-trained models (including fusion encoders) for both text and images, which may still bring useful knowledge with small-sized training datasets, is an active field of research<sup id="fnref-9-2e81d8"><a href="#fn-9-2e81d8" class="footnote-ref">9</a></sup> , in which Ekimetrics is fully involved. A high range of disruptive use cases can be designed: in the near future, efficient multimodal models will help optimize media communication, forecast demand or sales to optimize product design or supply chain, forecast or optimize prices, help diagnosis or prognosis, perform impactful social media analysis… Please stay connected for future news on the subject!</p></div><div class="footnotes"><hr><ol><li id="fn-1-2e81d8">« Learning Transferable Visual Models From Natural Language Supervision », A. Radford et al., 2021<a href="#fnref-1-2e81d8" class="footnote-backref">↩</a></li><li id="fn-2-2e81d8">« ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision », W. Kim &amp; al., 2021<a href="#fnref-2-2e81d8" class="footnote-backref">↩</a></li><li id="fn-3-2e81d8">« FLAVA: A Foundational Language And Vision Alignment Model », A. Singh, 2022<a href="#fnref-3-2e81d8" class="footnote-backref">↩</a></li><li id="fn-4-2e81d8">« BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation », J. Li et al., 2022<a href="#fnref-4-2e81d8" class="footnote-backref">↩</a></li><li id="fn-5-2e81d8">« BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models », J. Li et al., 2023<a href="#fnref-5-2e81d8" class="footnote-backref">↩</a></li><li id="fn-6-2e81d8">« Perceiver: General Perception with Iterative Attention », A. Jaegle et al., 2021<a href="#fnref-6-2e81d8" class="footnote-backref">↩</a></li><li id="fn-7-2e81d8">« Perceiver IO : a general architecture for structured inputs &amp; outputs », A. Jaegle et al., 2021<a href="#fnref-7-2e81d8" class="footnote-backref">↩</a></li><li id="fn-8-2e81d8">« HiP: Hierarchical Perceiver », J. Carreira et al., 2022<a href="#fnref-8-2e81d8" class="footnote-backref">↩</a></li><li id="fn-9-2e81d8">For instance, see « Flamingo: a Visual Language Model for Few-Shot Learning », J.B. Alayrac et al., 2022<a href="#fnref-9-2e81d8" class="footnote-backref">↩</a></li></ol></div></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_mRVl"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/data-science">data science</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/deep-learning">deep learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/multimodal">multimodal</a></li></ul></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/Bayesian_NN"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">Bayesian Neural Networks</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/Uncertainty_TS"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Uncertainty in time series forecasting</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#multimodal-data-is-everywhere" class="table-of-contents__link toc-highlight">Multimodal data is everywhere</a></li><li><a href="#a-multimodal-brain-is-better-than-several-unimodal-brains" class="table-of-contents__link toc-highlight">A multimodal brain is better than several unimodal brains</a></li><li><a href="#unimodal-embeddings-are-not-all-you-need" class="table-of-contents__link toc-highlight">Unimodal embeddings are not all you need</a></li><li><a href="#a-demonstration-by-ekimetrics" class="table-of-contents__link toc-highlight">A demonstration by Ekimetrics</a></li><li><a href="#what-about-tabular-data-and-time-series" class="table-of-contents__link toc-highlight">What about tabular data and time series?</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">About us</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://ekimetrics.com/who-we-are/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Who we are ?</a></li><li class="footer__item"><a href="https://ekimetrics.com/our-team/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Our team</a></li><li class="footer__item"><a href="https://ekimetrics.us13.list-manage.com/subscribe?u=85b8ce42caa0a733e98233bc4&amp;id=6355d0a6f9" target="_blank" rel="noopener noreferrer" class="footer__link-item">Subscribe to our newsletter</a></li></ul></div><div class="col footer__col"><div class="footer__title">Find us</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/ekimetrics" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://ekimetrics.com/careers/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Careers<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.welcometothejungle.com/fr/companies/ekimetrics" target="_blank" rel="noopener noreferrer" class="footer__link-item">Eki on Welcome to the jungle<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Contact</div><ul class="footer__items clean-list"><li class="footer__item"><a href="mailto:inno@ekimetrics.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Get in touch with our teams<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div></div></footer></div>
<script src="/assets/js/runtime~main.ea919b1e.js"></script>
<script src="/assets/js/main.604bb521.js"></script>
</body>
</html>