<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">How to give sight to a blind language model | Eki.Lab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://ekimetrics.github.io/blog/Blind_Language_Model"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="How to give sight to a blind language model | Eki.Lab"><meta data-rh="true" name="description" content="Get a good technical understanding of how vision-language models are built and trained."><meta data-rh="true" property="og:description" content="Get a good technical understanding of how vision-language models are built and trained."><meta data-rh="true" name="keywords" content="GenAI,Generative AI,Deep Learning,Machine learning,Data Science,AI Models,LLM,Multimodal,Object Detection,Computer Vision,VLM,Vision Language Models,Transformers,Attention"><meta data-rh="true" property="og:image" content="https://ekimetrics.github.io/img/blog/Blind_Language_header.jpg"><meta data-rh="true" name="twitter:image" content="https://ekimetrics.github.io/img/blog/Blind_Language_header.jpg"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-07-23T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://www.linkedin.com/in/fguerillon/"><meta data-rh="true" property="article:tag" content="GenAI,Generative AI,Deep Learning,Machine learning,Data Science,AI Models,LLM,Multimodal,Object Detection,Computer Vision"><link data-rh="true" rel="icon" href="/img/favicon.png"><link data-rh="true" rel="canonical" href="https://ekimetrics.github.io/blog/Blind_Language_Model"><link data-rh="true" rel="alternate" href="https://ekimetrics.github.io/blog/Blind_Language_Model" hreflang="en"><link data-rh="true" rel="alternate" href="https://ekimetrics.github.io/blog/Blind_Language_Model" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Eki.Lab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Eki.Lab Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-124520099-9","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>




<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-124520099-9"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-124520099-9",{anonymize_ip:!0}),gtag("config","G-MQNYE0E8GE",{anonymize_ip:!0})</script><link rel="stylesheet" href="/assets/css/styles.47419826.css">
<link rel="preload" href="/assets/js/runtime~main.0e3c3f04.js" as="script">
<link rel="preload" href="/assets/js/main.73cebcbb.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"dark")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><b class="navbar__title text--truncate">.</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/publications">Publications</a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">About us</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/about">Ekilab</a></li><li><a class="dropdown__link" href="/about/ekimetrics">Ekimetrics</a></li><li><a class="dropdown__link" href="/about/stack">Technology stack</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Resources</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/resources/trainings">Trainings</a></li><li><a class="dropdown__link" href="/resources/">Hackathons</a></li></ul></div><a href="https://www.ekimetrics.com/fr/join-ekimetrics" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Careers</a></div><div class="navbar__items navbar__items--right"><a href="https://ekimetrics.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Ekimetrics website</a><a href="mailto:inno@ekimetrics.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Contact us!<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="searchBox_ZlJk"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input type="search" id="search_input_react" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><main class="col col--9 col--offset-1" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="image" content="https://ekimetrics.github.io/img/blog/Blind_Language_header.jpg"><header><h1 class="title_f1Hy" itemprop="headline">How to give sight to a blind language model</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-07-23T00:00:00.000Z" itemprop="datePublished">July 23, 2025</time> · <!-- -->15 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/fguerillon/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="/img/authors/francois_guerillon.jpg" alt="François GUERILLON"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/fguerillon/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">François GUERILLON</span></a></div><small class="avatar__subtitle" itemprop="description">Multimodal Specialist</small></div></div></div></div></header><div id="__blog-post-container" class="markdown" itemprop="articleBody"><div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/optician-82a115bbd844bc56de078ad6c92a8535.jpg" width="483" height="427" class="img_ev3q"></p></div><div align="justify"><p><em>Note: this technical article is intended for data scientists and machine learning engineers having a good understanding of deep learning fundamentals and a basic knowledge of transformers and language models. If too long for you, just have a look at <a href="#conclusion">conclusion</a> :-)</em></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h2><p>A notable trend we have observed in generative AI for the last few years is the development of <strong>vision-language models (VLMs)</strong> which are just special versions of language models with ability to receive one – sometimes, several – image(s) as a new input, in addition to the textual input. Most ambitious closed-weight large language models (e.g. GPT-4o, Gemini Pro) now include vision capacities. Many “blind” open-weight language models have been used as starting points for new VLMs having more and more image understanding skills, such as LLaVA <a href="#1">[1]</a>, CogVLM <a href="#2">[2]</a>, PaliGemma <a href="#3">[3]</a>, Phi-3-vision <a href="#4">[4]</a>, Llama-3.2 <a href="#5">[5]</a> and Pixtral 12B <a href="#6">[6]</a>.</p><p>Although the performance of VLMs across different vision tasks may be very irregular and sometimes disapointing (see for instance the <a href="https://huggingface.co/spaces/opencompass/open_vlm_leaderboard" target="_blank" rel="noopener noreferrer">OpenVLM Leaderboard</a>), they are a new field of hope for various use cases such as accessibility improvement through image captioning, automated web user interface coding from mockup images, zero-shot rich feature extraction from images for machine learning, for retrieval-augmented generation or product catalog automation...</p><p>In this article, we will not focus on potential use cases, but we will help you get a good (though certainly not comprehensive) and technical understanding of how vision-language models are built.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="ingredients">Ingredients<a href="#ingredients" class="hash-link" aria-label="Direct link to Ingredients" title="Direct link to Ingredients">​</a></h2><p>A common choice in reference papers we have gathered for this review (see last section: <a href="#references">references</a>), to build a VLM, is to combine two already trained models: a blind <strong>language model</strong> and an <strong>image encoder</strong>.</p><p>The <strong>trained language model</strong> has a transformer decoder architecture, and has previously been trained in multiple steps, to be able to produce a relevant text, given an input text or sequence of texts, which may be an instruction, context data, chat history... Text generation is an iterative process: at inference time, the text is generated token per token (a token is a word or part of a word). At each generation iteration, the input of the model is made up of all prompted and previously generated tokens; each input token is converted to a token embedding (learnt during training, for all possible tokens in the token dictionary); then all token embeddings are analyzed and updated thanks to multiple transformer blocks. These blocks include <strong>masked self-attention</strong> layers, so that the model, to predict next token, can decide to pay different attention to any of the previous tokens.</p></div><div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/language_model-4f24067a231c10898d27337e3b2fa761.png" width="1391" height="902" class="img_ev3q"></p></div><div align="justify"><p>The <strong>trained image encoder</strong> has a transformer encoder architecture. As an input, it receives one image, which is divided into small patches (smaller images), as designed for Vision transformers <a href="#14">[14]</a>. These patches are gathered in a single sequence, and then go through a stack of transformer blocks which include a self-attention mechanism (attention between image patches at different positions in the original image). The output is a sequence of embeddings: one for each patch, which can be seen as a representation of the patch in the context of all other patches.</p></div><div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/image_encoder-bb4833b28af59d83767f3fbd1a2c5cbf.png" width="933" height="904" class="img_ev3q"></p></div><div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="cross-attention-vs-self-attention">Cross-attention vs. self-attention<a href="#cross-attention-vs-self-attention" class="hash-link" aria-label="Direct link to Cross-attention vs. self-attention" title="Direct link to Cross-attention vs. self-attention">​</a></h2><p>The first question you may ask is: how to plug these two deep learning models together? Well, two alternative solutions are implemented, and you know them: <strong>cross-attention</strong> vs. <strong>self-attention</strong>.</p><p><strong>Cross-attention</strong>, is the now commonly used name for encoder-decoder attention, as defined in the original transformer <a href="#7">[7]</a>. It is a way for a transformer decoder, at each position of its input sequence, to selectively pay attention to some of the embeddings produced by a separate encoder: it is the normal connection mode between a transformer decoder and a transformer encoder. So it seems to be the most natural way to teach the language model (decoder-only transformer) to pay attention to the image embeddings produced by the image encoder. Flamingo <a href="#8">[8]</a>, OpenFlamingo <a href="#13">[13]</a> and LLama 3.2 <a href="#5">[5]</a> apply this cross-attention solution: cross-attention layers are inserted in the transformer blocks of the blind language model. Note that cross-attention is asymetrical: the representation of the image is not influenced by the text content.</p><p>Note that with cross-attention between text and image, positional embeddings and self-attention masking strategies, previously defined in the image encoder and in the language model, are fully preserved. However, for interleaved texts and (possibly multiple) images, as in Flamingo <a href="#8">[8]</a>, a special masking strategy can be defined in cross-attention, so that text tokens may pay attention to an image only if the image is before that text token in the interleaved content.</p></div><div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/cross_attention-c8c3b427d5b8e496f5b573913b58c66e.png" width="1877" height="905" class="img_ev3q"></p></div><div align="justify"><p><strong>Self-attention</strong> between text tokens and image patches seems counterintuitive, since image patch embeddings and text token embeddings are two different sequences in two different representation spaces. Yet, self-attention is the most commonly used way to condition text generation with information from image: it is applied in BLIP-2 <a href="#9">[9]</a>, LLaMA-Adapter <a href="#10">[10]</a>, MiniGPT-4 <a href="#11">[11]</a>, LLaVA-NeXT <a href="#1">[1]</a>, CogVLM2 <a href="#2">[2]</a>, PaliGemma <a href="#3">[3]</a>, Phi-3-vision <a href="#4">[4]</a>, Pixtral <a href="#6">[6]</a>... The “trick” to make this possible is to add and train an <strong>adapter</strong> that converts the image patch embeddings into text token embeddings, which are inserted into the text sequence before the full sequence goes through all (or some) transformer blocks of the language model; different kinds of adapters have been experimented (see next section). After that conversion, the inserted converted embeddings are addressed just like text token embeddings, in the same self-attention layers and feed-forward (FF) layers of the language model; except for CogVLM <a href="#2">[2]</a> models, which use dedicated self-attention and FF weights for sequence positions coming from the image.</p><p>Adopting this self-attention solution requires making decisions on the image token embeddings which are inserted in the text sequence: which position to assign them for position embedding? Must they follow a causal mask as for text tokens (i.e. each token can only see previous tokens in self-attention computation)? Only a few papers mentionned here are explicit on their choices. For instance, CogVLM <a href="#2">[2]</a> relies on a causal mask for image tokens, but for position embedding, all tokens from image share a single position identifier. In Pixtral <a href="#6">[6]</a>, the image tokens are treated identically to the text tokens, including RoPE-1D positional encodings for all tokens, and causal self-attention. On the contrary, PaliGemma <a href="#3">[3]</a> avoids causal masking on image tokens. </p></div><div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/self_attention-7d06c17229475633bc7d258e5508a1c7.png" width="1913" height="934" class="img_ev3q"></p></div><div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="vision-and-language-alignment">Vision and language alignment<a href="#vision-and-language-alignment" class="hash-link" aria-label="Direct link to Vision and language alignment" title="Direct link to Vision and language alignment">​</a></h2><p>Image patch embeddings hold information in a representation space which is specific to the trained image encoder. Image encoders are usually trained on computer vision tasks, so they are not natively optimized for the job of providing useful information to a language model. So, how to ensure that the image encoder brings maximal useful information for text generation, for instance on various image-to-text tasks?</p><p>First intuition to reduce the risk of misalignment (misunderstanding) between the image encoder and the language model is to choose the most appropriate image encoder. Almost all studied VLMs, except Pixtral 12B <a href="#6">[6]</a>, rely on a <strong>“CLIP-like” image encoder</strong>. CLIP is a pair of image encoder and text encoder, which were jointly trained to embed any image and its caption (text) to very close representations in a same representation space. Therefore, we can consider that the image patch embeddings produced by CLIP image encoder contain almost the same information as would contain a good caption of the image. CLIP original paper <a href="#12">[12]</a> has inspired other scientific works based on the same image-text contrastive learning principle: we call them “CLIP-like”. With this training recipe, which “aligns” image representation and text representation, the image representation of CLIP-like models is expected to be more easily understood by a language model, or more easily convertible to text token embeddings.</p></div><div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/clip-6ea48ee674d14a5fe472d472aacf5245.png" width="1641" height="738" class="img_ev3q"></p></div><div align="justify"><p>Second intuition is that <strong>the image representation must be adapted to tasks with vision during VLM training</strong>, not only in VLMs based on self-attention (where an adapter is required), but also in those based on cross-attention between text and image. This adaptation principle is generalized; yet, adaptation strategy is very different between papers. In some earlier works such as BLIP-2 <a href="#9">[9]</a> and MiniGPT-4 <a href="#11">[11]</a>, a complex adapter (named “Q-Former”) is trained alone, during dedicated training steps (including on image-text matching and image-text contrastive training tasks), to convert the output of the image encoder into a new representation, fully optimized for text models. Other and more recent papers choose a simpler adapter, from a (still complex) Perceiver resampler in Flamingo <a href="#8">[8]</a>, to a classical multi-layer perceptron in LLaVA-NeXT <a href="#1">[1]</a>, CogVLM2 <a href="#2">[2]</a> and Pixtral 12B <a href="#6">[6]</a>, or even a mere linear projection in LLaVA <a href="#1">[1]</a>, LLaMA-Adapter <a href="#10">[10]</a> and PaliGemma <a href="#3">[3]</a>.</p><p>As for LLama 3.2 <a href="#5">[5]</a>, it relies on a different adaptation method: new trainable gated (i.e. inactivated before training) self-attention layers are inserted inside the image encoder, and progressively activated and trained during training on vision-language tasks.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="train-train-and-train-again">Train, train, and train again<a href="#train-train-and-train-again" class="hash-link" aria-label="Direct link to Train, train, and train again" title="Direct link to Train, train, and train again">​</a></h2><p>Okay ladies and gentlemen, we have solved the architecture questions: our language model is now fully connected to the trained image encoder. Now, how to fine-tune this vision-language model to ensure it can satisfy users on various image-text tasks, as if the language model had gained general vision abilities?</p><p>Well, the solutions implemented in our reference papers are widely inspired from state-of-the-art language model training strategies. The principle is to design and implement a path of various training tasks, starting with the simpler ones, and then jumping to more complex jobs.</p><p>More precisely, here are the most common or interesting <strong>supervised vision-language training tasks</strong> (or task families) for VLMs. Note that all of them just aim at improving next text token prediction:</p><ul><li><p><strong>Image captioning</strong>: used for most VLMs, such as BLIP-2 <a href="#9">[9]</a>, LLaMA-Adapter <a href="#10">[10]</a>, LLaVA &amp; LLaVA-NeXT <a href="#1">[1]</a>, CogVLM 1 &amp; 2 <a href="#2">[2]</a>, PaliGemma <a href="#3">[3]</a>, Phi-3-vision <a href="#4">[4]</a>, LLama 3.2 <a href="#5">[5]</a>.</p></li><li><p><strong>Optical character recognition</strong> (OCR): used for CogVLM 1 &amp; 2 <a href="#2">[2]</a>, PaliGemma <a href="#3">[3]</a>, LLaVA-NeXT <a href="#1">[1]</a>, Phi-3-vision <a href="#4">[4]</a>, LLama 3.2 <a href="#5">[5]</a>.</p></li><li><p>VQA i.e. <strong>visual question answering</strong> (answer questions regarding the provided image): used for BLIP-2 <a href="#9">[9]</a>, LLaMA-Adapter <a href="#10">[10]</a>, PaliGemma <a href="#3">[3]</a>.</p></li><li><p>Various <strong>visual grounding</strong> tasks, in which the model learns to understand or generate references to positions inside an image, such as “grounded captioning”, “referring expression generation”, “referring expression comprehension”, “grounded VQA”... : used for CogVLM 1 &amp; 2 <a href="#2">[2]</a>, PaliGemma <a href="#3">[3]</a>, LLama 3.2 <a href="#5">[5]</a>.</p></li><li><p>Reverse-engineering, to generate image source in <strong>HTML, Markdown or Latex</strong> : used for LLama 3.2 <a href="#5">[5]</a>.</p></li><li><p>Next text token prediction on <strong>interleaved content</strong> (text interleaved with one or several images, e.g. simplified web pages): used for Flamingo <a href="#8">[8]</a>, Phi-3-vision <a href="#4">[4]</a>. When several images are allowed, this can include training for <strong>in-context learning</strong>, which means that in training data, the text input (prompt or instruction) can include one or several examples of image paired with the expected answer, before inserting the image on which the model must answer.</p></li><li><p><strong>Multi-turn conversations</strong> (chat) with image input, including <strong>instruction following</strong>, <strong>image understanding</strong>, multi-turn VQA, OCR, reasoning...: used for LLaVA &amp; LLaVA-NeXT <a href="#1">[1]</a>, CogVLM 1 &amp; 2 <a href="#2">[2]</a>, Phi-3-vision <a href="#4">[4]</a>, LLama 3.2 <a href="#5">[5]</a>.</p></li><li><p><strong>&quot;Safety&quot; training</strong>, i.e. train to generate best answer among multiple candidate answers, as selected by human labellers: used for Phi-3-vision <a href="#4">[4]</a>.</p></li></ul><p>Moreover, additional fine-tuning steps based on <strong>reinforcement learning with human feedback</strong> (RLHF) can be added to increase (human) user satisfaction, for instance on VQA and chat with vision: this is applied for Phi-3-vision <a href="#4">[4]</a> and LLama 3.2 <a href="#5">[5]</a>.</p><p>Available bimodal (paired text and image) training data is probably not as big and diverse as unimodal data (text only or image only), which may induce risks of <strong>overfitting</strong> during these bimodal training steps. Unfortunately, most papers studied here are not verbose on solutions against overfitting. Dropouts, weight decays seem commonly used and tuned; yet, Llava 1.5 <a href="#1">[1]</a> avoids weight decay. Image augmentation is studied but not generalized: for instance, BLIP-2 <a href="#9">[9]</a> benefits from random resized cropping and horizontal flipping, and Flamingo <a href="#8">[8]</a> adds color augmentation; but PaliGemma <a href="#3">[3]</a> just relies on label smoothing and dropout, without any data augmentation, which is proven to have no significant benefit. Llama-3.2 <a href="#5">[5]</a> goes further with a model soup: a &quot;hyperparameter sweep&quot; is run with multiple random data subsets and hyperparameter combinations, then the models are ranked, and finally, the weights of best models are averaged to obtain the final model.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="freeze-or-dont-freeze">Freeze or don&#x27;t freeze?<a href="#freeze-or-dont-freeze" class="hash-link" aria-label="Direct link to Freeze or don&#x27;t freeze?" title="Direct link to Freeze or don&#x27;t freeze?">​</a></h2><p>We have understood that the most impressive VLMs, such as LLama 3.2 <a href="#5">[5]</a>, have followed a quite long chain of training steps, with different task(s) at each step. We know that bimodal (vision-language) training steps follow the unimodal (language-only and vision-only) ones, that had previously been implemented to train the blind language model and the image encoder. But a question remains: how the ensure that during vision-language training, the VLM does not (catastrophically) forget what its two components (image encoder, language model) had correctly learnt?</p><p>The answer is in the <strong>freezing strategy</strong> chosen for each model. Freezing the weights of both the image encoder and the language model, while training only the adapter and new layers added when building the VLM, is a secure way to avoid catastrophic forgetting, and to reduce risks of overfitting on images: it was adopted in earlier papers (Flamingo <a href="#8">[8]</a>, LLaMA-Adapter <a href="#10">[10]</a>, MiniGPT-4 <a href="#11">[11]</a>). However, total freezing probably reduces the ability of the VLM to maximize its performance on new vision-language tasks. Therefore, more recent works have experimented, with success, various unfreezing strategies. An intermediate strategy is to freeze the 2 initial models during the first vision-language training steps, and to unfreeze them (or one of the two) in the last training steps, as explained for BLIP-2 <a href="#9">[9]</a>, LLaVA &amp; LLaVA-NeXT <a href="#1">[1]</a>, CogVLM 1 &amp; 2 <a href="#2">[2]</a>. Some research teams go further: in LLama 3.2 <a href="#5">[5]</a>, the image encoder is not frozen at all, whilst the language model is unfrozen after a few vision-language training steps. More impressively, PaliGemma <a href="#3">[3]</a> is trained without any freezing.</p><p>Additionally, to preserve performance on text-only tasks, some papers explain that some vision-language training steps include language-only data, for instance CogVLM 2 <a href="#2">[2]</a> and Phi-3-vision <a href="#4">[4]</a>.</p><p>Here is a simplified summary of the freezing strategies:</p></div><div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/freezing_strategy-6dcde058682b0bb512d88259f19e8e0b.png" width="1892" height="1061" class="img_ev3q"></p></div><div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><p>Vision language models, and more generally multimodal generative AI, are a very active field of research: we have lifted the hood of some foundational or recent scientific papers, with no claim to exhaustiveness. To sum up what we have observed in this article, we may remember that:</p><ul><li><p>An already trained language model can be “converted” to a vision language model (VLM) thanks to image encoder plugging and dedicated additional training steps, without significant decrease of its language abilities. Thus most VLMs defined in state-of-the-art papers are based on a previously trained blind (unimodal) language model, and rely on this “conversion” process. </p></li><li><p>Plugging the image encoder may rely on cross-attention, or on self-attention with an adapter; there is currently no clear winner between these two alternative options.</p></li><li><p>Simple deep learning layers can be applied to compensate initial misalignment between image representation and text representation.</p></li><li><p>Rich and various training tasks (and of course, training data) are essential to get a performant VLM, and are presumably more important than architecture details.</p></li><li><p>Most advanced recipes to build and train a VLM rely on progressive vision-language training (up to six steps) with progressive unfreezing of the language model.</p></li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">​</a></h2><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="1">[1]<a href="#1" class="hash-link" aria-label="Direct link to [1]" title="Direct link to [1]">​</a></h4><p><a href="https://arxiv.org/abs/2304.08485" target="_blank" rel="noopener noreferrer">Visual instruction tuning</a> (H. Liu et al., 2023), <a href="https://arxiv.org/abs/2310.03744" target="_blank" rel="noopener noreferrer">Improved Baselines with Visual Instruction Tuning</a> (H. Liu et al., 2024)</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="2">[2]<a href="#2" class="hash-link" aria-label="Direct link to [2]" title="Direct link to [2]">​</a></h4><p><a href="https://arxiv.org/abs/2311.03079" target="_blank" rel="noopener noreferrer">CogVLM: Visual Expert for Pretrained Language Models</a> (W. Wang et al., 2023), <a href="https://arxiv.org/abs/2408.16500" target="_blank" rel="noopener noreferrer">CogVLM2: Visual Language Models for Image and Video Understanding</a> (W. Hong et al., 2024)</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="3">[3]<a href="#3" class="hash-link" aria-label="Direct link to [3]" title="Direct link to [3]">​</a></h4><p><a href="https://arxiv.org/abs/2407.07726" target="_blank" rel="noopener noreferrer">PaliGemma: A versatile 3B VLM for transfer</a> (L. Beyer et al., 2024)</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="4">[4]<a href="#4" class="hash-link" aria-label="Direct link to [4]" title="Direct link to [4]">​</a></h4><p><a href="https://arxiv.org/abs/2404.14219" target="_blank" rel="noopener noreferrer">Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone</a> (M. Abdin et al., 2024)</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="5">[5]<a href="#5" class="hash-link" aria-label="Direct link to [5]" title="Direct link to [5]">​</a></h4><p><a href="https://arxiv.org/abs/2407.21783" target="_blank" rel="noopener noreferrer">The Llama 3 Herd of Models</a> (A. Grattafiori et al., 2024)</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="6">[6]<a href="#6" class="hash-link" aria-label="Direct link to [6]" title="Direct link to [6]">​</a></h4><p><a href="https://arxiv.org/abs/2410.07073" target="_blank" rel="noopener noreferrer">Pixtral 12B</a> (P. Agrawal et al., 2024)</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="7">[7]<a href="#7" class="hash-link" aria-label="Direct link to [7]" title="Direct link to [7]">​</a></h4><p><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">Attention is all you need</a> (A. Vaswani et al., 2017)</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="8">[8]<a href="#8" class="hash-link" aria-label="Direct link to [8]" title="Direct link to [8]">​</a></h4><p><a href="https://arxiv.org/abs/2204.14198" target="_blank" rel="noopener noreferrer">Flamingo: a Visual Language Model for Few-Shot Learning</a> (J.-B. Alayrac et al., 2022)</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="9">[9]<a href="#9" class="hash-link" aria-label="Direct link to [9]" title="Direct link to [9]">​</a></h4><p><a href="https://arxiv.org/abs/2301.12597" target="_blank" rel="noopener noreferrer">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a> (J. Li et al., 2023)</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="10">[10]<a href="#10" class="hash-link" aria-label="Direct link to [10]" title="Direct link to [10]">​</a></h4><p><a href="https://arxiv.org/abs/2303.16199" target="_blank" rel="noopener noreferrer">LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention</a> (R. Zhang et al., 2023)</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="11">[11]<a href="#11" class="hash-link" aria-label="Direct link to [11]" title="Direct link to [11]">​</a></h4><p><a href="https://arxiv.org/abs/2304.10592" target="_blank" rel="noopener noreferrer">MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models</a> (D. Zhu et al., 2023)</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="12">[12]<a href="#12" class="hash-link" aria-label="Direct link to [12]" title="Direct link to [12]">​</a></h4><p><a href="https://arxiv.org/abs/2103.00020" target="_blank" rel="noopener noreferrer">Learning Transferable Visual Models From Natural Language Supervision</a> (A. Radford et al., 2021)</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="13">[13]<a href="#13" class="hash-link" aria-label="Direct link to [13]" title="Direct link to [13]">​</a></h4><p><a href="https://arxiv.org/abs/2308.01390" target="_blank" rel="noopener noreferrer">OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models</a> (A. Awadalla et al., 2023)</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="14">[14]<a href="#14" class="hash-link" aria-label="Direct link to [14]" title="Direct link to [14]">​</a></h4><p><a href="https://arxiv.org/abs/2010.11929" target="_blank" rel="noopener noreferrer">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a> (A. Dosovitskiy et al., 2020)</p></div></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_mRVl"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/gen-ai">GenAI</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/generative-ai">Generative AI</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/deep-learning">Deep Learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/machine-learning">Machine learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/data-science">Data Science</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ai-models">AI Models</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">LLM</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/multimodal">Multimodal</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/object-detection">Object Detection</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/computer-vision">Computer Vision</a></li></ul></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/clair_anniversary"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">CLAIR.bot: One Year of Responsible AI Serving Public Debate</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/Under_The_Hood"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Under the Hood: Technical Blueprint of a GenAI-Powered ESG Due Diligence Tool</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#ingredients" class="table-of-contents__link toc-highlight">Ingredients</a></li><li><a href="#cross-attention-vs-self-attention" class="table-of-contents__link toc-highlight">Cross-attention vs. self-attention</a></li><li><a href="#vision-and-language-alignment" class="table-of-contents__link toc-highlight">Vision and language alignment</a></li><li><a href="#train-train-and-train-again" class="table-of-contents__link toc-highlight">Train, train, and train again</a></li><li><a href="#freeze-or-dont-freeze" class="table-of-contents__link toc-highlight">Freeze or don&#39;t freeze?</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">About us</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://ekimetrics.com/who-we-are/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Who we are ?</a></li><li class="footer__item"><a href="https://ekimetrics.com/our-team/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Our team</a></li><li class="footer__item"><a href="https://ekimetrics.us13.list-manage.com/subscribe?u=85b8ce42caa0a733e98233bc4&amp;id=6355d0a6f9" target="_blank" rel="noopener noreferrer" class="footer__link-item">Subscribe to our newsletter</a></li></ul></div><div class="col footer__col"><div class="footer__title">Find us</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/ekimetrics" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://ekimetrics.com/careers/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Careers<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.welcometothejungle.com/fr/companies/ekimetrics" target="_blank" rel="noopener noreferrer" class="footer__link-item">Eki on Welcome to the jungle<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Contact</div><ul class="footer__items clean-list"><li class="footer__item"><a href="mailto:inno@ekimetrics.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Get in touch with our teams<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div></div></footer></div>
<script src="/assets/js/runtime~main.0e3c3f04.js"></script>
<script src="/assets/js/main.73cebcbb.js"></script>
</body>
</html>