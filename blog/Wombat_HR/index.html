<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Optimizing Chatbot Performance: A Two-Phase Methodology for Enhanced Performance in HR Policy Responses | Eki.Lab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://ekimetrics.github.io/blog/Wombat_HR"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Optimizing Chatbot Performance: A Two-Phase Methodology for Enhanced Performance in HR Policy Responses | Eki.Lab"><meta data-rh="true" name="description" content="At Ekimetrics, we&#x27;ve tackled the challenge of ensuring the relevance and accuracy of chatbot responses by developing a cutting-edge chatbot system designed to navigate the complex landscape of HR policies."><meta data-rh="true" property="og:description" content="At Ekimetrics, we&#x27;ve tackled the challenge of ensuring the relevance and accuracy of chatbot responses by developing a cutting-edge chatbot system designed to navigate the complex landscape of HR policies."><meta data-rh="true" name="keywords" content="GenAI,HR,Chatbot,Q&amp;A,HR policies,Automation"><meta data-rh="true" property="og:image" content="https://ekimetrics.github.io/img/blog/Wombat_HR_header.jpg"><meta data-rh="true" name="twitter:image" content="https://ekimetrics.github.io/img/blog/Wombat_HR_header.jpg"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2024-10-15T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://www.linkedin.com/in/vknotkrishnan/,https://www.linkedin.com/in/basile-el-azhari/"><meta data-rh="true" property="article:tag" content="GenAI,HR,Chatbot,Q&amp;A,HR policies,Automation"><link data-rh="true" rel="icon" href="/img/favicon.png"><link data-rh="true" rel="canonical" href="https://ekimetrics.github.io/blog/Wombat_HR"><link data-rh="true" rel="alternate" href="https://ekimetrics.github.io/blog/Wombat_HR" hreflang="en"><link data-rh="true" rel="alternate" href="https://ekimetrics.github.io/blog/Wombat_HR" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Eki.Lab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Eki.Lab Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-124520099-9","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>




<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-124520099-9"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-124520099-9",{anonymize_ip:!0}),gtag("config","G-MQNYE0E8GE",{anonymize_ip:!0})</script><link rel="stylesheet" href="/assets/css/styles.6225bfab.css">
<link rel="preload" href="/assets/js/runtime~main.e47407cd.js" as="script">
<link rel="preload" href="/assets/js/main.8bf90c87.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"dark")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><b class="navbar__title text--truncate">.</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">About us</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/about">Ekilab</a></li><li><a class="dropdown__link" href="/about/ekimetrics">Ekimetrics</a></li><li><a class="dropdown__link" href="/about/stack">Technology stack</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Resources</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/resources/">Hackathons</a></li><li><a class="dropdown__link" href="/resources/trainings">Trainings</a></li></ul></div><a href="https://ekimetrics.com/fr/carrieres/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Careers</a></div><div class="navbar__items navbar__items--right"><a href="https://ekimetrics.com/fr/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Ekimetrics website</a><a href="mailto:inno@ekimetrics.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Contact us!<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="searchBox_ZlJk"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input type="search" id="search_input_react" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><main class="col col--9 col--offset-1" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="image" content="https://ekimetrics.github.io/img/blog/Wombat_HR_header.jpg"><header><h1 class="title_f1Hy" itemprop="headline">Optimizing Chatbot Performance: A Two-Phase Methodology for Enhanced Performance in HR Policy Responses</h1><div class="container_mt6G margin-vert--md"><time datetime="2024-10-15T00:00:00.000Z" itemprop="datePublished">October 15, 2024</time> Â· <!-- -->9 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/vknotkrishnan/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="/img/authors/vinod_krishnan.png" alt="Vinod KRISHNAN"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/vknotkrishnan/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Vinod KRISHNAN</span></a></div><small class="avatar__subtitle" itemprop="description">Associate Partner</small></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/basile-el-azhari/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="/img/authors/basile_el_azhari.jpg" alt="Basile EL AZHARI"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/basile-el-azhari/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Basile EL AZHARI</span></a></div><small class="avatar__subtitle" itemprop="description">Domain Specialist</small></div></div></div></div></header><div id="__blog-post-container" class="markdown" itemprop="articleBody"><div align="justify"><p>In the ever-evolving landscape of AI-driven solutions, ensuring the relevance and accuracy of chatbot responses is paramount, especially when dealing with sensitive and critical knowledge bases such as HR policies. At Ekimetrics, we&#x27;ve tackled this challenge head-on by developing a cutting-edge chatbot system designed to navigate the complex landscape of HR policies. Our approach leverages Retrieval Augmented Generation (RAG) and employs a rigorous two-phase optimization process. In this post, we&#x27;ll walk you through our methodology, sharing insights that can be applied to enhance chatbot performance across various domains.</p><h1>Our Solution: A Two-Phase Optimization Approach</h1><p>HR policies form the backbone of any organization, guiding everything from leave policies to workplace conduct. However, the sheer volume and complexity of these policies can make it challenging for employees to quickly find the information they need. Our goal was to create a chatbot that could provide accurate, context-aware responses to HR queries, effectively serving as a 24/7 HR assistant. To achieve this, we developed a sophisticated optimization methodology divided into two distinct phases: enhancing source retrieval and fine-tuning answer generation. Let&#x27;s dive into each phase, illustrated by our custom-designed infographics.</p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="phase-1-optimizing-source-retrieval">Phase 1: Optimizing Source Retrieval<a href="#phase-1-optimizing-source-retrieval" class="hash-link" aria-label="Direct link to Phase 1: Optimizing Source Retrieval" title="Direct link to Phase 1: Optimizing Source Retrieval">â</a></h2><div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/Source_Retrieval-2d488c7929b96509d6ca8a321d4fa0b5.png" width="604" height="340" class="img_ev3q"></p></div><div align="justify"><p>The first phase of our methodology was dedicated to optimizing the retrieval of relevant documents that the chatbot uses to generate answers. </p><p>This phase is crucial as the quality of the final answer is directly influenced by the relevance of the retrieved sources.</p><p>As shown in the &quot;Sources Retrieval Pipeline&quot; infographic, our process involves several key steps:</p><ol><li><strong>Document Processing</strong>: </li></ol><ul><li>Parsing PDF documents into raw content</li><li>Cleaning and slicing the content using GPT-4</li><li>Embedding the document chunks with OpenAI ADA</li><li>Indexing the embeddings in a vector database</li></ul><ol start="2"><li><strong>Query Processing</strong>: </li></ol><ul><li>Reformulating the user&#x27;s question using GPT-4</li><li>Retrieving relevant sources from the vector database</li><li>Incorporating conversation history for context</li></ul><p>Key parameters we targeted for optimization included:</p><ul><li><strong>k</strong>: The number of documents retrieved in response to a query</li><li><strong>min_similarity</strong>: The minimum similarity threshold required for a source document to be considered relevant</li></ul><p>Process: We began by defining the parameters k and min_similarity, which directly impacted the chatbotâs ability to fetch the most relevant sources from the HR policy knowledge base. To optimize these parameters, we employed Optuna, a powerful hyperparameter optimization framework. The objective was to fine-tune k and min_similarity to maximize the relevance of retrieved documents, thereby setting a solid foundation for the subsequent answer generation phase.</p><p>Here is the code snippet that demonstrates the optimization process:</p></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">import mlflow</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">import optuna</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">import gc</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">def objective(trial):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  with mlflow.start_run(nested=True):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    gc.collect()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    k = trial.suggest_int(&quot;k&quot;, 1, 4)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    min_similarity = trial.suggest_float(&quot;min_similarity&quot;, 0, 0.04)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    chunk_size = trial.suggest_int(&#x27;chunk_size&#x27;, 200, 2000)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    metrics = get_metrics(k, min_similarity, indexes_names_template=&#x27;hr-dev-{chunk_size}&#x27;)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    mlflow.log_params({&quot;k&quot;: k, &quot;min_similarity&quot;: min_similarity, &quot;chunk_size&quot;: chunk_size})</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    log_metrics(metrics)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    gc.collect()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return -1*metrics[&#x27;f1_score&#x27;]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">with mlflow.start_run(run_name=&quot;tuned_wombat_chatbot&quot;):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  study = optuna.create_study()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  study.optimize(objective, n_trials=200)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">best_params = study.best_params</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">print(f&quot;best_params : {best_params} with final f_score of : {-1* study.best_value} on trial {study.best_trial}&quot;)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div align="justify">To create a realistic testbed, we engaged our Head of HR to formulate answers to 30 representative questions that the chatbot might encounter. These questions then served as our benchmark to systematically evaluate different configurations of k and min_similarity. By iterating through various combinations, we identified the optimal settings that resulted in the highest precision and recall rates during the retrieval process.</div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="phase-2-fine-tuning-answer-generation">Phase 2: Fine-tuning Answer Generation<a href="#phase-2-fine-tuning-answer-generation" class="hash-link" aria-label="Direct link to Phase 2: Fine-tuning Answer Generation" title="Direct link to Phase 2: Fine-tuning Answer Generation">â</a></h2><div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/Answer_Generation-c6bb9c061846ce00cab480aee58f5eb1.png" width="604" height="340" class="img_ev3q"></p></div><div align="justify"><p>The second phase focused on refining the chatbot&#x27;s ability to generate final answers that are not only accurate, but also contextually appropriate, based on the retrieved documents.</p><p>Key components of this phase include:</p><ol><li><strong>Question Reformulation</strong>: Using GPT-4 to enhance the original user question</li><li><strong>Source Retrieval</strong>: Fetching the most relevant documents based on the reformulated question</li><li><strong>Answer Generation</strong>: Combining retrieved sources, conversation history, and the enhanced question to generate a final answer using GPT-4</li></ol><p>Key focus areas for optimization included:</p><ul><li><strong>Temperature</strong>: Controls the randomness of the language modelâs predictions</li><li><strong>Prompts</strong>: Customizable templates that guide the chatbot in generating responses</li><li><strong>chunk_size</strong>: Optimizing the size of text chunks processed by the system</li></ul><p>Process: With the retrieval pipeline optimized, the next step was to fine-tune the parameters governing the language modelâs behavior during answer generation. We adjusted the temperature parameter to balance creativity with accuracy, ensuring that the chatbot&#x27;s responses remained relevant without deviating from the core content of the HR policies.</p><p>In addition to temperature, we experimented with different prompts that directed the chatbot on how to structure its responses.  This step involved crafting and testing various prompt versions to determine which formulations yielded the most accurate and contextually aligned answers.</p><p>Similar to the retrieval optimization, we used the same set of 30 HR-related questions and answers for evaluation.  By comparing the chatbot&#x27;s generated answers against the expected outcome, we employed a bespoke similarity metric to identify the most effective parameter configurations. These similarity metrics were computed by an external model called an LLM Judge that assessed the semantic similarity between the chatbot&#x27;s responses and the expected answers.</p><h1>Answer Similarity Assessment</h1><p>This involved using a custom prompt to assess the semantic similarity between the chatbot&#x27;s responses and the expected answers. By scoring this similarity and providing justifications, we gained additional insights into the chatbot&#x27;s ability to produce responses that align closely with the intended content.</p><details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Here is the prompt that we used for the similarity assessment!</summary><div><div class="collapsibleContent_i85q"><p>  <strong>Task</strong>:</p><p>  You must return the following fields in your response in two lines, one below the other:<br>
<!-- -->score: Your numerical score for the model&#x27;s answer_similarity based on the rubric<br>
<!-- -->justification: Your reasoning about the model&#x27;s answer_similarity score</p><p>  You are an impartial judge. You will be given an input that was sent to a machine learning model, and you will be given an output that the model produced. You may also be given additional information that was used by the model to generate the output.</p><p>  Your task is to determine a numerical score called answer_similarity based on the input, output, and target.
A definition of answer_similarity and a grading rubric are provided below.
You must use the grading rubric to determine your score. You must also justify your score.</p><p>  Examples could be included below for reference. Make sure to use them as references and to understand them before completing the task.</p><p>  Input:<br>
<!-- -->{input}</p><p>  Output:<br>
<!-- -->{output}</p><p>  Target:
{target}</p><p>  <strong>Metric definition</strong>:
Answer similarity is evaluated on the degree of semantic similarity of the provided output to the provided targets, which is the ground truth. Scores can be assigned based on the gradual similarity in meaning and description to the provided targets, where a higher score indicates greater alignment between the provided output and provided targets.</p><p>  <strong>Grading rubric</strong>:
Answer similarity: Below are the details for different scores:</p><ul><li>Score 1: The output has little to no semantic similarity to the provided targets.</li><li>Score 2: The output displays partial semantic similarity to the provided targets on some aspects.</li><li>Score 3: The output has moderate semantic similarity to the provided targets.</li><li>Score 4: The output aligns with the provided targets in most aspects and has substantial semantic similarity.</li><li>Score 5: The output closely aligns with the provided targets in all significant aspects.</li></ul><p>  <strong>Examples</strong>:</p><p>  Example Input:<br>
<!-- -->What is MLflow?</p><p>  Example Output:<br>
<!-- -->MLflow is an open-source platform for managing machine learning workflows, including experiment tracking, model packaging, versioning, and deployment, simplifying the ML lifecycle.</p><p>  Example Target:<br>
<!-- -->MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.</p><p>  Example score: 4<br>
<!-- -->Example justification: The definition effectively explains what MLflow is, its purpose, and its developer. It could be more concise for a 5-score.</p><p>  You must return the following fields in your response in two lines, one below the other:
score: Your numerical score for the model&#x27;s answer_similarity based on the rubric<br>
<!-- -->justification: Your reasoning about the model&#x27;s answer_similarity score<br>
<!-- -->Do not add additional new lines. Do not add any other fields.</p></div></div></details><p>  <a href="https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_similarity" target="_blank" rel="noopener noreferrer">https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_similarity</a></p><h1>Hyperparameter Optimization and Evaluation</h1><p>  Both phases utilized Optuna for hyperparameter optimization, with a focus on minimizing the negative F1 score for improving the retrieval quality, and minimizing the negative similarity score of the final answer relevance. Throughout the optimization process, all relevant metrics and parameters were logged using MLflow, ensuring comprehensive tracking and enabling systematic comparisons between different experimental runs.</p><h1>Potential Biases and Limitations</h1><p>The obvious bias in the above process stems from the fact that the benchmark answers are written or vetted by a single individual, although this individual, as head of HR, has the final authority over what constitutes an acceptable answer. Nonetheless, any idiosyncrasies in how a single individual crafts sentences and structures answers are likely to have been picked up when computing similarity scores. Further, any development of chatbots for general usage will likely be optimized for sounding friendly and empathetic in a way that may transcend a single individualâs writing style. </p><p>For instance, a user may expect interjections and exclamations such as âCongratulationsâ or âCommiserationsâ in output answers warranted by a given situation, which may deviate from sample answers that may be more factual. Our team had to balance such factors in their quest to achieve high similarity scores.</p><h1>Conclusion</h1><p>The methodology outlined here presents a robust, two-phase approach to optimizing a chatbot system designed to navigate the complex and sensitive domain of HR policies. By systematically refining both the source retrieval process and the answer generation mechanism, we have developed a chatbot that is both precise and contextually aware. The integration of advanced hyperparameter optimization techniques, combined with rigorous evaluation and tracking through MLflow, underscores the scientific rigor of this approach. As a result, the chatbot is well-equipped to deliver high-quality, relevant answers, enhancing user experience and ensuring that critical HR-related queries are addressed with the utmost accuracy. Finally, this approach holds relevance for topics that transcend HR â the methodology is just as applicable to building RAGs in any area or domain.</p></div></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_mRVl"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/gen-ai">GenAI</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/hr">HR</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/chatbot">Chatbot</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/q-a">Q&amp;A</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/hr-policies">HR policies</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/automation">Automation</a></li></ul></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/LLMs_fail"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">Where LLMs still fail</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/Bayesian_NN"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Bayesian Neural Networks</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#phase-1-optimizing-source-retrieval" class="table-of-contents__link toc-highlight">Phase 1: Optimizing Source Retrieval</a></li><li><a href="#phase-2-fine-tuning-answer-generation" class="table-of-contents__link toc-highlight">Phase 2: Fine-tuning Answer Generation</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">About us</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://ekimetrics.com/who-we-are/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Who we are ?</a></li><li class="footer__item"><a href="https://ekimetrics.com/our-team/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Our team</a></li><li class="footer__item"><a href="https://ekimetrics.us13.list-manage.com/subscribe?u=85b8ce42caa0a733e98233bc4&amp;id=6355d0a6f9" target="_blank" rel="noopener noreferrer" class="footer__link-item">Subscribe to our newsletter</a></li></ul></div><div class="col footer__col"><div class="footer__title">Find us</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/ekimetrics" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://ekimetrics.com/careers/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Careers<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.welcometothejungle.com/fr/companies/ekimetrics" target="_blank" rel="noopener noreferrer" class="footer__link-item">Eki on Welcome to the jungle<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Contact</div><ul class="footer__items clean-list"><li class="footer__item"><a href="mailto:inno@ekimetrics.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Get in touch with our teams<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div></div></footer></div>
<script src="/assets/js/runtime~main.e47407cd.js"></script>
<script src="/assets/js/main.8bf90c87.js"></script>
</body>
</html>