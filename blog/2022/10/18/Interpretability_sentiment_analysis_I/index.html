<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.70">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Eki.Lab Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Eki.Lab Blog Atom Feed">
<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-124520099-9","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script><title data-react-helmet="true">Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (1/2) | Eki.Lab</title><meta data-react-helmet="true" property="og:title" content="Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (1/2) | Eki.Lab"><meta data-react-helmet="true" name="description" content="Two illustrations of how attention coefficients can be a source of interpretability"><meta data-react-helmet="true" property="og:description" content="Two illustrations of how attention coefficients can be a source of interpretability"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_tag" content="default"><meta data-react-helmet="true" name="keywords" content="Data Science,EkiLab,Ekimetrics,Eki.Lab,Eki,Machine Learning,Artificial Intelligence,Data Science for business,Operational Research,Optimization,Knapsack problem,Deep Reinforcement Learning"><meta data-react-helmet="true" property="og:image" content="/./img/blog/interpretability_articles.jpg"><meta data-react-helmet="true" name="twitter:image" content="/./img/blog/interpretability_articles.jpg"><meta data-react-helmet="true" name="twitter:image:alt" content="Image for Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (1/2)"><link data-react-helmet="true" rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/styles.112324ad.css">
<link rel="preload" href="/styles.f71d4296.js" as="script">
<link rel="preload" href="/runtime~main.47d33273.js" as="script">
<link rel="preload" href="/main.fb9ad941.js" as="script">
<link rel="preload" href="/1.633fe5fc.js" as="script">
<link rel="preload" href="/2.ed921469.js" as="script">
<link rel="preload" href="/167.d6299521.js" as="script">
<link rel="preload" href="/01a85c17.0c2a4368.js" as="script">
<link rel="preload" href="/1be78505.82f54d6f.js" as="script">
<link rel="preload" href="/56ff6114.c8e9d4ed.js" as="script">
<link rel="preload" href="/6875c492.13454632.js" as="script">
<link rel="preload" href="/a6aa9e1f.b9e367b0.js" as="script">
<link rel="preload" href="/bc11c333.6eb53ebd.js" as="script">
<link rel="preload" href="/c4f5d8e4.c8d58096.js" as="script">
<link rel="preload" href="/ccc49370.fa89f86d.js" as="script">
<link rel="preload" href="/170.45dcc766.js" as="script">
<link rel="preload" href="/ece8ad2d.62181ec0.js" as="script">
<link rel="preload" href="/f98044db.007c2e8c.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"dark")}()</script><div id="__docusaurus">
<nav aria-label="Skip navigation links"><button type="button" tabindex="0" class="skipToContent_11B0">Skip to main content</button></nav><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg aria-label="Menu" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/"><strong class="navbar__title">Eki.Lab</strong></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/docs/">About us</a><a class="navbar__item navbar__link" href="/resources/">Resources</a><a href="https://ekimetrics.com/fr/carrieres/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Careers</a></div><div class="navbar__items navbar__items--right"><a href="https://ekimetrics.com/fr/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Ekimetrics website</a><a href="mailto:inno@ekimetrics.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Contact us!</a><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input type="search" id="search_input_react" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/"><strong class="navbar__title">Eki.Lab</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/blog">Blog</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/">About us</a></li><li class="menu__list-item"><a class="menu__link" href="/resources/">Resources</a></li><li class="menu__list-item"><a href="https://ekimetrics.com/fr/carrieres/" target="_blank" rel="noopener noreferrer" class="menu__link">Careers</a></li><li class="menu__list-item"><a href="https://ekimetrics.com/fr/" target="_blank" rel="noopener noreferrer" class="menu__link">Ekimetrics website</a></li><li class="menu__list-item"><a href="mailto:inno@ekimetrics.com" target="_blank" rel="noopener noreferrer" class="menu__link">Contact us!</a></li></ul></div></div></div></nav><div class="main-wrapper blog-wrapper"><div class="container container-wide margin-vert--lg"><div class="row"><div class="col col--2"></div><main class="col col--8"><article><header><h1 class="margin-bottom--sm blogPostTitle_kDB-">Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (1/2)</h1><div class="margin-vert--md"><p>Two illustrations of how attention coefficients can be a source of interpretability</p><time datetime="2022-10-18T00:00:00.000Z" class="blogPostDate_2HVl">October 18, 2022  · 7 min read</time></div><div class="avatar margin-vert--md"><div class="avatar__intro"><h4 class="avatar__name">Written by <a href="mailto:inno@ekimetrics.com" target="_blank" rel="noreferrer noopener">Milan Bhan</a></h4><small class="avatar__subtitle">Senior Data Science Consultant, PhD student</small></div></div><div class="margin-vert--md"><img class="img-blog-header" src="/./img/blog/interpretability_articles.jpg"></div></header><section class="markdown blog-article-custom"><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="summary"></a>Summary<a class="hash-link" href="#summary" title="Direct link to heading">#</a></h2><div align="justify"><p>We propose to illustrate how far BERT-type models can be considered as interpretable by design. We show that the attention coefficients specific to BERT architecture constitute a particularly rich piece of information that can be used to perform interpretability. There are mainly two ways to do interpretability: attribution and generation of counterfactual examples. Here we propose to evaluate how attention coefficients can form the basis of an attribution method. We will show in a second article how they can also be used to set up counterfactuals. </p></div><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="the-bert-architecture"></a>The BERT architecture<a class="hash-link" href="#the-bert-architecture" title="Direct link to heading">#</a></h2><div align="justify"><p>An artificial neural network is a computer system inspired by the functioning of the human brain and biological neurons to learn specific tasks. The neural networks represent a subset of machine learning algorithms. In order to perform a learning task, the neural network spreads information through an elementary network, called a perceptron. The way in which information is diffused can be formalized through linear algebra and the manipulation of various activation functions. A neural network can be defined as an association of elementary objects called formal neurons, like the perceptron. There are several types of layers that can be part of a neural network:</p><ul><li>Fully connected layers, which receive a vector as input, and produce a new vector as output by applying a linear combination and possibly an activation function;</li><li>Convolution layers, which learn localized patterns in space;</li><li>Attention layers, which model the general relations between different objects.</li></ul></div><div align="justify"><p>Attention mechanisms are particularly effective for natural language processing tasks. This is mainly due to the fact that they allow to properly model a word through mathematical representations. In particular, attention layers make it possible to assign a contextual representation of the word on a case-by-case basis. This makes it a much more efficient tool than Word2vec since the latter only models an average context, but does not adapt to the given situation. Attention mechanisms are at the heart of Transformers-type models as shown in the diagram below. The BERT model corresponds to a stack of the left part of the generic architecture of a Transformer [1].</p><p> <img alt="screenshot-app" src="/assets/images/Image_2-3ae3a962b8d25b8df96ed38b648465b2.jpg"></p><div align="center"> Figure 1 - Transformers architecture</div></div><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="fine-tuning-of-bert-for-sentiment-analysis"></a>Fine tuning of BERT for sentiment analysis<a class="hash-link" href="#fine-tuning-of-bert-for-sentiment-analysis" title="Direct link to heading">#</a></h2><div align="justify"><p>To illustrate how attention coefficients can be a source of interpretability in natural language processing, we propose to fine tune a DistilBERT for sentiment analysis. A DistilBERT is a distilled version of BERT. It is smaller, faster, cheaper, lighter and recovers 97% of BERT’s performance on GLUE [2]. A perfect compromise, in fact. Most transformers are available pre-trained on the Hugging Face transformers library [3]. The objective is to perform supervised classification on the IMDB database to assess the sentiment associated with a movie review. An illustration of the dataset is shown below:</p><p> <img alt="screenshot-app" src="/assets/images/Image_3-5154784fda191b99bf74ac11a362d962.jpg"></p><div align="center"> Figure 2 - IMDB sample</div><p> </p>To do so, we import all the libraries needed.  In particular, the tokenizer DistilBertTokenizer and the pre-trained hugging face model TFDistilBertForSequenceClassification are used.<div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#F8F8F2;background-color:#282A36"><div class="token-line" style="color:#F8F8F2"><span class="token plain">tokenizer = DistilBertTokenizer.from_pretrained(&#x27;distilbert-base-uncased&#x27;)</span></div><div class="token-line" style="color:#F8F8F2"><span class="token plain">sentence_encoder = TFDistilBertForSequenceClassification.from_pretrained(&#x27;distilbert-base-uncased&#x27;, output_attentions = True)</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>The parameter &quot;output_attention&quot; must be equal to &quot;True&quot;. It will allow us to retrieve the attention coefficients of the model. We add a dense layer with a softmax activation to fine tune the model to do sentiment analysis. In order to train the model, we use the following hyperparameters:</p><ul><li>initial_lr  = 1e-5</li><li>n_epochs    = 15</li><li>batch_size  = 64</li><li>random_seed = 42</li></ul><p>Finally, we make evolve the learning and stop the learning process if the val_loss does not decrease after a certain number of iterations. </p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#F8F8F2;background-color:#282A36"><div class="token-line" style="color:#F8F8F2"><span class="token plain">reduce_lr = ReduceLROnPlateau(monitor=&#x27;val_loss&#x27;, factor=0.2, verbose = 1,min_delta=0.005,patience=3, min_lr=3e-7)</span></div><div class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#F8F8F2"><span class="token plain">early_stop = EarlyStopping(monitor=&#x27;val_loss&#x27;, min_delta=0, patience=6, verbose=1, mode=&#x27;auto&#x27;,baseline=None, restore_best_weights=True)</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>We can finally fine tune the DistilBert.</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#F8F8F2;background-color:#282A36"><div class="token-line" style="color:#F8F8F2"><span class="token plain">history = model.fit(X_train, y_train, batch_size=bs, epochs=n_epochs, validation_data=(X_test, y_test), </span></div><div class="token-line" style="color:#F8F8F2"><span class="token plain">                    verbose=1,callbacks=[early_stop, reduce_lr])</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>We obtain a val_accurcay of 85%, which is sufficient for our further analysis. Note that a BERT or a RoBERTa would have certainly had a better val_loss, as they are more heavy and complex.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="recovery-of-attention-coefficients"></a>Recovery of attention coefficients<a class="hash-link" href="#recovery-of-attention-coefficients" title="Direct link to heading">#</a></h2><p>We are now able to analyze the attention coefficients related to movie reviews. In order to retrieve it, We need to predict the sentiment associated to a review.
Then, we select the layer(s) of attention to analyze. We focus here on the last layer of attention.</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#F8F8F2;background-color:#282A36"><div class="token-line" style="color:#F8F8F2"><span class="token plain">inputs = tokenizer.batch_encode_plus(reviews,truncation=True, </span></div><div class="token-line" style="color:#F8F8F2"><span class="token plain">                                     add_special_tokens = True, </span></div><div class="token-line" style="color:#F8F8F2"><span class="token plain">                                     max_length = max_len, </span></div><div class="token-line" style="color:#F8F8F2"><span class="token plain">                                     pad_to_max_length = True)</span></div><div class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#F8F8F2"><span class="token plain">tokenized = np.array(inputs[&quot;input_ids&quot;]).astype(&quot;int32&quot;)</span></div><div class="token-line" style="color:#F8F8F2"><span class="token plain">attention_mask = np.array(inputs[&quot;attention_mask&quot;]).astype(&quot;int32&quot;)</span></div><div class="token-line" style="color:#F8F8F2"><span class="token plain">encoded_att = model.layers[2](tokenized,attention_mask =attention_mask)</span></div><div class="token-line" style="color:#F8F8F2"><span class="token plain">#last attention layer</span></div><div class="token-line" style="color:#F8F8F2"><span class="token plain">last_attention=encoded_att.attentions[-1]</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>We finally recovered the 12 attention matrices from the last layer of the DistilBert.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="interpreting-through-attention-attribution"></a>Interpreting through attention attribution<a class="hash-link" href="#interpreting-through-attention-attribution" title="Direct link to heading">#</a></h2><p>A first way to take advantage of the attention coefficients is to directly look at their value in order to evaluate if the right words stand out. We choose to calculate the average attention on all attention layers and heads. A more in-depth work of selection of the most relevant layer would allow to refine the interpretability method. Here, we limit ourselves to the most basic case.</p><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#F8F8F2;background-color:#282A36"><div class="token-line" style="color:#F8F8F2"><span class="token plain">a,b = [], []</span></div><div class="token-line" style="color:#F8F8F2"><span class="token plain">for head in range(0,12) :</span></div><div class="token-line" style="color:#F8F8F2"><span class="token plain">    for i, elt in enumerate(inputs[&#x27;input_ids&#x27;][0]):</span></div><div class="token-line" style="color:#F8F8F2"><span class="token plain">        if np.array(elt) != 1:</span></div><div class="token-line" style="color:#F8F8F2"><span class="token plain">            att = last_attention.numpy()[0,head][0][i]</span></div><div class="token-line" style="color:#F8F8F2"><span class="token plain">            a.append(tokenizer.decode([elt]) + &#x27;_&#x27; + str(i))</span></div><div class="token-line" style="color:#F8F8F2"><span class="token plain">            b.append(att)</span></div><div class="token-line" style="color:#F8F8F2"><span class="token plain">            </span></div><div class="token-line" style="color:#F8F8F2"><span class="token plain">attention_all_head=pd.DataFrame({&quot;Token&quot;:a,&quot;Attention coefficient&quot;:b})</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>In order to have the average attention, we group by the attention score on all the layers and heads.
We finally have the average attention coefficients associated with the words of the film review. As an example, the attention coefficients associated with the following positive review is calculated:</p><p>“<em>Probably my all time favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring . it just never gets old despite my having seen it some 15 or more times in the last 25 years . paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight . the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch . and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling . if i had a dozen thumbs they’d all be up for this movie</em>&quot;.</p><p>The review being long, we represent the text in color. The more red the color, the higher the associated attention coefficient. The result is shown below:</p><p> <img alt="screenshot-app" src="/assets/images/Image_4-fc95973e15821ef99dbc2fd6e4a5b6c8.jpg"></p><div align="center"> Figure 3 - Attention-Based token importance</div><p> </p>We see that the word groups &quot;favorite movie&quot;, &quot;it just never gets old&quot;, &quot;performance brings tears&quot;, or &quot;it is believable and startling&quot; stand out. This explains well why the algorithm evaluated the review as positive and what was the semantic field at the root of this prediction.<p> </p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="next-step"></a>Next step<a class="hash-link" href="#next-step" title="Direct link to heading">#</a></h2><p>We will show in a future article how attention coefficients are useful for generating counterfactual examples to explain the model prediction.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="references"></a>References<a class="hash-link" href="#references" title="Direct link to heading">#</a></h2><p>[1] VASWANI, Ashish, SHAZEER, Noam, PARMAR, Niki, et al. Attention is all you need. Advances in neural information processing systems, 2017, vol. 30.</p><p>[2] SANH, Victor, DEBUT, Lysandre, CHAUMOND, Julien, et al. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.</p><p>[3] Hugging face library <a href="https://huggingface.co/" target="_blank" rel="noopener noreferrer">https://huggingface.co/</a></p></div></section><footer class="row margin-vert--lg"><div class="col"><strong>Tags:</strong><a class="margin-horiz--sm" href="/blog/tags/nlp">NLP</a><a class="margin-horiz--sm" href="/blog/tags/transformers">Transformers</a><a class="margin-horiz--sm" href="/blog/tags/bert">BERT</a><a class="margin-horiz--sm" href="/blog/tags/interpretability">interpretability</a><a class="margin-horiz--sm" href="/blog/tags/explainability">explainability</a><a class="margin-horiz--sm" href="/blog/tags/xai">XAI</a><a class="margin-horiz--sm" href="/blog/tags/attention">attention</a></div></footer></article><div class="margin-vert--xl"><nav class="pagination-nav" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/blog/2022/10/26/Interpretability_sentiment_analysis_II"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">« Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (2/2)</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/blog/2022/09/20/newsletter_Sept-2022"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Newsletter for September 2022 »</div></a></div></nav></div></main><div class="col col--2"><div class="tableOfContents_2xL- thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#summary" class="table-of-contents__link">Summary</a></li><li><a href="#the-bert-architecture" class="table-of-contents__link">The BERT architecture</a></li><li><a href="#fine-tuning-of-bert-for-sentiment-analysis" class="table-of-contents__link">Fine tuning of BERT for sentiment analysis</a></li><li><a href="#recovery-of-attention-coefficients" class="table-of-contents__link">Recovery of attention coefficients</a></li><li><a href="#interpreting-through-attention-attribution" class="table-of-contents__link">Interpreting through attention attribution</a></li><li><a href="#next-step" class="table-of-contents__link">Next step</a></li><li><a href="#references" class="table-of-contents__link">References</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><h4 class="footer__title">About us</h4><ul class="footer__items"><li class="footer__item"><a href="https://ekimetrics.com/who-we-are/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Who we are ?</a></li><li class="footer__item"><a href="https://ekimetrics.com/our-team/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Our team</a></li><li class="footer__item"><a href="https://ekimetrics.us13.list-manage.com/subscribe?u=85b8ce42caa0a733e98233bc4&amp;id=6355d0a6f9" target="_blank" rel="noopener noreferrer" class="footer__link-item">Subscribe to our newsletter</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">Find us</h4><ul class="footer__items"><li class="footer__item"><a href="https://github.com/ekimetrics" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github</a></li><li class="footer__item"><a href="https://ekimetrics.com/careers/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Careers</a></li><li class="footer__item"><a href="https://www.welcometothejungle.com/fr/companies/ekimetrics" target="_blank" rel="noopener noreferrer" class="footer__link-item">Eki on Welcome to the jungle</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">Contact</h4><ul class="footer__items"><li class="footer__item"><a href="mailto:inno@ekimetrics.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Get in touch with our teams</a></li></ul></div></div><div class="footer__bottom text--center"></div></div></footer></div>
<script src="/styles.f71d4296.js"></script>
<script src="/runtime~main.47d33273.js"></script>
<script src="/main.fb9ad941.js"></script>
<script src="/1.633fe5fc.js"></script>
<script src="/2.ed921469.js"></script>
<script src="/167.d6299521.js"></script>
<script src="/01a85c17.0c2a4368.js"></script>
<script src="/1be78505.82f54d6f.js"></script>
<script src="/56ff6114.c8e9d4ed.js"></script>
<script src="/6875c492.13454632.js"></script>
<script src="/a6aa9e1f.b9e367b0.js"></script>
<script src="/bc11c333.6eb53ebd.js"></script>
<script src="/c4f5d8e4.c8d58096.js"></script>
<script src="/ccc49370.fa89f86d.js"></script>
<script src="/170.45dcc766.js"></script>
<script src="/ece8ad2d.62181ec0.js"></script>
<script src="/f98044db.007c2e8c.js"></script>
</body>
</html>