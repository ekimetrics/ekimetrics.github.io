<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (2/2) | Eki.Lab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="http://ekimetrics.github.io/img/10-cubecube03.jpg"><meta data-rh="true" name="twitter:image" content="http://ekimetrics.github.io/img/10-cubecube03.jpg"><meta data-rh="true" property="og:url" content="https://ekimetrics.github.io/blog/2022/10/26/Interpretability_sentiment_analysis_II"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (2/2) | Eki.Lab"><meta data-rh="true" name="description" content="Two illustrations of how attention coefficients can be a source of interpretability."><meta data-rh="true" property="og:description" content="Two illustrations of how attention coefficients can be a source of interpretability."><meta data-rh="true" name="keywords" content="Data Science,EkiLab,Ekimetrics,Eki.Lab,Eki,Machine Learning,Artificial Intelligence,Data Science for business,Operational Research,Optimization,Knapsack problem,Deep Reinforcement Learning"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2022-10-26T00:00:00.000Z"><meta data-rh="true" property="article:author" content="mailto:inno@ekimetrics.com"><meta data-rh="true" property="article:tag" content="NLP,Transformers,BERT,interpretability,explainability,XAI,attention"><link data-rh="true" rel="icon" href="/img/favicon.png"><link data-rh="true" rel="canonical" href="https://ekimetrics.github.io/blog/2022/10/26/Interpretability_sentiment_analysis_II"><link data-rh="true" rel="alternate" href="https://ekimetrics.github.io/blog/2022/10/26/Interpretability_sentiment_analysis_II" hreflang="en"><link data-rh="true" rel="alternate" href="https://ekimetrics.github.io/blog/2022/10/26/Interpretability_sentiment_analysis_II" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Eki.Lab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Eki.Lab Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-124520099-9","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>




<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-124520099-9"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-124520099-9",{anonymize_ip:!0}),gtag("config","G-MQNYE0E8GE",{anonymize_ip:!0})</script><link rel="stylesheet" href="/assets/css/styles.29261950.css">
<link rel="preload" href="/assets/js/runtime~main.4ea5c118.js" as="script">
<link rel="preload" href="/assets/js/main.1e95cc93.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"dark")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><b class="navbar__title text--truncate">.</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">About us</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/about">Ekilab</a></li><li><a class="dropdown__link" href="/about/ekimetrics">Ekimetrics</a></li><li><a class="dropdown__link" href="/about/stack">Technology stack</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Resources</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/resources/">Hackathons</a></li><li><a class="dropdown__link" href="/resources/trainings">Trainings</a></li></ul></div><a href="https://ekimetrics.com/fr/carrieres/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Careers</a></div><div class="navbar__items navbar__items--right"><a href="https://ekimetrics.com/fr/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Ekimetrics website</a><a href="mailto:inno@ekimetrics.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Contact us!<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="searchBox_ZlJk"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input type="search" id="search_input_react" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><main class="col col--9 col--offset-1" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="title_f1Hy" itemprop="headline">Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (2/2)</h1><div class="container_mt6G margin-vert--md"><time datetime="2022-10-26T00:00:00.000Z" itemprop="datePublished">October 26, 2022</time> · <!-- -->7 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="mailto:inno@ekimetrics.com" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Milan Bhan</span></a></div><small class="avatar__subtitle" itemprop="description">Senior Data Science Consultant, PhD student</small></div></div></div></div></header><div id="__blog-post-container" class="markdown" itemprop="articleBody"><div align="center"><p>  <img loading="lazy" alt="screenshot-app " src="/assets/images/interpretability_articles_2-cc917aa65ee22681f92d30c33c40d0d0.jpg" width="5731" height="3821" class="img_ev3q"></p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="summary">Summary<a href="#summary" class="hash-link" aria-label="Direct link to Summary" title="Direct link to Summary">​</a></h2><div align="justify"><p>We propose to illustrate how far BERT-type models can be considered as interpretable by design. We show that the attention coefficients specific to BERT architecture constitute a particularly rich piece of information that can be used to perform interpretability. There are mainly two ways to do interpretability: attribution and generation of counterfactual examples. In a first article, we showed how attention coefficients could be the basis of an attribution interpretability method. Here we propose to evaluate how they can also be used to set up counterfactuals. </p></div><div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="work-presented-in-the-previous-article">Work presented in the previous article<a href="#work-presented-in-the-previous-article" class="hash-link" aria-label="Direct link to Work presented in the previous article" title="Direct link to Work presented in the previous article">​</a></h2><p>Previously, the BERT <!-- -->[1]<!-- --> and DistilBERT <!-- -->[2]<!-- --> models have been mobilized to tackle the well-known problem of sentiment analysis. In particular, we have shown that the BERT and DistilBERT models contain within their architecture attention coefficients that can be at the heart of an attribution interpretability method. Starting from an initial text, a visualization of the weight assignment method was proposed. The more red the color, the higher the associated attention coefficient. </p><div align="center"><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/Image_2-fc95973e15821ef99dbc2fd6e4a5b6c8.jpg" width="769" height="91" class="img_ev3q"></p><p>Figure 1 - Attention-Based token importance</p></div><p> </p><p> We saw that the word groups &quot;<em>favorite movie</em>&quot;, &quot;<em>it just never gets old</em>&quot;, &quot;<em>performance brings tears</em>&quot;, or &quot;<em>it is believable and startling</em>&quot; stood out. This explained well why the algorithm evaluated the review as positive and what was the semantic field at the root of this prediction. This work was done using the Hugging Face transformers library <!-- -->[3]<!-- -->.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="interpreting-through-counterfactual-generation">Interpreting through counterfactual generation<a href="#interpreting-through-counterfactual-generation" class="hash-link" aria-label="Direct link to Interpreting through counterfactual generation" title="Direct link to Interpreting through counterfactual generation">​</a></h2><p>Another way to do interpretability is to generate counterfactual examples. According to Judea Pearl, counterfactual &quot;involves answering questions which ask what might have been, had circumstances been different” <!-- -->[4]<!-- -->. Thus, the idea is to understand a prediction by generating a counterfactual example, resulting in an opposite prediction. In the context of natural language processing, it is therefore a matter of changing the right words in the review. In order to generate a counterfactual example, we propose the following methodology:</p><ul><li>Compute the attention coefficients of the tokens in a text corpus on each attention layer (6). The text corpus size must be statistically significant </li><li>Perform token clustering based on their 6-dimensional representation</li><li>Detect clusters associated with positively and negatively charged sentiment words</li><li>Replace the tokens with the highest average attention with their &quot;opposite token&quot; in their &quot;opposite cluster&quot;
This approach allows us to validate the interpretative strength of the tokens put forward by the attention coefficients, while illustrating what a close review would have been with an opposite sentiment.
We apply the methodology on a corpus of 1000 reviews. The clustering method used is the hierarchical ascending classification (HAC) and gives 3 clusters. The obtained clusters and the counterfactual generation procedure can be represented in 2 dimensions as follows:</li></ul><div align="center"><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/Image_3-094ab2c3d6b69a58223f9a733aef845b.jpg" width="416" height="342" class="img_ev3q"></p><p>Figure 2 - Token clusters &amp; replacements</p></div><p>We then generate the counterfactual example of the review tested earlier by changing 2 words: </p><div align="center">delight ➡ torment<p>favorite ➡ worst</p></div><p>This gives us the following counterfactual example:</p><p>“<em>Probably my all time worst movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring . it just never gets old despite my having seen it some 15 or more times in the last 25 years . paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a torment. the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch . and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling . if i had a dozen thumbs they’d all be up for this movie</em>&quot;.</p><p>As the text is quite long, 2 tokens are not enough to change the feeling associated with the review. The probability score nevertheless drops significantly by 0.3pts.
One way to assess the quality of the generated counterfactual examples is to evaluate the proportion of reviews in a corpus whose associated sentiment has changed. The result can be represented as a &quot;counterfactual confusion matrix&quot; as follows:</p><p>One way to assess the quality of the generated counterfactual examples is to evaluate the proportion of reviews in a corpus whose associated sentiment has changed. The result can be represented as a &quot;counterfactual confusion matrix&quot; as follows:</p><div align="center"><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/Image_4-91c11a737d5e41b98a8657cd3bb17de7.jpg" width="756" height="74" class="img_ev3q"></p><p>Table 1 - Counterfactual confusion matrix example</p></div><p>Where :</p><ul><li>X<sub>11</sub> represents the share of reviews whose initial associated sentiment and the sentiment of the counterfactual example are positive; sentiment has remained the same </li><li>X<sub>12</sub> represents the share of reviews whose sentiment changed from positive to negative; sentiment did change </li><li>X<sub>21</sub> represents the share of reviews whose sentiment changed from negative to positive; sentiment changed well</li><li>X<sub>22</sub> represents the share of reviews whose initial associated sentiment and the sentiment of the counterfactual example are negative; sentiment has remained the same</li></ul><p>We compute the &quot;counterfactual confusion matrix&quot; on the same text corpus that enabled us to perform clustering, picking 5 tokens for each review. The result is given below:</p><div align="center"><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/Image_5-de802c493bd17b6c40e5c5714774720e.jpg" width="756" height="73" class="img_ev3q"></p><p>Table 2 - Actual counterfactual confusion matrix</p></div><p> </p>Thus, we see that changing the 5 tokens with the highest average attention produces a change in sentiment perception in 44% of cases. In particular, the rate of sentiment change for reviews initially perceived as positive is 31% while the rate of sentiment change for reviews initially perceived as negative is 53%. The change from negative to positive seems to be better achieved with our method.<p>We have shown that attention coefficients can be a source of interpretability. Used in the right way, the attention coefficients allow the detection of tokens with high predictive value. They can also be used to generate counterfactual examples in order to better understand what the sentence should have been in order to be associated with an opposite sentiment. The interest of the attention coefficients is reinforced by the &quot;counterfactual confusion matrix&quot;: The high transformation rate of the reviews&#x27; sentiments shows that the tokens selected thanks to the attention are strongly meaningful.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="next-step">Next step<a href="#next-step" class="hash-link" aria-label="Direct link to Next step" title="Direct link to Next step">​</a></h2><p>We plan to test other ways to generate counterfactual examples. One way would be to take advantage of the way DistilBert has been trained: the mask language modeling (MLM). The idea would be to mask the tokens with high average attention, and replace them with the tokens with the highest softmax in the &quot;opposite cluster&quot;. This would ensure the grammatical correctness of the generated counterfactual example. Finally, the generation of counterfactual examples can have other applications than interpretability. In particular, it becomes possible to perform data augmentation in order to give more examples to a model. It can mitigate biases by balancing the sentiments of biased discriminated populations. This would improve fairness indicators while not degrading accuracy. </p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">​</a></h2><p>[1]<!-- --> VASWANI, Ashish, SHAZEER, Noam, PARMAR, Niki, et al. Attention is all you need. Advances in neural information processing systems, 2017, vol. 30.</p><p>[2]<!-- --> SANH, Victor, DEBUT, Lysandre, CHAUMOND, Julien, et al. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.</p><p>[3]<!-- --> Hugging face library <a href="https://huggingface.co/" target="_blank" rel="noopener noreferrer">https://huggingface.co/</a></p><p>[4]<!-- --> PEARL, Judea et MACKENZIE, Dana. The book of why: the new science of cause and effect. Basic books, 2018</p></div></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_mRVl"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/nlp">NLP</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/transformers">Transformers</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/bert">BERT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/interpretability">interpretability</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/explainability">explainability</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/xai">XAI</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/attention">attention</a></li></ul></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/2022/11/10/creative_execution_and_marketing_effectiveness_part_I"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">Exploring the links between creative execution and marketing effectiveness - Part I: Detectron2 Pre-Trained Object Detection Models</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/2022/10/18/Interpretability_sentiment_analysis_I"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (1/2)</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#summary" class="table-of-contents__link toc-highlight">Summary</a></li><li><a href="#work-presented-in-the-previous-article" class="table-of-contents__link toc-highlight">Work presented in the previous article</a></li><li><a href="#interpreting-through-counterfactual-generation" class="table-of-contents__link toc-highlight">Interpreting through counterfactual generation</a></li><li><a href="#next-step" class="table-of-contents__link toc-highlight">Next step</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">About us</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://ekimetrics.com/who-we-are/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Who we are ?</a></li><li class="footer__item"><a href="https://ekimetrics.com/our-team/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Our team</a></li><li class="footer__item"><a href="https://ekimetrics.us13.list-manage.com/subscribe?u=85b8ce42caa0a733e98233bc4&amp;id=6355d0a6f9" target="_blank" rel="noopener noreferrer" class="footer__link-item">Subscribe to our newsletter</a></li></ul></div><div class="col footer__col"><div class="footer__title">Find us</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/ekimetrics" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://ekimetrics.com/careers/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Careers<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.welcometothejungle.com/fr/companies/ekimetrics" target="_blank" rel="noopener noreferrer" class="footer__link-item">Eki on Welcome to the jungle<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Contact</div><ul class="footer__items clean-list"><li class="footer__item"><a href="mailto:inno@ekimetrics.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Get in touch with our teams<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div></div></footer></div>
<script src="/assets/js/runtime~main.4ea5c118.js"></script>
<script src="/assets/js/main.1e95cc93.js"></script>
</body>
</html>