"use strict";(self.webpackChunkeki_lab=self.webpackChunkeki_lab||[]).push([[3927],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>p});var s=a(67294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function n(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);t&&(s=s.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,s)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?n(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):n(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function r(e,t){if(null==e)return{};var a,s,i=function(e,t){if(null==e)return{};var a,s,i={},n=Object.keys(e);for(s=0;s<n.length;s++)a=n[s],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(s=0;s<n.length;s++)a=n[s],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var l=s.createContext({}),u=function(e){var t=s.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},d=function(e){var t=u(e.components);return s.createElement(l.Provider,{value:t},e.children)},c="mdxType",g={inlineCode:"code",wrapper:function(e){var t=e.children;return s.createElement(s.Fragment,{},t)}},h=s.forwardRef((function(e,t){var a=e.components,i=e.mdxType,n=e.originalType,l=e.parentName,d=r(e,["components","mdxType","originalType","parentName"]),c=u(a),h=i,p=c["".concat(l,".").concat(h)]||c[h]||g[h]||n;return a?s.createElement(p,o(o({ref:t},d),{},{components:a})):s.createElement(p,o({ref:t},d))}));function p(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var n=a.length,o=new Array(n);o[0]=h;var r={};for(var l in t)hasOwnProperty.call(t,l)&&(r[l]=t[l]);r.originalType=e,r[c]="string"==typeof e?e:i,o[1]=r;for(var u=2;u<n;u++)o[u]=a[u];return s.createElement.apply(null,o)}return s.createElement.apply(null,a)}h.displayName="MDXCreateElement"},49878:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>g,frontMatter:()=>n,metadata:()=>r,toc:()=>u});var s=a(87462),i=(a(67294),a(3905));const n={title:"Building a datalake - Part 2 - Smart storage & computing strategies for better usability and usefulness",author:"Emilien BOUCAUD",author_title:"Data Engineering & Architecture Consultant",author_url:"mailto:inno@ekimetrics.com",header_image_url:"img/blog/overview_datalake_part_2_v2.png",image:"img/blog/overview_datalake_part_2_v2.png",tags:["Datalake","Data Engineering","Architecture","Data Governance","Data Mesh"],draft:!1,description:"For this second part of datalake building, we\u2019ll go deeper into the journey of data, more specifically expand on storage and compute strategies.",keywords:["Data Science","EkiLab","Ekimetrics","Eki.Lab","Eki","Machine Learning","Artificial Intelligence","Data Science for business","Datalake","Data Engineering","Lakehouse","Data Architecture","Data Governance","Data Validation","Data Mesh","Azure","AWS","GCP"]},o=void 0,r={permalink:"/blog/2023/02/28/building_datalake_part_2",source:"@site/blog/2023-02-28-building_datalake_part_2.md",title:"Building a datalake - Part 2 - Smart storage & computing strategies for better usability and usefulness",description:"For this second part of datalake building, we\u2019ll go deeper into the journey of data, more specifically expand on storage and compute strategies.",date:"2023-02-28T00:00:00.000Z",formattedDate:"February 28, 2023",tags:[{label:"Datalake",permalink:"/blog/tags/datalake"},{label:"Data Engineering",permalink:"/blog/tags/data-engineering"},{label:"Architecture",permalink:"/blog/tags/architecture"},{label:"Data Governance",permalink:"/blog/tags/data-governance"},{label:"Data Mesh",permalink:"/blog/tags/data-mesh"}],readingTime:8.445,hasTruncateMarker:!0,authors:[{name:"Emilien BOUCAUD",title:"Data Engineering & Architecture Consultant",url:"mailto:inno@ekimetrics.com"}],frontMatter:{title:"Building a datalake - Part 2 - Smart storage & computing strategies for better usability and usefulness",author:"Emilien BOUCAUD",author_title:"Data Engineering & Architecture Consultant",author_url:"mailto:inno@ekimetrics.com",header_image_url:"img/blog/overview_datalake_part_2_v2.png",image:"img/blog/overview_datalake_part_2_v2.png",tags:["Datalake","Data Engineering","Architecture","Data Governance","Data Mesh"],draft:!1,description:"For this second part of datalake building, we\u2019ll go deeper into the journey of data, more specifically expand on storage and compute strategies.",keywords:["Data Science","EkiLab","Ekimetrics","Eki.Lab","Eki","Machine Learning","Artificial Intelligence","Data Science for business","Datalake","Data Engineering","Lakehouse","Data Architecture","Data Governance","Data Validation","Data Mesh","Azure","AWS","GCP"]},prevItem:{title:"Does your company level with your car in terms of Analytics?",permalink:"/blog/2023/03/07/level_car_analytics"},nextItem:{title:"Exploring the links between creative execution and marketing effectiveness - Part V: Key Paths to Success and Common Pitfalls to Avoid",permalink:"/blog/2023/02/21/creative_execution_and_marketing_effectiveness_part_V"}},l={authorsImageUrls:[void 0]},u=[{value:"Article Scope",id:"article-scope",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Where designing storage &amp; compute strategies really matters",id:"where-designing-storage--compute-strategies-really-matters",level:2},{value:"Going from source to raw: storage strategy",id:"going-from-source-to-raw-storage-strategy",level:3},{value:"Going from raw input to refined output: computing strategy",id:"going-from-raw-input-to-refined-output-computing-strategy",level:3},{value:"Conclusion",id:"conclusion",level:2}],d={toc:u},c="wrapper";function g(e){let{components:t,...n}=e;return(0,i.kt)(c,(0,s.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("div",{align:"center"},(0,i.kt)("p",null,"  ",(0,i.kt)("img",{alt:"screenshot-app ",src:a(93838).Z,width:"5464",height:"3640"}))),(0,i.kt)("h2",{id:"article-scope"},"Article Scope"),(0,i.kt)("div",{align:"justify"},(0,i.kt)("p",null,"In a previous article - ",(0,i.kt)("a",{parentName:"p",href:"https://ekimetrics.github.io/blog/2022/02/07/building_datalake_part_1/"},"Building a datalake - Part 1 - Usable, Useful, Used, or how to avoid dataswamp and empty shell traps | Eki.Lab")," - we took a look at the foundation architecture used at Ekimetrics when building a datalake. Its focus was to present design elements to ensure your datalake is useful and usable at its core, as well as best practices to avoid falling into the so-called data swamp and empty shell traps."),(0,i.kt)("p",null,"For this second part, we\u2019ll go deeper into the journey of data, more specifically expand on storage and compute strategies, and see how the organisation of data and the way it is transformed impact a datalake\u2019s usability & usefuleness.")),(0,i.kt)("h2",{id:"introduction"},"Introduction"),(0,i.kt)("div",{align:"justify"},(0,i.kt)("p",null,"The overview of data\u2019s journey through a datalake or data platform can be broken down into five steps, represented below from left to right."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"screenshot-app",src:a(79560).Z,width:"1803",height:"693"})),(0,i.kt)("div",{align:"center"}," Data\u2019s journey, from source to usage"),(0,i.kt)("br",null),(0,i.kt)("p",null,"A data platform is highly versatile in providing technical options the five steps above. Getting your data from one step to the next means applying a strategy for how the data input is stored, how it is processed and how the output is exposed for the next step."),(0,i.kt)("p",null,"These strategies will vary depending on the use case and platform, taking business as well as technical constraints in consideration. Designing your datalake\u2019s strategies to be versatile and homogeneous is essential. It allows your data platform to grow fast, in terms of data content as well as use case possibilities. It also ensures that the datalake is under control with a common way of treating data, where its only varying specifities are the entry point (data sources) and output (serving layer). "),(0,i.kt)("p",null,"At Ekimetrics, we\u2019ve developed versatile strategies that are applicable to most common use cases, easily reproducible. These strategies help build new capabilities and provide a better understanding of your data platform.")),(0,i.kt)("br",null),(0,i.kt)("h2",{id:"where-designing-storage--compute-strategies-really-matters"},"Where designing storage & compute strategies really matters"),(0,i.kt)("div",{align:"justify"},(0,i.kt)("p",null,"In most datalake architectures, the data journey\u2019s step where the most impactful design decisions can be made is ",(0,i.kt)("em",{parentName:"p"},"Data storage and processing"),". The other steps are more straightforward:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Data sources are usually out of the architect or data engineer\u2019s control, as they often sit outside of the data platform (e.g. on a third-party server)."),(0,i.kt)("li",{parentName:"ul"},"Ingestion is a step for which design questions around data validation & organization processes may be worth considering. We\u2019ve talked about these processes in the first part of our \u201cBuilding a datalake\u201d articles. As for streaming vs batch, it is only dependent on the source: if the source system is streaming data, a resource to ingest that is necessary ; otherwise, recurrent batch ingestion is the go-to."),(0,i.kt)("li",{parentName:"ul"},"Serving will depend on the target use case, so even if there are important design decisions to make, they will only be impactful in the scope of their use case, not for the whole datalake."),(0,i.kt)("li",{parentName:"ul"},"Usage will most of the time be outside the datalake and depend on the use case. The few design decisions that may be necessary here won\u2019t be as impactful to the datalake\u2019s usability and usefulness either.")),(0,i.kt)("p",null,"Of course, this is not to say that designing relevant strategies and architecture for these steps doesn\u2019t matter: they must be tailored to the business case and technical constraints."),(0,i.kt)("p",null,"On the flipside, the ",(0,i.kt)("em",{parentName:"p"},"Data storage and processing")," step is the central piece in the datalake puzzle. It is where storage and compute strategies will be the most impactful. "),(0,i.kt)("p",null,"What could it look like, then ? Inside this central step, data transits through four zones, from its raw form to fully processed for a particular use case, ready to serve to your businesses. These four zones are detailed below, in between ingestion and serving."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"screenshot-app",src:a(56213).Z,width:"1803",height:"500"})),(0,i.kt)("div",{align:"center"}," Data storage and processing - storage zones"),(0,i.kt)("br",null),(0,i.kt)("p",null,"These zones can be found under various names: Landing - Bronze - Silver - Gold, or Temp - Raw - Cleaned - Conformed, etc. The intent is the same, where data becomes more and more usable and business use case oriented with each zone."),(0,i.kt)("p",null,"In between each storage zone, organization and transformation processes are applied to organize and extract insights out of data. This is where our storage and computing strategies come in.")),(0,i.kt)("h3",{id:"going-from-source-to-raw-storage-strategy"},"Going from source to raw: storage strategy"),(0,i.kt)("div",{align:"justify"},(0,i.kt)("p",null,"An ingested source will usually be exposed in one of two possible ways: "),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Incremental changes, where only what is new or updated is exposed."),(0,i.kt)("li",{parentName:"ul"},"Full datasets, where all of the up-to-date data (or a new timeframe of data) is exposed.")),(0,i.kt)("p",null,"In these two cases, the ingestion strategy we recommend results in the same outcome. Here the strategy is to historize all received data, adding metadata about its reception date (or validity date) through organization in the storage architecture or in the dataset itself. The RAW storage zone becomes a source of historical knowledge about all data points and their changes through time. Doing this, we\u2019re also able to add a \u201cslowly changing dimension\u201d / \u201cchange data capture\u201d aspect to our data, where we can easily find a data point\u2019s values at any moment in time."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"screenshot-app",src:a(99529).Z,width:"1803",height:"1297"})),(0,i.kt)("div",{align:"center"}," Fig. 1: Computing & storage strategies in between Landing and Raw zones"),(0,i.kt)("br",null),(0,i.kt)("admonition",{type:"note"},(0,i.kt)("p",{parentName:"admonition"},"There are limitations to historizing all received data, notably storage costs: this is where the data\u2019s lifecycle must be discussed. Keeping all older versions of a single dataset may start to become expensive as the data piles up, outweighing the pros if only the latest values are used."),(0,i.kt)("p",{parentName:"admonition"},"In this case, a solution can be to keep only the latest values of the source\u2019s data points, or just one version of the source every N periods on top of the latest ones. We can then archive older and unused versions in cold, less costly storage resources. The archived data enters a different lifecycle, where it could be removed at some point in the future if it doesn\u2019t serve any business purpose.")),(0,i.kt)("p",null,"The \u201chistorize everything\u201d strategy ensures RAW storage is the most useful it can be: use case agnostic, your businesses can use and explore the data at its full potential. It also helps unify the way RAW storage is meant to be read by your later processes, improving usability while allowing for costs optimization without sacrificing the underlying principle.")),(0,i.kt)("h3",{id:"going-from-raw-input-to-refined-output-computing-strategy"},"Going from raw input to refined output: computing strategy"),(0,i.kt)("div",{align:"justify"},(0,i.kt)("p",null,"By applying this highly inclusive storage strategy for our RAW storage, we\u2019re then able to fetch the necessary data for our business cases, whether it\u2019s using the full history of a source, its latest version or the latest changes only."),(0,i.kt)("p",null,"The most common computing strategy will be to generate an up-to-date view of the data: at the time of computing, what are the insights\u2019 state ? We\u2019re also able to use older data to track changes and generate insights from these."),(0,i.kt)("p",null,"Refined data can then be exposed in two ways, acting as the source for another system: "),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Exposing the full dataset, where all data is up-to-date"),(0,i.kt)("li",{parentName:"ul"},"Exposing incremental changes, where only what is new or updated is sent to the serving layer")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"screenshot-app",src:a(20240).Z,width:"1803",height:"1561"})),(0,i.kt)("div",{align:"center"}," Fig. 2: computing and storage strategies in between Raw, Trusted and Refined zones"),(0,i.kt)("br",null),(0,i.kt)("p",null,"For some use cases, you can generate \u201cfrozen in time\u201d views of the data, only updating the current timeframe\u2019s view. For instance, we could update the current month\u2019s exposed insights each day, then stop updating it at the end of the month, writing a new one for the next month\u2019s computed insights, and so on."),(0,i.kt)("admonition",{type:"note"},(0,i.kt)("p",{parentName:"admonition"},"Historizing our transformations and results is useful for debugging and business exploration, but iterations may start to pile up, so defining a lifecycle for Trusted (TRD) and Refined (RFD) storage is important as well: do our technical processes or business use cases need all the iterations history stored in trusted TRD ? Can our business use cases work with a simplification of RFD storage, similar to the Delta historization for RAW zone in figure 1 ? ")),(0,i.kt)("p",null,"You may have noticed, our exposed Refined storage looks strangely similar to what we had in the data source: we\u2019re exposing either the full up-to-date version of our insights, or just updates and new ones, just like our sources. The datalake is now a source for your business\u2019 use cases, so it makes sense that it would be able to expose data in a similar way."),(0,i.kt)("p",null,"The \u201cup-to-date view\u201d strategy is highly useful for most use cases, and through smart use of historization, still allows for your businesses to get insights on the data\u2019s evolution through time. It also ensures versatility in making your datalake a usable source for other systems when exposing data."),(0,i.kt)("p",null,"From there, the serving layer can leverage this source in a wide range of solutions, be it a database, reporting, CRM, AI models, etc. ")),(0,i.kt)("br",null),(0,i.kt)("h2",{id:"conclusion"},"Conclusion"),(0,i.kt)("div",{align:"justify"},"In essence, storage & compute strategies can be sumarized in three questions:",(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Why are these strategies necessary: to ensure versatility for businesses and technical processes, all the while improving usability."),(0,i.kt)("li",{parentName:"ul"},"How do they do that: by capturing and organizing data\u2019s history efficiently, unifying the way we look at data while allowing for versatility in its usage."),(0,i.kt)("li",{parentName:"ul"},"What they mean: a unified, highly useful and usable storage, and computing that help the datalake become the source for a wide range of systems and use cases.")),(0,i.kt)("p",null,"As data engineers and architects, we\u2019re always looking for ways to improve our data products. In the context of building a datalake, this means finding ways to make data easy to find, explain and extract insights from. The strategies we\u2019ve talked are key answers to these challenges ; as we use them to grow data platforms and apply them to new use cases, these storage & compute strategies have proved themselves to be highly useful for other data challenges, providing new capabilities and solutions for our clients.")))}g.isMDXComponent=!0},79560:(e,t,a)=>{a.d(t,{Z:()=>s});const s=a.p+"assets/images/img1_data_journey-b88b0c7ad94bd8652615a4bdb7ae128a.png"},56213:(e,t,a)=>{a.d(t,{Z:()=>s});const s=a.p+"assets/images/img2_data_storage-905c99d1d5a0c6e5421ba02084e2063e.png"},99529:(e,t,a)=>{a.d(t,{Z:()=>s});const s=a.p+"assets/images/img3_computing_1-cef3f81042eb0c848b50f0d4c4451404.png"},20240:(e,t,a)=>{a.d(t,{Z:()=>s});const s=a.p+"assets/images/img4_computing_2-dcc7a76dc859f9bae73bb079498fd3eb.png"},93838:(e,t,a)=>{a.d(t,{Z:()=>s});const s=a.p+"assets/images/overview_datalake_part_2_v2-2239a71ec2a5454c6e79ec8ed9c3ad48.png"}}]);